# Resultados - Leaderboard

**Melhor Score:** 0.72935 (TF-IDF + Logistic Regression)

---

## ðŸ“Š Resultados por Categoria

### 1. TF-IDF (Bag of Words)

| Data       | Classificador       | Score   | Status       | Notebook                         |
|------------|---------------------|---------|--------------|----------------------------------|
| 2025-02-19 | Logistic Regression | 0.72935 | âœ… Submetido | tfidf/submit_tfidf.ipynb         |
| 2025-02-19 | LightGBM            | 0.70273 | âœ… Submetido | tfidf/submit_tfidf_lgbm.ipynb    |
| -          | XGBoost             | -       | â³ Pendente  | tfidf/submit_tfidf_xgboost.ipynb |
| -          | CatBoost            | -       | â³ Pendente  | tfidf/submit_tfidf_catboost.ipynb|
| -          | TabPFN              | -       | â³ Pendente  | tfidf/submit_tfidf_tabpfn.ipynb  |
| -          | LinearSVC           | -       | â³ A criar   | -                                |
| -          | SGDClassifier       | -       | â³ A criar   | -                                |
| -          | NaiveBayes          | -       | â³ A criar   | -                                |
| -          | RandomForest        | -       | â³ A criar   | -                                |

**Experimentos pendentes TF-IDF:**
- [ ] Testar diferentes `max_features` (5k, 10k, 20k, 30k)
- [ ] Testar `ngram_range` (1,1), (1,2), (1,3), (2,3)
- [ ] Char n-grams vs Word n-grams
- [ ] TF-IDF + SVD (reduÃ§Ã£o de dimensionalidade)
- [ ] Ensemble de classificadores lineares

---

### 2. Word2Vec / Embeddings EstÃ¡ticos

| Data       | Modelo + Classificador     | Score   | Status       | Notebook                                 |
|------------|----------------------------|---------|--------------|------------------------------------------|
| -          | W2V + LightGBM             | -       | â³ Pendente  | word2vec/submit_word2vec.ipynb           |
| -          | W2V + SVM (RBF)            | -       | â³ Pendente  | word2vec/submit_word2vec_svm.ipynb       |
| -          | W2V + XGBoost              | -       | â³ Pendente  | word2vec/submit_word2vec_xgboost.ipynb   |
| -          | FastText + LogReg          | -       | â³ Pendente  | word2vec/submit_fasttext.ipynb           |
| -          | W2V + Max/Mean Pool        | -       | â³ Pendente  | word2vec/submit_word2vec_maxpool.ipynb   |
| -          | W2V + TF-IDF Weighted      | -       | â³ Pendente  | word2vec/submit_word2vec_tfidf_weighted.ipynb |
| -          | W2V NILC (pre-trained)     | -       | â³ Pendente  | word2vec/submit_word2vec_nilc.ipynb      |

**Experimentos pendentes Word2Vec:**
- [ ] Testar diferentes `vector_size` (100, 200, 300)
- [ ] Testar diferentes `window` (3, 5, 7, 10)
- [x] Usar pesos TF-IDF na mÃ©dia dos embeddings
- [x] Testar FastText (subword)
- [ ] GloVe prÃ©-treinado portuguÃªs

---

### 3. Transformers Fine-tuned

| Data       | Modelo                    | Score   | Status       | Notebook                                          |
|------------|---------------------------|---------|--------------|---------------------------------------------------|
| -          | BERTimbau-base            | -       | â³ Pendente  | transformers/submit_bertimbau.ipynb               |
| -          | BERTimbau-large + Focal   | -       | â³ Pendente  | transformers/submit_bertimbau_large_focal.ipynb   |
| -          | BERTimbau + LoRA          | -       | â³ Pendente  | transformers/submit_bertimbau_lora.ipynb          |
| -          | DistilBERT                | -       | â³ Pendente  | transformers/submit_distilbert.ipynb              |
| -          | mDeBERTa + ClassWeights   | -       | â³ Pendente  | transformers/submit_mdeberta_classweights.ipynb   |
| -          | DeBERTa-v3                | -       | â³ Pendente  | transformers/submit_deberta.ipynb                 |
| -          | XLM-RoBERTa + MeanPool    | -       | â³ Pendente  | transformers/submit_xlmroberta_meanpool.ipynb     |
| -          | BioBERTpt                 | -       | â³ Pendente  | transformers/submit_biobertpt.ipynb               |
| -          | Custom Transformer        | -       | â³ Pendente  | transformers/submit_custom_transformer.ipynb      |

**Experimentos pendentes Transformers:**
- [x] Focal Loss para desbalanceamento
- [x] Class Weights
- [x] Mean Pooling
- [x] LoRA (PEFT)
- [ ] Learning rate tuning (1e-5, 2e-5, 3e-5, 5e-5)
- [ ] Batch size (8, 16, 32)
- [ ] Layer-wise Learning Rate Decay

---

### 4. Sentence Transformers (Embeddings â†’ ML)

| Data       | Modelo + Classificador | Score   | Status       | Notebook                                   |
|------------|------------------------|---------|--------------|--------------------------------------------|
| -          | SBERT + Logistic       | -       | â³ Pendente  | sentence_transformers/submit_sbert.ipynb   |
| -          | SBERT + LightGBM       | -       | â³ A criar   | -                                          |
| -          | SBERT + XGBoost        | -       | â³ A criar   | -                                          |
| -          | SBERT + CatBoost       | -       | â³ A criar   | -                                          |
| -          | SBERT + TabPFN         | -       | â³ A criar   | -                                          |

**Modelos SBERT a testar:**
- [ ] `neuralmind/bert-base-portuguese-cased` (sentence embeddings)
- [ ] `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- [ ] `sentence-transformers/all-mpnet-base-v2`
- [ ] `sentence-transformers/LaBSE`

---

### 5. Ensemble

| Data       | MÃ©todo              | Score   | Status       | Notebook                              |
|------------|---------------------|---------|--------------|---------------------------------------|
| -          | Voting (Soft)       | -       | â³ Pendente  | ensemble/submit_ensemble.ipynb        |
| -          | Voting Classifier   | -       | â³ Pendente  | ensemble/submit_ensemble_voting.ipynb |
| -          | Stacking            | -       | â³ Pendente  | ensemble/submit_stacking.ipynb        |

**Experimentos pendentes Ensemble:**
- [x] Voting: TF-IDF + SBERT + BERT
- [x] Stacking com meta-learner (Logistic Regression)
- [ ] Weighted average baseado em OOF F1
- [ ] Blending

---

## ðŸ“ˆ Resumo de Performance

| Categoria              | Melhor Score | Modelo                |
|------------------------|--------------|------------------------|
| TF-IDF                 | 0.72935      | Logistic Regression    |
| Word2Vec               | -            | -                      |
| Transformers           | -            | -                      |
| Sentence Transformers  | -            | -                      |
| Ensemble               | -            | -                      |

---

## ðŸ” Insights

1. **Logistic Regression > LightGBM** em TF-IDF sugere que modelos lineares funcionam bem para texto esparso
2. Priorizar: LinearSVC, SGDClassifier, NaiveBayes antes de tree-based
3. Transformers podem melhorar se dados forem suficientes para fine-tuning
