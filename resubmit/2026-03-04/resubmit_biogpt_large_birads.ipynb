{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2f0cd6",
   "metadata": {},
   "source": [
    "# SPR 2026 - BioGPT Large (Microsoft)\n",
    "\n",
    "**Modelo:** microsoft/BioGPT-Large\n",
    "\n",
    "**Características:**\n",
    "- Modelo generativo especializado em terminologia biomédica\n",
    "- Desenvolvido pela Microsoft Research\n",
    "- Treinado em literatura biomédica (PubMed)\n",
    "- ~1.5B parâmetros\n",
    "\n",
    "**Abordagem:** BI-RADS Instruction\n",
    "\n",
    "---\n",
    "## CONFIGURAÇÃO KAGGLE:\n",
    "1. **Add Input** → **Models** → `biogpt-large`\n",
    "2. **Add Input** → **Competition** → `spr-2026-mammography-report-classification`\n",
    "3. **Settings** → Internet → **OFF**, GPU → **T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aeeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BIOGPT LARGE =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - BioGPT Large (Microsoft)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SEED = 42\n",
    "MAX_NEW_TOKENS = 10\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device} | Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BI-RADS INSTRUCTION PROMPT (OTIMIZADO PARA BIOGPT) =====\n",
    "# BioGPT é treinado em inglês biomédico, prompt direto sem chat template\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Task: Classify this mammography report according to BI-RADS categories.\n",
    "\n",
    "BI-RADS Categories:\n",
    "0 = Incomplete (needs additional imaging)\n",
    "1 = Negative (normal)\n",
    "2 = Benign (0% malignancy)\n",
    "3 = Probably Benign (<2% malignancy)\n",
    "4 = Suspicious (biopsy needed)\n",
    "5 = Highly Suggestive of Malignancy (>95%)\n",
    "6 = Known Malignancy (biopsy-proven)\n",
    "\n",
    "Mammography Report:\n",
    "{report}\n",
    "\n",
    "BI-RADS Category (0-6):\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR MODELO =====\n",
    "print(\"Carregando BioGPT...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, local_files_only=True,\n",
    "    torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# BioGPT pode não ter pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Modelo: {model.config.architectures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21375551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CLASSIFICAÇÃO =====\n",
    "def classify_report(report, model, tokenizer):\n",
    "    # BioGPT usa prompt direto (não é chat model)\n",
    "    prompt = PROMPT_TEMPLATE.format(report=report)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=MAX_NEW_TOKENS, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Extrair número\n",
    "    for char in response.strip():\n",
    "        if char.isdigit() and char in '0123456':\n",
    "            return int(char)\n",
    "    return 2\n",
    "\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"Real: {sample['target']}, Pred: {classify_report(sample['report'], model, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01091eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDAÇÃO =====\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "val_sample = train_df.groupby('target', group_keys=False).apply(lambda x: x.sample(min(20, len(x)), random_state=SEED))\n",
    "val_preds = [classify_report(row['report'], model, tokenizer) for _, row in tqdm(val_sample.iterrows(), total=len(val_sample))]\n",
    "val_labels = val_sample['target'].values\n",
    "\n",
    "print(f\"\\nF1-Macro: {f1_score(val_labels, val_preds, average='macro'):.5f}\")\n",
    "print(classification_report(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f187e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SUBMISSION =====\n",
    "test_preds = [classify_report(row['report'], model, tokenizer) for _, row in tqdm(test_df.iterrows(), total=len(test_df))]\n",
    "\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
