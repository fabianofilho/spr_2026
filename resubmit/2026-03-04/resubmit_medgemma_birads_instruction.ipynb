{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d767d169",
   "metadata": {},
   "source": [
    "# SPR 2026 - MedGemma BI-RADS Instruction\n",
    "\n",
    "**Abordagem:** LLM médica com instrução detalhada sobre classificação BI-RADS\n",
    "\n",
    "**Diferencial:**\n",
    "- **MedGemma**: Modelo do Google treinado em dados médicos\n",
    "- Prompt de sistema explicando o sistema BI-RADS\n",
    "- Contexto radiológico completo para guiar a classificação\n",
    "\n",
    "**Modelo:** MedGemma-4B-IT ou MedGemma-27B-IT\n",
    "\n",
    "---\n",
    "## CONFIGURAÇÃO KAGGLE:\n",
    "1. **Add Input** → **Models** → `medgemma-4b-it` (ou 27b se GPU permitir)\n",
    "2. **Add Input** → **Competition** → `spr-2026-mammography-report-classification`\n",
    "3. **Settings** → Internet → **OFF**, GPU → **T4 x2** (ou P100 para 27B)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MEDGEMMA BI-RADS INSTRUCTION =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - MedGemma BI-RADS Instruction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SEED = 42\n",
    "BATCH_SIZE = 1\n",
    "MAX_NEW_TOKENS = 10\n",
    "\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Auto-detectar modelo MedGemma\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49119a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PROMPT BI-RADS INSTRUCTION (OTIMIZADO PARA MEDGEMMA) =====\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a senior breast radiologist with extensive expertise in the BI-RADS (Breast Imaging Reporting and Data System) classification.\n",
    "\n",
    "## BI-RADS Classification System\n",
    "\n",
    "The BI-RADS system is the standardized reporting framework for mammography developed by the American College of Radiology:\n",
    "\n",
    "### Category 0 - Incomplete\n",
    "- Examination is inconclusive, requires additional evaluation\n",
    "- Need for comparison with prior studies or additional imaging\n",
    "- Keywords: \"incomplete\", \"needs comparison\", \"additional evaluation\", \"complementary imaging\"\n",
    "\n",
    "### Category 1 - Negative\n",
    "- Completely normal mammogram\n",
    "- No findings to report\n",
    "- Symmetric breasts, no nodules, calcifications, or distortions\n",
    "- Keywords: \"normal\", \"negative\", \"no abnormalities\", \"unremarkable\"\n",
    "\n",
    "### Category 2 - Benign Finding\n",
    "- Definitely benign findings\n",
    "- Includes: benign calcifications, intramammary lymph nodes, calcified fibroadenomas\n",
    "- Malignancy risk: 0%\n",
    "- Keywords: \"benign\", \"benign calcification\", \"fibroadenoma\", \"simple cyst\"\n",
    "\n",
    "### Category 3 - Probably Benign\n",
    "- Finding with high probability of being benign\n",
    "- Malignancy risk: <2%\n",
    "- Short-interval follow-up recommended (6 months)\n",
    "- Keywords: \"probably benign\", \"6-month follow-up\", \"short-interval control\"\n",
    "\n",
    "### Category 4 - Suspicious\n",
    "- Suspicious finding for malignancy\n",
    "- Subdivided into 4A (low), 4B (moderate), 4C (high suspicion)\n",
    "- Malignancy risk: 2-95%\n",
    "- Biopsy recommended\n",
    "- Keywords: \"suspicious\", \"biopsy\", \"FNAB\", \"core biopsy\", \"atypical\"\n",
    "\n",
    "### Category 5 - Highly Suggestive of Malignancy\n",
    "- Classic malignant finding\n",
    "- Malignancy risk: >95%\n",
    "- Appropriate action should be taken\n",
    "- Keywords: \"highly suspicious\", \"malignant\", \"cancer\", \"neoplasm\"\n",
    "\n",
    "### Category 6 - Known Biopsy-Proven Malignancy\n",
    "- Malignancy already proven by prior biopsy\n",
    "- Awaiting definitive treatment\n",
    "- Keywords: \"confirmed carcinoma\", \"positive biopsy\", \"preoperative\"\n",
    "\n",
    "## Your Task\n",
    "Analyze the provided mammography report and classify it into ONE of the categories above (0 to 6).\n",
    "Respond with ONLY a single number from 0 to 6, without any explanation.\"\"\"\n",
    "\n",
    "# MedGemma trabalha bem com português, mas prompt em inglês pode ser mais robusto\n",
    "USER_TEMPLATE = \"\"\"Mammography Report (Portuguese):\n",
    "{report}\n",
    "\n",
    "BI-RADS Classification (respond with only a number from 0 to 6):\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR MODELO =====\n",
    "print(\"Carregando MedGemma...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.bfloat16,  # MedGemma prefere bfloat16\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Modelo carregado: {model.config.architectures}\")\n",
    "print(f\"Parâmetros: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(f\"\\nDistribuição de classes (train):\")\n",
    "print(train_df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cea23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FUNÇÃO DE CLASSIFICAÇÃO =====\n",
    "def classify_report(report, model, tokenizer):\n",
    "    \"\"\"Classifica um laudo usando BI-RADS instruction com MedGemma.\"\"\"\n",
    "    \n",
    "    # Formatar no estilo Gemma chat\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(report=report)}\"}\n",
    "    ]\n",
    "    \n",
    "    # Aplicar template de chat (Gemma style)\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(report=report)}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decodificar resposta\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Extrair número da resposta\n",
    "    for char in response.strip():\n",
    "        if char.isdigit() and char in '0123456':\n",
    "            return int(char)\n",
    "    \n",
    "    # Fallback: classe mais comum\n",
    "    return 2\n",
    "\n",
    "# Testar com uma amostra\n",
    "sample = train_df.iloc[0]\n",
    "pred = classify_report(sample['report'], model, tokenizer)\n",
    "print(f\"Exemplo:\")\n",
    "print(f\"  Report: {sample['report'][:100]}...\")\n",
    "print(f\"  Real: {sample['target']}, Predito: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDAR EM AMOSTRA =====\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Amostra estratificada para validação rápida\n",
    "val_sample = train_df.groupby('target', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(20, len(x)), random_state=SEED)\n",
    ")\n",
    "\n",
    "print(f\"Validando em {len(val_sample)} amostras...\")\n",
    "\n",
    "val_preds = []\n",
    "val_labels = val_sample['target'].values\n",
    "\n",
    "for _, row in tqdm(val_sample.iterrows(), total=len(val_sample)):\n",
    "    pred = classify_report(row['report'], model, tokenizer)\n",
    "    val_preds.append(pred)\n",
    "\n",
    "val_preds = np.array(val_preds)\n",
    "f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "print(f\"\\nF1-Macro (validação): {f1:.5f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54099f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ANÁLISE DE ERROS =====\n",
    "from collections import Counter\n",
    "\n",
    "errors = []\n",
    "for true, pred in zip(val_labels, val_preds):\n",
    "    if true != pred:\n",
    "        errors.append((true, pred))\n",
    "\n",
    "print(\"Erros mais comuns (real -> predito):\")\n",
    "for (true, pred), count in Counter(errors).most_common(10):\n",
    "    print(f\"  {true} -> {pred}: {count}x\")\n",
    "\n",
    "# Matriz de confusão simples\n",
    "print(\"\\nDistribuição de predições:\")\n",
    "print(f\"  Real:     {dict(Counter(val_labels))}\")\n",
    "print(f\"  Predito:  {dict(Counter(val_preds))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GERAR SUBMISSION =====\n",
    "print(\"\\nGerando predições para teste...\")\n",
    "\n",
    "test_preds = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    pred = classify_report(row['report'], model, tokenizer)\n",
    "    test_preds.append(pred)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc18d90",
   "metadata": {},
   "source": [
    "## Sobre o MedGemma\n",
    "\n",
    "**MedGemma** é uma família de modelos do Google especificamente treinados para tarefas médicas:\n",
    "\n",
    "- **MedGemma-4B-IT**: Versão leve, cabe em T4\n",
    "- **MedGemma-27B-IT**: Versão completa, precisa de GPU maior\n",
    "\n",
    "### Vantagens para BI-RADS:\n",
    "1. Pré-treinado em literatura médica\n",
    "2. Entende terminologia radiológica\n",
    "3. Conhecimento de guidelines clínicos\n",
    "\n",
    "### Próximos Passos:\n",
    "1. Se F1 baixo: testar prompt em português\n",
    "2. Adicionar exemplos (few-shot médico)\n",
    "3. Comparar com Qwen BI-RADS Instruction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
