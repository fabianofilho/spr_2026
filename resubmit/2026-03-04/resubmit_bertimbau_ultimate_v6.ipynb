{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau Ultimate v6\n",
    "\n",
    "**Estrat\u00e9gia Final** combinando: BERTimbau + Focal Loss + 3-Seed Ensemble + 5-Fold CV + Grid Search Thresholds\n",
    "\n",
    "---\n",
    "**CONFIGURA\u00c7\u00c3O KAGGLE:**\n",
    "1. Add Input > Models > `bertimbau-ptbr-complete`\n",
    "2. Add Input > Competition > `spr-2026-mammography-report-classification`\n",
    "3. Settings > Internet OFF, GPU T4 x2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('='*60)\n",
    "print('SPR 2026 - BERTimbau Ultimate v6')\n",
    "print('='*60)\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "SEEDS = [42, 123, 456]\n",
    "N_FOLDS = 5\n",
    "\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search(d, depth=0):\n",
    "        if depth > 10: return None\n",
    "        try:\n",
    "            for item in os.listdir(d):\n",
    "                path = os.path.join(d, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')): return path\n",
    "                r = search(path, depth+1) if os.path.isdir(path) else None\n",
    "                if r: return r\n",
    "        except: pass\n",
    "        return None\n",
    "    return search(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma = alpha, gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce).mean()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts, self.labels, self.tokenizer, self.max_len = texts, labels, tokenizer, max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(str(self.texts[idx]), truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n",
    "        item = {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "print(f'Train: {len(train_df)}, Test: {len(test_df)}')\n",
    "print(train_df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_thresholds(probs, labels):\n",
    "    thresholds = {}\n",
    "    for c in range(NUM_CLASSES):\n",
    "        best_t, best_f1 = 0.5, 0\n",
    "        for t in np.arange(0.1, 0.9, 0.02):\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            mask = (probs[:, c] >= t) & (probs[:, c] == probs.max(axis=1))\n",
    "            preds[mask] = c\n",
    "            f1 = f1_score(labels, preds, average='macro')\n",
    "            if f1 > best_f1: best_f1, best_t = f1, t\n",
    "        thresholds[c] = round(best_t, 2)\n",
    "    return thresholds\n",
    "\n",
    "def apply_thresholds(probs, thresholds):\n",
    "    preds = []\n",
    "    for i in range(len(probs)):\n",
    "        pred = np.argmax(probs[i])\n",
    "        for c in sorted(thresholds.keys(), reverse=True):\n",
    "            if probs[i, c] >= thresholds[c] and probs[i, c] > probs[i, pred] * 0.85:\n",
    "                pred = c\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "all_oof, all_test, all_thresh = [], [], []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f'\\n{\"=\"*60}\\nSEED {seed}\\n{\"=\"*60}')\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(train_df), NUM_CLASSES))\n",
    "    test_probs = np.zeros((len(test_df), NUM_CLASSES))\n",
    "    fold_thresh = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['report'], train_df['target'])):\n",
    "        print(f'Fold {fold+1}/{N_FOLDS}')\n",
    "        train_ds = TextDataset(train_df['report'].iloc[train_idx].values, train_df['target'].iloc[train_idx].values, tokenizer, MAX_LEN)\n",
    "        val_ds = TextDataset(train_df['report'].iloc[val_idx].values, train_df['target'].iloc[val_idx].values, tokenizer, MAX_LEN)\n",
    "        test_ds = TextDataset(test_df['report'].values, None, tokenizer, MAX_LEN)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_CLASSES, local_files_only=True).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader)*EPOCHS)\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "                loss = criterion(out.logits, batch['labels'].to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_p = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "                val_p.append(F.softmax(out.logits, dim=-1).cpu().numpy())\n",
    "        val_p = np.vstack(val_p)\n",
    "        oof[val_idx] = val_p\n",
    "        \n",
    "        ft = find_thresholds(val_p, train_df['target'].iloc[val_idx].values)\n",
    "        fold_thresh.append(ft)\n",
    "        \n",
    "        test_p = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                out = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "                test_p.append(F.softmax(out.logits, dim=-1).cpu().numpy())\n",
    "        test_probs += np.vstack(test_p) / N_FOLDS\n",
    "        \n",
    "        print(f'  F1: {f1_score(train_df[\"target\"].iloc[val_idx].values, np.argmax(val_p, axis=1), average=\"macro\"):.5f}')\n",
    "    \n",
    "    avg_t = {c: round(np.mean([ft[c] for ft in fold_thresh]), 2) for c in range(NUM_CLASSES)}\n",
    "    all_oof.append(oof)\n",
    "    all_test.append(test_probs)\n",
    "    all_thresh.append(avg_t)\n",
    "    print(f'Seed {seed} OOF F1: {f1_score(train_df[\"target\"].values, apply_thresholds(oof, avg_t), average=\"macro\"):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60 + '\\nENSEMBLE FINAL\\n' + '='*60)\n",
    "final_oof = np.mean(all_oof, axis=0)\n",
    "final_test = np.mean(all_test, axis=0)\n",
    "final_t = {c: round(np.mean([t[c] for t in all_thresh]), 2) for c in range(NUM_CLASSES)}\n",
    "print(f'Thresholds: {final_t}')\n",
    "\n",
    "base_f1 = f1_score(train_df['target'].values, np.argmax(final_oof, axis=1), average='macro')\n",
    "tuned_f1 = f1_score(train_df['target'].values, apply_thresholds(final_oof, final_t), average='macro')\n",
    "print(f'Baseline: {base_f1:.5f}')\n",
    "print(f'Tuned:    {tuned_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = apply_thresholds(final_test, final_t)\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission salva!')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}