{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa2737d",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau v5 (Class-Adaptive Alpha)\n",
    "\n",
    "**Baseado em:** BERTimbau v4 (0.82073) ðŸ†\n",
    "\n",
    "**Melhoria:** Î± adaptativo por classe baseado na distribuiÃ§Ã£o\n",
    "\n",
    "**EstratÃ©gia:**\n",
    "- Calcular Î± inversamente proporcional Ã  frequÃªncia\n",
    "- Classes raras (5, 6) recebem maior peso\n",
    "- Combinar com threshold tuning\n",
    "\n",
    "---\n",
    "## CONFIGURAÃ‡ÃƒO KAGGLE:\n",
    "1. **Add Input** â†’ **Models** â†’ `bertimbau-ptbr-complete`\n",
    "2. **Add Input** â†’ **Competition** â†’ `spr-2026-mammography-report-classification`\n",
    "3. **Settings** â†’ Internet â†’ **OFF**, GPU â†’ **T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BERTIMBAU v5 - CLASS-ADAPTIVE ALPHA =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - BERTimbau v5 (Class-Adaptive Alpha)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "THRESHOLDS = {0: 0.50, 1: 0.50, 2: 0.50, 3: 0.50, 4: 0.50, 5: 0.30, 6: 0.25}\n",
    "\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device} | Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# Calcular distribuiÃ§Ã£o de classes\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(\"DistribuiÃ§Ã£o de classes:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Calcular alpha adaptativo (inverso da frequÃªncia, normalizado)\n",
    "class_weights = 1.0 / class_counts.values\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES\n",
    "ALPHA_PER_CLASS = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"\\nAlpha por classe: {class_weights.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FOCAL LOSS ADAPTATIVO =====\n",
    "class AdaptiveFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha_per_class, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha_per_class  # tensor [NUM_CLASSES]\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        # Alpha adaptativo por classe\n",
    "        alpha_t = self.alpha[targets]\n",
    "        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = AdaptiveFocalLoss(ALPHA_PER_CLASS, gamma=FOCAL_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9238238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATASET =====\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts, self.labels, self.tokenizer, self.max_len = texts, labels, tokenizer, max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(str(self.texts[idx]), truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n",
    "        item = {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values, train_df['target'].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df['target']\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df['report'].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cccfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TREINAR =====\n",
    "print(\"\\nTreinando modelo...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_CLASSES, local_files_only=True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        loss = criterion(outputs.logits, batch['labels'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"  Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== AVALIAR E SUBMISSION =====\n",
    "def apply_thresholds(probs, thresholds):\n",
    "    preds = []\n",
    "    for i in range(len(probs)):\n",
    "        pred = np.argmax(probs[i])\n",
    "        for c in [6, 5]:\n",
    "            if probs[i, c] >= thresholds[c]:\n",
    "                pred = c\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)\n",
    "\n",
    "model.eval()\n",
    "val_probs, val_labels_list = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        val_probs.append(F.softmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "        val_labels_list.extend(batch['labels'].numpy())\n",
    "\n",
    "val_probs = np.vstack(val_probs)\n",
    "val_labels = np.array(val_labels_list)\n",
    "\n",
    "baseline_preds = np.argmax(val_probs, axis=1)\n",
    "tuned_preds = apply_thresholds(val_probs, THRESHOLDS)\n",
    "\n",
    "print(f\"Baseline F1: {f1_score(val_labels, baseline_preds, average='macro'):.5f}\")\n",
    "print(f\"Threshold F1: {f1_score(val_labels, tuned_preds, average='macro'):.5f}\")\n",
    "\n",
    "# Test submission\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        test_probs.append(F.softmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "\n",
    "test_probs = np.vstack(test_probs)\n",
    "predictions = apply_thresholds(test_probs, THRESHOLDS)\n",
    "\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
