{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7043411d",
   "metadata": {},
   "source": [
    "# SPR 2026 - Qwen 1.5B BI-RADS Instruction\n",
    "\n",
    "**Abordagem:** LLM com instrução detalhada sobre classificação BI-RADS\n",
    "\n",
    "**Diferencial:**\n",
    "- Prompt de sistema explicando o sistema BI-RADS em detalhes\n",
    "- Contexto médico completo para guiar a classificação\n",
    "- Modelo entende a semântica das categorias\n",
    "\n",
    "**Modelo:** Qwen2.5-1.5B-Instruct\n",
    "\n",
    "---\n",
    "## CONFIGURAÇÃO KAGGLE:\n",
    "1. **Add Input** → **Models** → `qwen2.5-1.5b-instruct` (ou equivalente)\n",
    "2. **Add Input** → **Competition** → `spr-2026-mammography-report-classification`\n",
    "3. **Settings** → Internet → **OFF**, GPU → **T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QWEN BI-RADS INSTRUCTION =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - Qwen BI-RADS Instruction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SEED = 42\n",
    "BATCH_SIZE = 1  # LLM precisa de batch pequeno\n",
    "MAX_NEW_TOKENS = 10\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Auto-detectar modelo Qwen\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PROMPT BI-RADS INSTRUCTION =====\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Você é um especialista em radiologia mamária com profundo conhecimento do sistema BI-RADS (Breast Imaging Reporting and Data System).\n",
    "\n",
    "## Sistema de Classificação BI-RADS\n",
    "\n",
    "O BI-RADS é um sistema padronizado de laudos mamográficos desenvolvido pelo American College of Radiology. As categorias são:\n",
    "\n",
    "### Categoria 0 - Incompleto\n",
    "- Exame inconclusivo, necessita avaliação adicional\n",
    "- Indicado quando há necessidade de comparação com exames anteriores\n",
    "- Pode requerer ultrassom, RM ou outras incidências mamográficas\n",
    "- Palavras-chave: \"inconclusivo\", \"necessita comparação\", \"avaliação adicional\", \"complementar\"\n",
    "\n",
    "### Categoria 1 - Negativo\n",
    "- Mamografia completamente normal\n",
    "- Nenhum achado a reportar\n",
    "- Mamas simétricas, sem nódulos, calcificações ou distorções\n",
    "- Palavras-chave: \"normal\", \"negativo\", \"sem alterações\", \"mamas simétricas\"\n",
    "\n",
    "### Categoria 2 - Achado Benigno\n",
    "- Achados definitivamente benignos\n",
    "- Inclui: calcificações benignas, linfonodos intramamários, fibroadenomas calcificados\n",
    "- Risco de malignidade: 0%\n",
    "- Palavras-chave: \"benigno\", \"calcificação benigna\", \"fibroadenoma\", \"cisto simples\"\n",
    "\n",
    "### Categoria 3 - Provavelmente Benigno\n",
    "- Achado com alta probabilidade de ser benigno\n",
    "- Risco de malignidade: <2%\n",
    "- Recomendado acompanhamento em curto intervalo (6 meses)\n",
    "- Palavras-chave: \"provavelmente benigno\", \"controle em 6 meses\", \"acompanhamento\"\n",
    "\n",
    "### Categoria 4 - Suspeito\n",
    "- Achado suspeito para malignidade\n",
    "- Subdivide-se em 4A (baixa), 4B (moderada), 4C (alta suspeita)\n",
    "- Risco de malignidade: 2-95%\n",
    "- Biópsia recomendada\n",
    "- Palavras-chave: \"suspeito\", \"biópsia\", \"PAAF\", \"core biopsy\", \"atípico\"\n",
    "\n",
    "### Categoria 5 - Altamente Sugestivo de Malignidade\n",
    "- Achado clássico de malignidade\n",
    "- Risco de malignidade: >95%\n",
    "- Ação apropriada deve ser tomada\n",
    "- Palavras-chave: \"altamente suspeito\", \"maligno\", \"câncer\", \"neoplasia\"\n",
    "\n",
    "### Categoria 6 - Malignidade Confirmada\n",
    "- Malignidade já comprovada por biópsia prévia\n",
    "- Aguardando tratamento definitivo\n",
    "- Palavras-chave: \"carcinoma confirmado\", \"biópsia positiva\", \"pré-operatório\"\n",
    "\n",
    "## Sua Tarefa\n",
    "Analise o laudo mamográfico fornecido e classifique-o em UMA das categorias acima (0 a 6).\n",
    "Responda APENAS com um único número de 0 a 6, sem explicações.\"\"\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Laudo mamográfico:\n",
    "{report}\n",
    "\n",
    "Classificação BI-RADS (responda apenas o número de 0 a 6):\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ba07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR MODELO =====\n",
    "print(\"Carregando modelo...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Modelo carregado: {model.config.architectures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(f\"\\nDistribuição de classes (train):\")\n",
    "print(train_df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267eae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FUNÇÃO DE CLASSIFICAÇÃO =====\n",
    "def classify_report(report, model, tokenizer):\n",
    "    \"\"\"Classifica um laudo usando BI-RADS instruction.\"\"\"\n",
    "    \n",
    "    # Formatar mensagens no estilo chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(report=report)}\n",
    "    ]\n",
    "    \n",
    "    # Aplicar template de chat\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        text = f\"{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(report=report)}\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decodificar resposta\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Extrair número da resposta\n",
    "    for char in response.strip():\n",
    "        if char.isdigit() and char in '0123456':\n",
    "            return int(char)\n",
    "    \n",
    "    # Fallback: classe mais comum\n",
    "    return 2\n",
    "\n",
    "# Testar com uma amostra\n",
    "sample = train_df.iloc[0]\n",
    "pred = classify_report(sample['report'], model, tokenizer)\n",
    "print(f\"Exemplo:\")\n",
    "print(f\"  Report: {sample['report'][:100]}...\")\n",
    "print(f\"  Real: {sample['target']}, Predito: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDAR EM AMOSTRA =====\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Usar amostra estratificada para validação rápida\n",
    "val_sample = train_df.groupby('target', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(20, len(x)), random_state=SEED)\n",
    ")\n",
    "\n",
    "print(f\"Validando em {len(val_sample)} amostras...\")\n",
    "\n",
    "val_preds = []\n",
    "val_labels = val_sample['target'].values\n",
    "\n",
    "for _, row in tqdm(val_sample.iterrows(), total=len(val_sample)):\n",
    "    pred = classify_report(row['report'], model, tokenizer)\n",
    "    val_preds.append(pred)\n",
    "\n",
    "val_preds = np.array(val_preds)\n",
    "f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "print(f\"\\nF1-Macro (validação): {f1:.5f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== THRESHOLD TUNING (OPCIONAL) =====\n",
    "# Se o modelo gerar probabilidades, podemos ajustar thresholds\n",
    "# Como LLM gera texto, vamos usar a classificação direta\n",
    "\n",
    "# Analisar erros por classe\n",
    "from collections import Counter\n",
    "\n",
    "errors = []\n",
    "for true, pred in zip(val_labels, val_preds):\n",
    "    if true != pred:\n",
    "        errors.append((true, pred))\n",
    "\n",
    "print(\"Erros mais comuns (real -> predito):\")\n",
    "for (true, pred), count in Counter(errors).most_common(10):\n",
    "    print(f\"  {true} -> {pred}: {count}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GERAR SUBMISSION =====\n",
    "print(\"\\nGerando predições para teste...\")\n",
    "\n",
    "test_preds = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    pred = classify_report(row['report'], model, tokenizer)\n",
    "    test_preds.append(pred)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11c59f",
   "metadata": {},
   "source": [
    "## Próximos Passos\n",
    "\n",
    "Se o resultado não for satisfatório, considere:\n",
    "\n",
    "1. **Ajustar SYSTEM_PROMPT** com mais exemplos específicos do dataset\n",
    "2. **Few-shot**: Adicionar 1-2 exemplos de cada classe no prompt\n",
    "3. **Chain-of-Thought**: Pedir ao modelo explicar antes de classificar\n",
    "4. **Fine-tuning**: Treinar o modelo nos dados (LoRA/QLoRA)\n",
    "\n",
    "## Comparativo de Abordagens\n",
    "\n",
    "| Abordagem | Vantagem | Desvantagem |\n",
    "|-----------|----------|-------------|\n",
    "| Zero-shot | Simples | Sem contexto específico |\n",
    "| One-shot | Exemplos | Tokens limitados |\n",
    "| **Instruction** | Contexto rico | Depende do prompt |\n",
    "| Fine-tuning | Melhor resultado | Requer mais recursos |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
