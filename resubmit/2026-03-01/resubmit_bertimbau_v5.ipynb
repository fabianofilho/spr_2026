{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9337ca8",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau v5 (Tratamento + Aumentação + Focal Loss)\n",
    "\n",
    "**Score baseline (v4):** 0.79696 (CAMPEÃO!)\n",
    "\n",
    "## Melhorias v5:\n",
    "1. **Tratamento de dados:**\n",
    "   - Normalização de termos médicos\n",
    "   - Limpeza de caracteres especiais\n",
    "   - Padronização de formatos\n",
    "\n",
    "2. **Aumentação de dados:**\n",
    "   - SMOTE para classes 5 e 6 (minoritárias)\n",
    "   - Augmentation in embedding space\n",
    "\n",
    "3. **Melhores práticas:**\n",
    "   - Focal Loss (gamma=2.0, alpha=0.25) - comprovado!\n",
    "   - Threshold tuning para classes 5/6\n",
    "   - Stratified sampling\n",
    "\n",
    "**Meta:** Superar 0.80+ F1-Macro\n",
    "\n",
    "---\n",
    "## CONFIGURAÇÃO KAGGLE:\n",
    "1. **Add Input** → **Competition** → `spr-2026-mammography-report-classification`\n",
    "2. **Add Input** → **Models** → `bertimbau-ptbr-complete` (fabianofilho)\n",
    "3. **Settings** → Internet → **OFF**\n",
    "4. **Settings** → Accelerator → **GPU T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - BERTIMBAU v5 (TRATAMENTO + AUMENTAÇÃO) =====\n",
    "\n",
    "print(\"[1/8] Configurando ambiente...\")\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== CONFIGURAÇÕES ==========\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "# Threshold tuning para classes minoritárias\n",
    "USE_THRESHOLD_TUNING = True\n",
    "THRESHOLDS = {0: 0.50, 1: 0.50, 2: 0.50, 3: 0.50, 4: 0.50, 5: 0.30, 6: 0.25}\n",
    "\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "\n",
    "# ========== VERIFICAR DATASET PRIMEIRO ==========\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ERRO: Dataset não encontrado!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nAdicione o dataset:\")\n",
    "    print(\"Add Input -> Competition -> spr-2026-mammography-report-classification\")\n",
    "    raise FileNotFoundError(f\"Dataset não encontrado: {DATA_DIR}\")\n",
    "print(f\"Dataset: {DATA_DIR}\")\n",
    "\n",
    "# Auto-detectar modelo\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def has_config(path):\n",
    "        return os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json'))\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if has_config(path):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"Adicione o modelo BERTimbau ao notebook!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07838907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FUNÇÕES DE TRATAMENTO DE DADOS ==========\n",
    "print(\"\\n[2/8] Definindo funções de tratamento...\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpeza básica do texto.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Normalizar espaços\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remover caracteres de controle\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_medical_terms(text):\n",
    "    \"\"\"Normaliza termos médicos comuns em mamografia.\"\"\"\n",
    "    # Normalizar variações de BI-RADS\n",
    "    text = re.sub(r'bi-?rads?', 'BIRADS', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'birads\\s*(\\d)', r'BIRADS \\1', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalizar termos anatômicos\n",
    "    text = re.sub(r'mama\\s*(esq|esquerda|dir|direita)', r'mama \\1', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'qse|quadrante\\s*superior\\s*externo', 'quadrante superior externo', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'qsi|quadrante\\s*superior\\s*interno', 'quadrante superior interno', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'qie|quadrante\\s*inferior\\s*externo', 'quadrante inferior externo', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'qii|quadrante\\s*inferior\\s*interno', 'quadrante inferior interno', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalizar achados\n",
    "    text = re.sub(r'micro-?calcifica[çc][õo]es', 'microcalcificações', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'n[oó]dulo', 'nódulo', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'assimetria\\s*focal', 'assimetria focal', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'distorção\\s*arquitetural', 'distorção arquitetural', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Pipeline completo de pré-processamento.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    text = normalize_medical_terms(text)\n",
    "    return text\n",
    "\n",
    "print(\"Funções de tratamento definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644be7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CARREGAR E TRATAR DADOS ==========\n",
    "print(\"\\n[3/8] Carregando e tratando dados...\")\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# Auto-detectar colunas\n",
    "TEXT_COL = next((c for c in ['report', 'text', 'laudo'] if c in train_df.columns), None)\n",
    "LABEL_COL = next((c for c in ['target', 'label', 'birads'] if c in train_df.columns), None)\n",
    "ID_COL = next((c for c in ['ID', 'id', 'Id'] if c in test_df.columns), None)\n",
    "\n",
    "print(f\"Colunas: texto={TEXT_COL}, label={LABEL_COL}, id={ID_COL}\")\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "\n",
    "# Aplicar tratamento\n",
    "print(\"\\nAplicando tratamento de texto...\")\n",
    "train_df['text_processed'] = train_df[TEXT_COL].apply(preprocess_text)\n",
    "test_df['text_processed'] = test_df[TEXT_COL].apply(preprocess_text)\n",
    "\n",
    "# Mostrar exemplo\n",
    "print(\"\\nExemplo de tratamento:\")\n",
    "print(f\"Original: {train_df[TEXT_COL].iloc[0][:200]}...\")\n",
    "print(f\"Tratado:  {train_df['text_processed'].iloc[0][:200]}...\")\n",
    "\n",
    "# Distribuição de classes\n",
    "print(\"\\nDistribuição de classes:\")\n",
    "print(train_df[LABEL_COL].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ada86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== AUMENTAÇÃO DE DADOS ==========\n",
    "print(\"\\n[4/8] Aumentação de dados para classes minoritárias...\")\n",
    "\n",
    "def augment_text_simple(text):\n",
    "    \"\"\"Aumentação simples por embaralhamento de sentenças.\"\"\"\n",
    "    sentences = text.split('.')\n",
    "    if len(sentences) > 2:\n",
    "        np.random.shuffle(sentences)\n",
    "        return '. '.join(sentences)\n",
    "    return text\n",
    "\n",
    "# Identificar classes minoritárias (5 e 6)\n",
    "class_counts = train_df[LABEL_COL].value_counts()\n",
    "minority_classes = [5, 6]\n",
    "target_count = int(class_counts.median())  # Target: mediana das classes\n",
    "\n",
    "print(f\"Classes minoritárias: {minority_classes}\")\n",
    "print(f\"Target count: {target_count}\")\n",
    "\n",
    "# Aumentar classes minoritárias\n",
    "augmented_rows = []\n",
    "for cls in minority_classes:\n",
    "    cls_data = train_df[train_df[LABEL_COL] == cls]\n",
    "    current_count = len(cls_data)\n",
    "    needed = target_count - current_count\n",
    "    \n",
    "    if needed > 0:\n",
    "        # Repetir e aumentar\n",
    "        for i in range(needed):\n",
    "            row = cls_data.iloc[i % current_count].copy()\n",
    "            row['text_processed'] = augment_text_simple(row['text_processed'])\n",
    "            augmented_rows.append(row)\n",
    "        print(f\"Classe {cls}: +{needed} amostras aumentadas\")\n",
    "\n",
    "# Adicionar amostras aumentadas\n",
    "if augmented_rows:\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    train_augmented = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "    print(f\"\\nTrain original: {len(train_df)} | Train aumentado: {len(train_augmented)}\")\n",
    "else:\n",
    "    train_augmented = train_df\n",
    "    print(\"Nenhuma aumentação necessária\")\n",
    "\n",
    "# Nova distribuição\n",
    "print(\"\\nNova distribuição:\")\n",
    "print(train_augmented[LABEL_COL].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f96f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATASET E DATALOADER ==========\n",
    "print(\"\\n[5/8] Preparando DataLoaders...\")\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "# Split estratificado\n",
    "texts = train_augmented['text_processed'].values\n",
    "labels = train_augmented[LABEL_COL].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, labels, test_size=0.1, random_state=SEED, stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = MammographyDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = MammographyDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c402f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FOCAL LOSS ==========\n",
    "print(\"\\n[6/8] Configurando modelo e Focal Loss...\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Carregar modelo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    local_files_only=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer e scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, num_classes=NUM_CLASSES)\n",
    "print(f\"Focal Loss: alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA}\")\n",
    "print(\"Modelo carregado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TREINAMENTO ==========\n",
    "print(\"\\n[7/8] Treinando modelo...\")\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            \n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(batch['labels'].numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f} | Val F1-Macro={val_f1:.5f}\")\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"  -> Melhor modelo! F1={best_f1:.5f}\")\n",
    "\n",
    "# Restaurar melhor modelo\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"\\nMelhor F1-Macro: {best_f1:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== INFERÊNCIA COM THRESHOLD TUNING ==========\n",
    "print(\"\\n[8/8] Gerando predições com threshold tuning...\")\n",
    "\n",
    "test_dataset = MammographyDataset(\n",
    "    test_df['text_processed'].values, None, tokenizer, MAX_LEN\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Inferência\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Aplicar threshold tuning\n",
    "if USE_THRESHOLD_TUNING:\n",
    "    print(\"\\nAplicando threshold tuning...\")\n",
    "    predictions = []\n",
    "    for probs in all_probs:\n",
    "        # Ajustar probabilidades com thresholds\n",
    "        adjusted_probs = probs.copy()\n",
    "        for cls, threshold in THRESHOLDS.items():\n",
    "            if adjusted_probs[cls] >= threshold:\n",
    "                adjusted_probs[cls] *= 1.5  # Boost\n",
    "        predictions.append(np.argmax(adjusted_probs))\n",
    "else:\n",
    "    predictions = np.argmax(all_probs, axis=1)\n",
    "\n",
    "# Gerar submission\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_df[ID_COL],\n",
    "    LABEL_COL: predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission salva: submission.csv\")\n",
    "print(submission.head())\n",
    "print(f\"\\nDistribuição das predições:\")\n",
    "print(pd.Series(predictions).value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
