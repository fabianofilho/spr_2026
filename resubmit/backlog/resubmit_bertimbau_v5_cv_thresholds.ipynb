{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c7a9e6",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau v5 (Cross-Validation Thresholds)\n",
    "\n",
    "**Baseado em:** BERTimbau v4 (0.82073) ðŸ†\n",
    "\n",
    "**Melhoria:** Usar cross-validation para encontrar thresholds mais estÃ¡veis\n",
    "\n",
    "**EstratÃ©gia:**\n",
    "- 5-Fold CV para treinar modelo\n",
    "- Calcular thresholds Ã³timos em cada fold\n",
    "- Usar mÃ©dia dos thresholds para submissÃ£o\n",
    "\n",
    "---\n",
    "## CONFIGURAÃ‡ÃƒO KAGGLE:\n",
    "1. **Add Input** â†’ **Models** â†’ `bertimbau-ptbr-complete`\n",
    "2. **Add Input** â†’ **Competition** â†’ `spr-2026-mammography-report-classification`\n",
    "3. **Settings** â†’ Internet â†’ **OFF**, GPU â†’ **T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BERTIMBAU v5 - CV THRESHOLDS =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - BERTimbau v5 (CV Thresholds)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "N_FOLDS = 5\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device} | Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16505afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FOCAL LOSS & DATASET =====\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma = alpha, gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts, self.labels, self.tokenizer, self.max_len = texts, labels, tokenizer, max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(str(self.texts[idx]), truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n",
    "        item = {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "test_ds = TextDataset(test_df['report'].values, None, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42937f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CROSS-VALIDATION =====\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "fold_thresholds = []\n",
    "fold_f1s = []\n",
    "oof_probs = np.zeros((len(train_df), NUM_CLASSES))\n",
    "test_probs_sum = np.zeros((len(test_df), NUM_CLASSES))\n",
    "\n",
    "def find_optimal_thresholds(probs, labels):\n",
    "    thresholds = {}\n",
    "    for c in range(NUM_CLASSES):\n",
    "        best_thresh, best_f1 = 0.5, 0\n",
    "        for thresh in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            for i in range(len(probs)):\n",
    "                if probs[i, c] >= thresh and probs[i, c] == probs[i].max():\n",
    "                    preds[i] = c\n",
    "            f1 = f1_score(labels, preds, average='macro')\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_thresh = f1, thresh\n",
    "        thresholds[c] = best_thresh\n",
    "    return thresholds\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['report'], train_df['target'])):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold+1}/{N_FOLDS}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_texts = train_df['report'].iloc[train_idx].values\n",
    "    val_texts = train_df['report'].iloc[val_idx].values\n",
    "    train_labels = train_df['target'].iloc[train_idx].values\n",
    "    val_labels = train_df['target'].iloc[val_idx].values\n",
    "    \n",
    "    train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_CLASSES, local_files_only=True)\n",
    "    model.to(device)\n",
    "    criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*EPOCHS)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "            loss = criterion(outputs.logits, batch['labels'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Avaliar fold\n",
    "    model.eval()\n",
    "    val_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "            val_probs_list.append(F.softmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "    \n",
    "    val_probs = np.vstack(val_probs_list)\n",
    "    oof_probs[val_idx] = val_probs\n",
    "    \n",
    "    # Calcular thresholds do fold\n",
    "    fold_thresh = find_optimal_thresholds(val_probs, val_labels)\n",
    "    fold_thresholds.append(fold_thresh)\n",
    "    \n",
    "    preds = np.argmax(val_probs, axis=1)\n",
    "    f1 = f1_score(val_labels, preds, average='macro')\n",
    "    fold_f1s.append(f1)\n",
    "    print(f\"Fold {fold+1} F1: {f1:.5f}\")\n",
    "    print(f\"Thresholds: {fold_thresh}\")\n",
    "    \n",
    "    # Acumular prediÃ§Ãµes de teste\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "            start = i * BATCH_SIZE\n",
    "            end = min(start + BATCH_SIZE, len(test_df))\n",
    "            test_probs_sum[start:end] += F.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "test_probs_avg = test_probs_sum / N_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abe005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== THRESHOLDS MÃ‰DIOS E SUBMISSION =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTADOS CV\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F1-Macro por fold: {fold_f1s}\")\n",
    "print(f\"F1-Macro mÃ©dio: {np.mean(fold_f1s):.5f} Â± {np.std(fold_f1s):.5f}\")\n",
    "\n",
    "# MÃ©dia dos thresholds\n",
    "avg_thresholds = {}\n",
    "for c in range(NUM_CLASSES):\n",
    "    avg_thresholds[c] = round(np.mean([ft[c] for ft in fold_thresholds]), 2)\n",
    "print(f\"\\nThresholds mÃ©dios: {avg_thresholds}\")\n",
    "\n",
    "def apply_thresholds(probs, thresholds):\n",
    "    preds = []\n",
    "    for i in range(len(probs)):\n",
    "        pred = np.argmax(probs[i])\n",
    "        for c in [6, 5]:\n",
    "            if probs[i, c] >= thresholds[c]:\n",
    "                pred = c\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)\n",
    "\n",
    "predictions = apply_thresholds(test_probs_avg, avg_thresholds)\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
