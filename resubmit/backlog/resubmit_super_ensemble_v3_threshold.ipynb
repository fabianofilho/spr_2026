{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9931ba23",
   "metadata": {},
   "source": [
    "# SPR 2026 - Super Ensemble v3 (Threshold Tuning)\n",
    "\n",
    "**Baseado em:** BERTimbau v4 (0.82073) ðŸ†\n",
    "\n",
    "**Melhoria:** Combinar TODOS os melhores modelos + threshold tuning\n",
    "\n",
    "**Componentes:**\n",
    "1. BERTimbau + Focal Loss + Threshold\n",
    "2. TF-IDF + LinearSVC (calibrado)\n",
    "3. TF-IDF + SGDClassifier\n",
    "4. Stacking dos TF-IDF models\n",
    "\n",
    "**EstratÃ©gia:**\n",
    "- Weighted average baseado em F1\n",
    "- Threshold tuning no ensemble final\n",
    "\n",
    "---\n",
    "## CONFIGURAÃ‡ÃƒO KAGGLE:\n",
    "1. **Add Input** â†’ **Models** â†’ `bertimbau-ptbr-complete`\n",
    "2. **Add Input** â†’ **Competition** â†’ `spr-2026-mammography-report-classification`\n",
    "3. **Settings** â†’ Internet â†’ **OFF**, GPU â†’ **T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SUPER ENSEMBLE v3 - THRESHOLD TUNING =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - Super Ensemble v3 (Threshold Tuning)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== CONFIG =====\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "THRESHOLDS = {0: 0.50, 1: 0.50, 2: 0.50, 3: 0.50, 4: 0.50, 5: 0.30, 6: 0.25}\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device} | Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values, train_df['target'].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df['target']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PARTE 1: TF-IDF MODELS =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. TF-IDF Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train = tfidf.fit_transform(train_texts)\n",
    "X_val = tfidf.transform(val_texts)\n",
    "X_test = tfidf.transform(test_df['report'].values)\n",
    "\n",
    "# LinearSVC calibrado\n",
    "print(\"Treinando LinearSVC + Calibration...\")\n",
    "svc = LinearSVC(C=1.0, max_iter=10000, random_state=SEED)\n",
    "svc_cal = CalibratedClassifierCV(svc, cv=3, method='sigmoid')\n",
    "svc_cal.fit(X_train, train_labels)\n",
    "svc_probs = svc_cal.predict_proba(X_val)\n",
    "print(f\"  LinearSVC F1: {f1_score(val_labels, svc_cal.predict(X_val), average='macro'):.5f}\")\n",
    "\n",
    "# SGDClassifier calibrado\n",
    "print(\"Treinando SGDClassifier + Calibration...\")\n",
    "sgd = SGDClassifier(loss='hinge', random_state=SEED)\n",
    "sgd_cal = CalibratedClassifierCV(sgd, cv=3, method='sigmoid')\n",
    "sgd_cal.fit(X_train, train_labels)\n",
    "sgd_probs = sgd_cal.predict_proba(X_val)\n",
    "print(f\"  SGDClassifier F1: {f1_score(val_labels, sgd_cal.predict(X_val), average='macro'):.5f}\")\n",
    "\n",
    "# Stacking\n",
    "print(\"Treinando Stacking...\")\n",
    "base_estimators = [\n",
    "    ('svc', CalibratedClassifierCV(LinearSVC(random_state=SEED), cv=3)),\n",
    "    ('sgd', CalibratedClassifierCV(SGDClassifier(random_state=SEED), cv=3)),\n",
    "]\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    cv=3\n",
    ")\n",
    "stacking.fit(X_train, train_labels)\n",
    "stacking_probs = stacking.predict_proba(X_val)\n",
    "print(f\"  Stacking F1: {f1_score(val_labels, stacking.predict(X_val), average='macro'):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0373fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PARTE 2: BERTIMBAU =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. BERTimbau + Focal Loss\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma = alpha, gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts, self.labels, self.tokenizer, self.max_len = texts, labels, tokenizer, max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(str(self.texts[idx]), truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n",
    "        item = {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze()}\n",
    "        if self.labels is not None: item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df['report'].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_CLASSES, local_files_only=True)\n",
    "model.to(device)\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        loss = criterion(outputs.logits, batch['labels'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "model.eval()\n",
    "bert_probs_val = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        bert_probs_val.append(F.softmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "bert_probs_val = np.vstack(bert_probs_val)\n",
    "print(f\"  BERTimbau F1: {f1_score(val_labels, np.argmax(bert_probs_val, axis=1), average='macro'):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SUPER ENSEMBLE =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. Super Ensemble\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def apply_thresholds(probs, thresholds):\n",
    "    preds = []\n",
    "    for i in range(len(probs)):\n",
    "        pred = np.argmax(probs[i])\n",
    "        for c in [6, 5]:\n",
    "            if probs[i, c] >= thresholds[c]:\n",
    "                pred = c\n",
    "                break\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)\n",
    "\n",
    "# Calcular F1 de cada modelo\n",
    "f1s = {\n",
    "    'bert': f1_score(val_labels, np.argmax(bert_probs_val, axis=1), average='macro'),\n",
    "    'svc': f1_score(val_labels, svc_cal.predict(X_val), average='macro'),\n",
    "    'sgd': f1_score(val_labels, sgd_cal.predict(X_val), average='macro'),\n",
    "    'stacking': f1_score(val_labels, stacking.predict(X_val), average='macro'),\n",
    "}\n",
    "print(f\"F1 individual: {f1s}\")\n",
    "\n",
    "# Pesos proporcionais ao F1\n",
    "total = sum(f1s.values())\n",
    "weights = {k: v/total for k, v in f1s.items()}\n",
    "print(f\"Pesos: {weights}\")\n",
    "\n",
    "# Ensemble\n",
    "ensemble_probs_val = (\n",
    "    weights['bert'] * bert_probs_val +\n",
    "    weights['svc'] * svc_probs +\n",
    "    weights['sgd'] * sgd_probs +\n",
    "    weights['stacking'] * stacking_probs\n",
    ")\n",
    "\n",
    "baseline_preds = np.argmax(ensemble_probs_val, axis=1)\n",
    "tuned_preds = apply_thresholds(ensemble_probs_val, THRESHOLDS)\n",
    "\n",
    "print(f\"Ensemble Baseline F1: {f1_score(val_labels, baseline_preds, average='macro'):.5f}\")\n",
    "print(f\"Ensemble Threshold F1: {f1_score(val_labels, tuned_preds, average='macro'):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SUBMISSION =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. Submission\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Probabilidades de teste\n",
    "bert_probs_test = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        bert_probs_test.append(F.softmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "bert_probs_test = np.vstack(bert_probs_test)\n",
    "\n",
    "svc_probs_test = svc_cal.predict_proba(X_test)\n",
    "sgd_probs_test = sgd_cal.predict_proba(X_test)\n",
    "stacking_probs_test = stacking.predict_proba(X_test)\n",
    "\n",
    "ensemble_probs_test = (\n",
    "    weights['bert'] * bert_probs_test +\n",
    "    weights['svc'] * svc_probs_test +\n",
    "    weights['sgd'] * sgd_probs_test +\n",
    "    weights['stacking'] * stacking_probs_test\n",
    ")\n",
    "\n",
    "predictions = apply_thresholds(ensemble_probs_test, THRESHOLDS)\n",
    "\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
