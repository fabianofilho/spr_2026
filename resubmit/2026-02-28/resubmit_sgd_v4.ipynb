{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98512796",
   "metadata": {},
   "source": [
    "# SPR 2026 - SGDClassifier v4 (RandomSearch Intensivo + SMOTE)\n",
    "\n",
    "**Score v3:** 0.77036 (+2.7% melhoria!)\n",
    "\n",
    "**Estratégia v4:** Replicar e intensificar o sucesso do v3\n",
    "- RandomizedSearchCV com **50 iterações** (vs 30 no v3)\n",
    "- **SMOTE ativo** para classes 5/6 (500 samples cada)\n",
    "- Class weights balanceados\n",
    "- Threshold tuning por classe\n",
    "\n",
    "**Meta:** Alcançar 0.80+ F1-Macro\n",
    "\n",
    "---\n",
    "**CONFIGURAÇÃO KAGGLE:** Internet OFF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296948b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import loguniform, uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - SGDClassifier v4 (RandomSearch Intensivo + SMOTE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# ========== VERIFICAR DATASET PRIMEIRO ==========\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ERRO: Dataset não encontrado!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nAdicione o dataset:\")\n",
    "    print(\"Add Input → Competition → spr-2026-mammography-report-classification\")\n",
    "    raise FileNotFoundError(f\"Dataset não encontrado: {DATA_DIR}\")\n",
    "print(f\"Dataset: {DATA_DIR}\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ========== FLAGS - v4 CONFIGURAÇÃO ==========\n",
    "REMOVE_CLASS_2 = False\n",
    "USE_SMOTE = True                # ATIVO - diferencial v4\n",
    "USE_THRESHOLD_TUNING = True\n",
    "N_SEARCH_ITER = 50              # 50 iter (vs 30 no v3)\n",
    "SMOTE_TARGET_5 = 500            # Oversample classe 5\n",
    "SMOTE_TARGET_6 = 500            # Oversample classe 6\n",
    "# =============================================\n",
    "\n",
    "# Dados\n",
    "print(\"\\n[1/6] Carregando dados...\")\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "\n",
    "# Auto-detectar colunas\n",
    "def find_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "        for col in df.columns:\n",
    "            if col.lower() == c.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "TEXT_COL = find_col(train, ['report', 'text', 'laudo', 'texto', 'content'])\n",
    "LABEL_COL = find_col(train, ['target', 'label', 'birads', 'classe', 'class'])\n",
    "ID_COL = find_col(test, ['ID', 'id', 'Id', 'index', 'idx'])\n",
    "print(f\"Colunas: texto={TEXT_COL}, label={LABEL_COL}, id={ID_COL}\")\n",
    "\n",
    "if REMOVE_CLASS_2:\n",
    "    train = train[train[LABEL_COL] != 2].reset_index(drop=True)\n",
    "    print(f\"Sem classe 2: {train.shape}\")\n",
    "\n",
    "# Distribuição de classes\n",
    "print(\"\\nDistribuição de classes:\")\n",
    "print(train[LABEL_COL].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF otimizado\n",
    "print(\"\\n[2/6] TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,          # Aumentado para capturar mais features\n",
    "    ngram_range=(1, 3),          # Trigramas incluídos\n",
    "    min_df=2, \n",
    "    max_df=0.95, \n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train = tfidf.fit_transform(train[TEXT_COL])\n",
    "X_test = tfidf.transform(test[TEXT_COL])\n",
    "y_train = train[LABEL_COL].values\n",
    "print(f\"Shape TF-IDF: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE para classes minoritárias\n",
    "print(\"\\n[3/6] SMOTE para classes 5 e 6...\")\n",
    "\n",
    "if USE_SMOTE:\n",
    "    try:\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        \n",
    "        # Verificar contagem atual\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        print(\"Antes do SMOTE:\")\n",
    "        for u, c in zip(unique, counts):\n",
    "            print(f\"  Classe {u}: {c} samples\")\n",
    "        \n",
    "        # SMOTE apenas para classes 5 e 6\n",
    "        smote_strategy = {}\n",
    "        if 5 in unique and counts[list(unique).index(5)] < SMOTE_TARGET_5:\n",
    "            smote_strategy[5] = SMOTE_TARGET_5\n",
    "        if 6 in unique and counts[list(unique).index(6)] < SMOTE_TARGET_6:\n",
    "            smote_strategy[6] = SMOTE_TARGET_6\n",
    "        \n",
    "        if smote_strategy:\n",
    "            smote = SMOTE(\n",
    "                sampling_strategy=smote_strategy, \n",
    "                random_state=SEED, \n",
    "                k_neighbors=3\n",
    "            )\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            \n",
    "            print(f\"\\nDepois do SMOTE: {X_train.shape}\")\n",
    "            unique, counts = np.unique(y_train, return_counts=True)\n",
    "            for u, c in zip(unique, counts):\n",
    "                print(f\"  Classe {u}: {c} samples\")\n",
    "        else:\n",
    "            print(\"SMOTE não necessário - classes já têm samples suficientes\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"⚠️ imblearn não disponível - continuando sem SMOTE\")\n",
    "else:\n",
    "    print(\"SMOTE desativado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54401140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearch INTENSIVO\n",
    "print(f\"\\n[4/6] RandomizedSearchCV INTENSIVO ({N_SEARCH_ITER} iterações)...\")\n",
    "\n",
    "param_dist = {\n",
    "    'alpha': loguniform(1e-6, 1e-2),           # Range mais amplo\n",
    "    'loss': ['log_loss', 'modified_huber'],     # Focado nos que funcionam\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'l1_ratio': uniform(0.1, 0.8),              # Distribuição contínua\n",
    "    'max_iter': [2000, 3000, 5000],              # Mais iterações\n",
    "    'learning_rate': ['optimal', 'adaptive'],\n",
    "    'eta0': loguniform(1e-4, 1e-1),\n",
    "    'power_t': uniform(0.1, 0.9),                # Novo parâmetro\n",
    "    'tol': loguniform(1e-5, 1e-3),              # Tolerância\n",
    "}\n",
    "\n",
    "base = SGDClassifier(\n",
    "    class_weight='balanced', \n",
    "    random_state=SEED, \n",
    "    n_jobs=-1, \n",
    "    early_stopping=True, \n",
    "    validation_fraction=0.1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    base, \n",
    "    param_dist, \n",
    "    n_iter=N_SEARCH_ITER, \n",
    "    cv=cv, \n",
    "    scoring='f1_macro', \n",
    "    n_jobs=-1, \n",
    "    random_state=SEED, \n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MELHORES PARÂMETROS:\")\n",
    "print(f\"{'='*60}\")\n",
    "for param, value in search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMelhor F1-macro CV: {search.best_score_:.5f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning por classe\n",
    "print(\"\\n[5/6] Threshold tuning...\")\n",
    "\n",
    "if USE_THRESHOLD_TUNING and hasattr(best_model, 'predict_proba'):\n",
    "    proba = best_model.predict_proba(X_test)\n",
    "    classes = best_model.classes_\n",
    "    \n",
    "    # Thresholds ajustados para classes minoritárias\n",
    "    thresholds = {\n",
    "        0: 0.50,\n",
    "        1: 0.50,\n",
    "        2: 0.50,\n",
    "        3: 0.50,\n",
    "        4: 0.50,\n",
    "        5: 0.30,  # Mais sensível - classe minoritária\n",
    "        6: 0.25   # Muito mais sensível - classe muito minoritária\n",
    "    }\n",
    "    \n",
    "    print(\"Thresholds por classe:\")\n",
    "    for c, t in thresholds.items():\n",
    "        print(f\"  Classe {c}: {t}\")\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(len(proba)):\n",
    "        adj = proba[i].copy()\n",
    "        for j, c in enumerate(classes):\n",
    "            if c in thresholds:\n",
    "                # Ajustar probabilidades baseado no threshold\n",
    "                adj[j] *= (0.5 / thresholds[c])\n",
    "        preds.append(classes[np.argmax(adj)])\n",
    "    \n",
    "    predictions = np.array(preds)\n",
    "else:\n",
    "    print(\"Predição padrão (sem threshold tuning)...\")\n",
    "    predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submissão\n",
    "print(\"\\n[6/6] Gerando submissão...\")\n",
    "\n",
    "sample_path = f'{DATA_DIR}/sample_submission.csv'\n",
    "if os.path.exists(sample_path):\n",
    "    sample_sub = pd.read_csv(sample_path)\n",
    "    SUB_ID = sample_sub.columns[0]\n",
    "    SUB_LABEL = sample_sub.columns[1]\n",
    "else:\n",
    "    SUB_ID = ID_COL\n",
    "    SUB_LABEL = LABEL_COL\n",
    "\n",
    "submission = pd.DataFrame({SUB_ID: test[ID_COL], SUB_LABEL: predictions})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SGDClassifier v4 CONCLUÍDO!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMelhor F1-macro CV: {search.best_score_:.5f}\")\n",
    "print(\"\\nDistribuição das predições:\")\n",
    "print(submission[SUB_LABEL].value_counts().sort_index())\n",
    "print(\"\\n✅ submission.csv criado!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
