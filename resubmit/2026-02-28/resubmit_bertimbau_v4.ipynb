{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db8f32a",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau v4 (Threshold Tuning Apenas)\n",
    "\n",
    "**Score baseline:** 0.79696 (CAMPEÃƒO!)\n",
    "\n",
    "**EstratÃ©gia v4:** NÃƒO MEXER NO MODELO - apenas threshold tuning\n",
    "- Usar modelo exatamente como estÃ¡ (config original)\n",
    "- Ajustar APENAS thresholds na inferÃªncia\n",
    "- Thresholds mais sensÃ­veis para classes 5 e 6\n",
    "\n",
    "**IMPORTANTE:** O modelo original funciona bem. AlteraÃ§Ãµes quebraram v3.\n",
    "\n",
    "**Meta:** Manter 0.79+ ou melhorar para 0.80+\n",
    "\n",
    "---\n",
    "## ðŸ“¥ MODELO - Baixar com: `models/download_bertimbau_large.ipynb`\n",
    "\n",
    "**Kaggle Models:** Add Input â†’ Models â†’ `bertimbau-ptbr-complete`\n",
    "\n",
    "---\n",
    "**CONFIGURAÃ‡ÃƒO KAGGLE:**\n",
    "1. Settings â†’ Internet â†’ **OFF**\n",
    "2. Settings â†’ Accelerator â†’ **GPU T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - BERTIMBAU v4 (THRESHOLD TUNING APENAS) =====\n",
    "\n",
    "print(\"[1/7] Configurando ambiente...\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== CONFIGURAÃ‡Ã•ES ORIGINAIS - NÃƒO ALTERAR ==========\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "# ===========================================================\n",
    "\n",
    "# ========== THRESHOLD TUNING - AJUSTAR AQUI ==========\n",
    "USE_THRESHOLD_TUNING = True\n",
    "THRESHOLDS = {\n",
    "    0: 0.50,\n",
    "    1: 0.50,\n",
    "    2: 0.50,\n",
    "    3: 0.50,\n",
    "    4: 0.50,\n",
    "    5: 0.30,   # Classe 5 - mais sensÃ­vel\n",
    "    6: 0.25,   # Classe 6 - muito mais sensÃ­vel\n",
    "}\n",
    "# ====================================================\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# ========== VERIFICAR DATASET PRIMEIRO ==========\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ERRO: Dataset nÃ£o encontrado!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nAdicione o dataset:\")\n",
    "    print(\"Add Input â†’ Competition â†’ spr-2026-mammography-report-classification\")\n",
    "    raise FileNotFoundError(f\"Dataset nÃ£o encontrado: {DATA_DIR}\")\n",
    "print(f\"Dataset: {DATA_DIR}\")\n",
    "\n",
    "# Auto-detectar modelo\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def has_config(path):\n",
    "        return os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json'))\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if has_config(path):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"Adicione o modelo BERTimbau ao notebook!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1342fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss - CONFIGURAÃ‡ÃƒO ORIGINAL\n",
    "print(\"[2/7] Definindo Focal Loss (config original)...\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "print(f'Focal Loss: gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "print(\"[3/7] Carregando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0672e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "print(\"[4/7] Preparando datasets...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values, train_df['target'].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df['target']\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df['report'].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e treinar modelo\n",
    "print(\"[5/7] Carregando e treinando modelo...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH, num_labels=NUM_CLASSES, local_files_only=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}'):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # ValidaÃ§Ã£o\n",
    "    model.eval()\n",
    "    val_preds, val_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(batch['labels'].numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_true, val_preds, average='macro')\n",
    "    print(f'Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, Val F1={val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "\n",
    "print(f'\\nMelhor Val F1: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InferÃªncia com threshold tuning\n",
    "print(\"[6/7] InferÃªncia com threshold tuning...\")\n",
    "\n",
    "model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='InferÃªncia'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "print(f\"Probabilidades shape: {all_probs.shape}\")\n",
    "\n",
    "# Aplicar threshold tuning\n",
    "if USE_THRESHOLD_TUNING:\n",
    "    print(\"\\nAplicando threshold tuning...\")\n",
    "    print(\"Thresholds:\")\n",
    "    for c, t in THRESHOLDS.items():\n",
    "        print(f\"  Classe {c}: {t}\")\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(all_probs)):\n",
    "        adj_probs = all_probs[i].copy()\n",
    "        for c, t in THRESHOLDS.items():\n",
    "            if c < len(adj_probs):\n",
    "                adj_probs[c] *= (0.5 / t)\n",
    "        predictions.append(np.argmax(adj_probs))\n",
    "    predictions = np.array(predictions)\n",
    "else:\n",
    "    predictions = np.argmax(all_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubmissÃ£o\n",
    "print(\"[7/7] Gerando submissÃ£o...\")\n",
    "\n",
    "sample_path = f'{DATA_DIR}/sample_submission.csv'\n",
    "if os.path.exists(sample_path):\n",
    "    sample_sub = pd.read_csv(sample_path)\n",
    "    SUB_ID = sample_sub.columns[0]\n",
    "    SUB_LABEL = sample_sub.columns[1]\n",
    "else:\n",
    "    SUB_ID = 'ID'\n",
    "    SUB_LABEL = 'target'\n",
    "\n",
    "submission = pd.DataFrame({SUB_ID: test_df['ID'], SUB_LABEL: predictions})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BERTimbau v4 CONCLUÃDO!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMelhor Val F1: {best_f1:.4f}\")\n",
    "print(f\"Threshold tuning: {'ATIVO' if USE_THRESHOLD_TUNING else 'DESATIVADO'}\")\n",
    "print(\"\\nDistribuiÃ§Ã£o das prediÃ§Ãµes:\")\n",
    "print(submission[SUB_LABEL].value_counts().sort_index())\n",
    "print(\"\\nâœ… submission.csv criado!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
