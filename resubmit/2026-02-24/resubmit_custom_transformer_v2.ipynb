{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c952d9a",
   "metadata": {},
   "source": [
    "# SPR 2026 - Custom Transformer Encoder v2\n",
    "\n",
    "**Melhorias sobre v1 (0.77272):**\n",
    "\n",
    "- ‚úÖ **Focal Loss** (Œ≥=2) - foca nas classes dif√≠ceis\n",
    "- ‚úÖ Mais epochs (15 vs 10)\n",
    "- ‚úÖ Learning rate scheduler com warmup\n",
    "- ‚úÖ Label smoothing (0.1)\n",
    "- ‚úÖ Gradient accumulation\n",
    "\n",
    "**Objetivo: superar 0.77272 (competir com BERTimbau)**\n",
    "\n",
    "---\n",
    "**CONFIGURA√á√ÉO KAGGLE:**\n",
    "1. Settings ‚Üí Internet ‚Üí **OFF**\n",
    "2. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "3. **N√£o precisa de nenhum modelo externo!**\n",
    "4. **IMPORTANTE:** Execute \"Run All\" ap√≥s commit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ca7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPR 2026 - CUSTOM TRANSFORMER v2 (COM FOCAL LOSS)\n",
    "# =============================================================================\n",
    "# Melhorias:\n",
    "# - Focal Loss para classes desbalanceadas\n",
    "# - Label smoothing\n",
    "# - Mais epochs + scheduler\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15           # Aumentado de 10\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 7\n",
    "LABEL_SMOOTHING = 0.1  # Novo\n",
    "FOCAL_GAMMA = 2.0      # Novo\n",
    "\n",
    "# Transformer Config\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "N_ENCODER_LAYERS = 4\n",
    "D_FF = 512\n",
    "DROPOUT = 0.1\n",
    "MIN_FREQ = 2\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FOCAL LOSS (CHAVE PARA CLASSES DESBALANCEADAS)\n",
    "# =============================================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss para lidar com desbalanceamento de classes.\n",
    "    \n",
    "    Refer√™ncia: Lin et al., \"Focal Loss for Dense Object Detection\"\n",
    "    \n",
    "    Foca nos exemplos dif√≠ceis, reduzindo peso de exemplos f√°ceis.\n",
    "    gamma=0 √© equivalente a CrossEntropy.\n",
    "    gamma=2 √© o valor recomendado.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, label_smoothing=0.0, weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        # Cross entropy com label smoothing\n",
    "        ce_loss = F.cross_entropy(\n",
    "            input, target, \n",
    "            weight=self.weight,\n",
    "            label_smoothing=self.label_smoothing,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Probabilidade da classe correta\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal weight: (1 - pt)^gamma\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Focal loss\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "print('Focal Loss definido!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR DADOS\n",
    "# =============================================================================\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test: {test_df.shape}')\n",
    "\n",
    "# Calcular class weights para Focal Loss\n",
    "class_counts = train_df['target'].value_counts().sort_index().values\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f'\\nClass weights: {class_weights.cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOKENIZER SIMPLES (DO ZERO)\n",
    "# =============================================================================\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Tokenizer baseado em palavras, constru√≠do do dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=2):\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        self.pad_token = '[PAD]'\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "        \n",
    "    def _tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r'[a-z√°√†√¢√£√©√®√™√≠√Ø√≥√¥√µ√∫√º√ß√±]+|[0-9]+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self._tokenize(text)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        self.word2idx = {self.pad_token: 0, self.unk_token: 1}\n",
    "        \n",
    "        for word, freq in counter.most_common():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "        \n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "        print(f'Vocabul√°rio: {self.vocab_size} tokens')\n",
    "        \n",
    "    def encode(self, text, max_length=256):\n",
    "        tokens = self._tokenize(text)\n",
    "        ids = [self.word2idx.get(t, self.unk_token_id) for t in tokens]\n",
    "        \n",
    "        if len(ids) > max_length:\n",
    "            ids = ids[:max_length]\n",
    "        \n",
    "        attention_mask = [1] * len(ids)\n",
    "        \n",
    "        padding_length = max_length - len(ids)\n",
    "        ids = ids + [self.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        \n",
    "        return {'input_ids': ids, 'attention_mask': attention_mask}\n",
    "\n",
    "tokenizer = SimpleTokenizer(min_freq=MIN_FREQ)\n",
    "tokenizer.fit(train_df['report'].tolist())\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_IDX = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split treino/valida√ß√£o\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values,\n",
    "    train_df['target'].values,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['target']\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_texts)}, Val: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer.encode(self.texts[idx], max_length=MAX_LEN)\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_df['report'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064093a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELO TRANSFORMER CUSTOMIZADO\n",
    "# =============================================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ExtraSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=mask)\n",
    "        x = self.norm(x + self.dropout(attn_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_encoder_layers=4,\n",
    "                 d_ff=512, num_classes=7, max_len=256, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_encoder_layers)\n",
    "        self.extra_attention = ExtraSelfAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        x = self.extra_attention(x, mask=padding_mask)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "            x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "model = TransformerEncoderClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_encoder_layers=N_ENCODER_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    max_len=MAX_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TREINAMENTO COM FOCAL LOSS + SCHEDULER\n",
    "# =============================================================================\n",
    "criterion = FocalLoss(gamma=FOCAL_GAMMA, label_smoothing=LABEL_SMOOTHING, weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "# Scheduler com warmup\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / warmup_steps\n",
    "    return max(0.1, 1 - (step - warmup_steps) / (total_steps - warmup_steps))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(f'Focal Loss: gamma={FOCAL_GAMMA}, label_smoothing={LABEL_SMOOTHING}')\n",
    "print(f'Scheduler: {warmup_steps} warmup steps, {total_steps} total steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e87875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), f1_score(targets, preds, average='macro')\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return f1_score(targets, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a845d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS} | LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f'‚úÖ Novo melhor modelo! F1: {best_f1:.4f}')\n",
    "\n",
    "print(f'\\nüèÜ Melhor F1: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b556962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREDI√á√ïES\n",
    "# =============================================================================\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Submiss√£o\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('‚úÖ submission.csv criado!')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
