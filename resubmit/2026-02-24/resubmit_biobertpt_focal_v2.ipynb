{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28a976b",
   "metadata": {},
   "source": [
    "# SPR 2026 - BioBERTpt + Focal Loss v2\n",
    "\n",
    "**Melhorias sobre v1 (0.72480):**\n",
    "\n",
    "- ‚úÖ **Focal Loss** (Œ≥=2) - mesma estrat√©gia do BERTimbau campe√£o\n",
    "- ‚úÖ Mais epochs (7 vs 5)\n",
    "- ‚úÖ Learning rate ajustado (3e-5)\n",
    "- ‚úÖ Warmup ratio aumentado (15%)\n",
    "- ‚úÖ Gradient accumulation (2 steps)\n",
    "\n",
    "**Objetivo: superar 0.72480 (competir com top 5)**\n",
    "\n",
    "---\n",
    "## üì• MODELO NECESS√ÅRIO\n",
    "\n",
    "**Notebook de download:**\n",
    "1. Rode `models/bert/download_biobertpt.ipynb` no Kaggle com **Internet ON**\n",
    "2. Save Version ‚Üí Save & Run All (Commit)\n",
    "3. Aqui: Add Input ‚Üí **Your Work** ‚Üí selecione o notebook `download-biobertpt`\n",
    "\n",
    "---\n",
    "**CONFIGURA√á√ÉO KAGGLE:**\n",
    "1. Settings ‚Üí Internet ‚Üí **OFF**\n",
    "2. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "3. Add Input ‚Üí adicione o modelo (Your Work)\n",
    "4. **IMPORTANTE:** Execute \"Run All\" ap√≥s commit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff737a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION = 2  # Efetivo: batch_size * 2 = 32\n",
    "EPOCHS = 7                  # Aumentado de 5\n",
    "LR = 3e-5                   # Aumentado de 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "WARMUP_RATIO = 0.15\n",
    "\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "\n",
    "# Busca autom√°tica do modelo\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def has_config(path):\n",
    "        return os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json'))\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if has_config(path):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f'Modelo encontrado: {MODEL_PATH}')\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"Adicione o modelo via Add Input ‚Üí Your Work!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff260f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FOCAL LOSS (CHAVE DO SUCESSO DO BERTimbau!)\n",
    "# =============================================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss - mesma implementa√ß√£o usada no BERTimbau campe√£o (0.79696)\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "print('‚úÖ Focal Loss (Œ≥=2) configurado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test: {test_df.shape}')\n",
    "print(f'\\nDistribui√ß√£o:')\n",
    "print(train_df['target'].value_counts().sort_index())\n",
    "\n",
    "# Calcular class weights\n",
    "class_counts = train_df['target'].value_counts().sort_index().values\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f'\\nClass weights: {class_weights.cpu().numpy().round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49693d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values,\n",
    "    train_df['target'].values,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['target']\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_texts)}, Val: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer e Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_df['report'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo BioBERTpt\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer e Scheduler\n",
    "criterion = FocalLoss(gamma=FOCAL_GAMMA, weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "total_steps = (len(train_loader) // GRADIENT_ACCUMULATION) * EPOCHS\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'Total steps: {total_steps}, Warmup: {warmup_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(loader, desc='Training')):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        preds.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), f1_score(targets, preds, average='macro')\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return f1_score(targets, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    \n",
    "    train_loss, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, GRADIENT_ACCUMULATION\n",
    "    )\n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f'‚úÖ Novo melhor modelo! F1: {best_f1:.4f}')\n",
    "\n",
    "print(f'\\nüèÜ Melhor F1: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba656929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predi√ß√µes\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Submiss√£o\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('‚úÖ submission.csv criado!')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
