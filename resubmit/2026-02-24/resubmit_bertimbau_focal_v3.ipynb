{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "948b7d67",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau + Focal Loss v3\n",
    "\n",
    "**Vers√£o melhorada do melhor modelo (0.79696)**\n",
    "\n",
    "Estrat√©gias adicionais:\n",
    "- ‚úÖ Focal Loss com alpha por classe (pesos baseados em frequ√™ncia)\n",
    "- ‚úÖ Threshold tuning por classe (0.35 para classes 5 e 6)\n",
    "- ‚úÖ Label Smoothing (0.1)\n",
    "- ‚úÖ Gradient accumulation (2 steps)\n",
    "\n",
    "---\n",
    "## üì• MODELO - `models/download_bertimbau.ipynb` ou `download_bertimbau_large.ipynb`\n",
    "\n",
    "**Kaggle Models:** Add Input ‚Üí Models ‚Üí `bertimbau-ptbr-complete` (fabianofilho)\n",
    "\n",
    "---\n",
    "**CONFIGURA√á√ÉO KAGGLE:**\n",
    "1. Settings ‚Üí Internet ‚Üí **OFF**\n",
    "2. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FLAGS DE CONFIGURA√á√ÉO =====\n",
    "USE_CLASS_ALPHA = True      # Alpha por classe (vs alpha fixo)\n",
    "USE_THRESHOLD_TUNING = True # Threshold diferente por classe\n",
    "USE_LABEL_SMOOTHING = True  # Label smoothing 0.1\n",
    "GRADIENT_ACCUMULATION = 2   # Accumulate gradients\n",
    "\n",
    "# ===== SETUP E IMPORTS =====\n",
    "print(\"[1/8] Configurando ambiente...\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "\n",
    "# ==== AUTO-DETECTAR PATH DO MODELO ====\n",
    "def find_model_path():\n",
    "    \"\"\"Encontra automaticamente o path do modelo.\"\"\"\n",
    "    base = '/kaggle/input'\n",
    "    \n",
    "    def has_config(path):\n",
    "        return os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json'))\n",
    "    \n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if has_config(path):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"Adicione o modelo BERTimbau ao notebook!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS ====\n",
    "print(\"[2/8] Carregando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')\n",
    "\n",
    "# Calcular frequ√™ncia das classes para alpha\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(f'\\nDistribui√ß√£o das classes:\\n{class_counts}')\n",
    "\n",
    "# Alpha inversamente proporcional √† frequ√™ncia\n",
    "if USE_CLASS_ALPHA:\n",
    "    total = len(train_df)\n",
    "    class_alpha = torch.tensor([total / (NUM_CLASSES * c) for c in class_counts.values], dtype=torch.float32)\n",
    "    class_alpha = class_alpha / class_alpha.sum()  # Normalizar\n",
    "    print(f'\\nAlpha por classe: {class_alpha.numpy().round(3)}')\n",
    "else:\n",
    "    class_alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FOCAL LOSS COM ALPHA POR CLASSE ====\n",
    "print(\"[3/8] Definindo Focal Loss...\")\n",
    "\n",
    "class FocalLossWeighted(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # tensor [num_classes] ou None\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Label smoothing\n",
    "        if self.label_smoothing > 0:\n",
    "            n_classes = inputs.size(-1)\n",
    "            targets_smooth = torch.zeros_like(inputs).scatter_(\n",
    "                1, targets.unsqueeze(1), 1.0\n",
    "            )\n",
    "            targets_smooth = targets_smooth * (1 - self.label_smoothing) + self.label_smoothing / n_classes\n",
    "            ce_loss = -(targets_smooth * F.log_softmax(inputs, dim=-1)).sum(dim=-1)\n",
    "        else:\n",
    "            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        # Alpha per class\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.to(inputs.device)[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "label_smooth = 0.1 if USE_LABEL_SMOOTHING else 0.0\n",
    "criterion = FocalLossWeighted(alpha=class_alpha, gamma=FOCAL_GAMMA, label_smoothing=label_smooth)\n",
    "print(f'Focal Loss: gamma={FOCAL_GAMMA}, label_smoothing={label_smooth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATASET CLASS ====\n",
    "print(\"[4/8] Preparando dataset...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "print('Tokenizer carregado!')\n",
    "\n",
    "# Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values, train_df['target'].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df['target']\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df['report'].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== MODELO ====\n",
    "print(\"[5/8] Carregando modelo...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    local_files_only=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'Modelo carregado! Parametros: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae90d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== THRESHOLD TUNING ====\n",
    "if USE_THRESHOLD_TUNING:\n",
    "    # Classes 5 e 6 s√£o minorit√°rias - threshold mais baixo para serem mais sens√≠veis\n",
    "    CLASS_THRESHOLDS = {0: 0.5, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.35, 6: 0.35}\n",
    "    print(f'Thresholds por classe: {CLASS_THRESHOLDS}')\n",
    "\n",
    "def predict_with_threshold(logits, thresholds=None):\n",
    "    \"\"\"Predi√ß√£o com threshold customizado por classe.\"\"\"\n",
    "    if thresholds is None:\n",
    "        return logits.argmax(dim=1)\n",
    "    \n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    preds = []\n",
    "    for prob in probs:\n",
    "        # Para cada amostra, verificar se alguma classe minorit√°ria passa o threshold\n",
    "        pred = prob.argmax().item()\n",
    "        for cls, thresh in thresholds.items():\n",
    "            if prob[cls].item() >= thresh and thresh < 0.5:\n",
    "                if prob[cls].item() > prob[pred].item() * 0.7:  # Se est√° pr√≥ximo do m√°ximo\n",
    "                    pred = cls\n",
    "                    break\n",
    "        preds.append(pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAMENTO ====\n",
    "print(\"[6/8] Treinando modelo...\")\n",
    "\n",
    "def evaluate(model, loader, use_threshold=False):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_logits = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "            if 'labels' in batch:\n",
    "                all_labels.extend(batch['labels'].numpy())\n",
    "    \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    \n",
    "    if use_threshold and USE_THRESHOLD_TUNING:\n",
    "        all_preds = predict_with_threshold(all_logits, CLASS_THRESHOLDS)\n",
    "    else:\n",
    "        all_preds = all_logits.argmax(dim=1).numpy()\n",
    "    \n",
    "    if all_labels:\n",
    "        return f1_score(all_labels, all_preds, average='macro'), all_preds\n",
    "    return all_preds\n",
    "\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    \n",
    "    for step, batch in enumerate(pbar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels) / GRADIENT_ACCUMULATION\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION\n",
    "        \n",
    "        if (step + 1) % GRADIENT_ACCUMULATION == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item() * GRADIENT_ACCUMULATION:.4f}'})\n",
    "    \n",
    "    # Avaliar com e sem threshold\n",
    "    val_f1, _ = evaluate(model, val_loader, use_threshold=False)\n",
    "    val_f1_thresh, _ = evaluate(model, val_loader, use_threshold=True)\n",
    "    print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Val F1={val_f1:.4f}, F1+Thresh={val_f1_thresh:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_model.pt')\n",
    "        print(f'  -> Melhor modelo salvo! F1={best_f1:.4f}')\n",
    "\n",
    "print(f'\\nMelhor F1 valida√ß√£o: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c373221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PREDI√á√ÉO ====\n",
    "print(\"[7/8] Gerando predi√ß√µes...\")\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_model.pt'))\n",
    "predictions = evaluate(model, test_loader, use_threshold=USE_THRESHOLD_TUNING)\n",
    "print(f'Predi√ß√µes geradas: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISS√ÉO ====\n",
    "print(\"[8/8] Criando submiss√£o...\")\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print('‚úÖ Submiss√£o salva!')\n",
    "print(f'\\nDistribui√ß√£o das predi√ß√µes:')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
