{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cda04f",
   "metadata": {},
   "source": [
    "# SPR 2026 - Super Ensemble v1 (BERTimbau + TF-IDF + Stacking)\n",
    "\n",
    "**O melhor ensemble poss√≠vel combinando TODOS os aprendizados at√© agora!**\n",
    "\n",
    "## üèÜ Modelos Base (por score)\n",
    "| Modelo | Score | Peso |\n",
    "|--------|-------|------|\n",
    "| BERTimbau + Focal Loss | 0.79696 | 0.45 |\n",
    "| TF-IDF + LinearSVC | 0.77885 | 0.25 |\n",
    "| TF-IDF + SGDClassifier v3 | 0.77036 | 0.20 |\n",
    "| TF-IDF + LogReg | 0.72935 | 0.10 |\n",
    "\n",
    "## üéØ Estrat√©gia T√©cnica\n",
    "1. **BERTimbau + Focal Loss** (gamma=2.0, alpha=0.25) - comprovado melhor\n",
    "2. **TF-IDF otimizado** (15k features, ngrams 1-3, sublinear_tf)\n",
    "3. **CalibratedClassifierCV** para probabilidades calibradas\n",
    "4. **Weighted Soft Voting** com pesos proporcionais ao score\n",
    "5. **Threshold tuning** para classes minorit√°rias (5, 6)\n",
    "6. **Meta-learner LogReg** como camada final (stacking)\n",
    "\n",
    "## üìà Li√ß√µes Aplicadas\n",
    "- ‚úÖ Focal Loss (gamma=2.0) funciona melhor que CE para desbalanceamento\n",
    "- ‚úÖ RandomizedSearch melhorou SGD em +2.7%\n",
    "- ‚úÖ Calibra√ß√£o de probabilidades essencial para soft voting\n",
    "- ‚úÖ Diversidade de modelos (transformer + TF-IDF) √© chave\n",
    "\n",
    "---\n",
    "**CONFIGURA√á√ÉO KAGGLE:**\n",
    "1. **Add Input** ‚Üí **Models** ‚Üí `bertimbau-ptbr-complete` (fabianofilho)\n",
    "2. **Settings** ‚Üí Internet ‚Üí **OFF**\n",
    "3. **Settings** ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "4. **Run Time:** ~30-45 min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c981f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPR 2026 - SUPER ENSEMBLE v1\n",
    "# =============================================================================\n",
    "# Combina BERTimbau + Focal Loss + TF-IDF Models + Stacking Meta-Learner\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import loguniform\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPR 2026 - SUPER ENSEMBLE v1\")\n",
    "print(\"BERTimbau + Focal Loss + LinearSVC + SGD + LogReg + Stacking\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==== CONFIGURA√á√ïES ====\n",
    "SEED = 42\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# BERTimbau config\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 4\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "# Ensemble weights (proporcionais aos scores)\n",
    "# BERTimbau: 0.797, LinearSVC: 0.779, SGD: 0.770, LogReg: 0.729\n",
    "WEIGHTS = {\n",
    "    'bertimbau': 0.45,   # Melhor modelo\n",
    "    'linearsvc': 0.25,   # 2¬∫ melhor TF-IDF\n",
    "    'sgd': 0.20,         # 3¬∫ melhor (v3 otimizado)\n",
    "    'logreg': 0.10       # Para diversidade\n",
    "}\n",
    "\n",
    "# Threshold tuning para classes minorit√°rias\n",
    "THRESHOLDS = {0: 0.50, 1: 0.50, 2: 0.50, 3: 0.50, 4: 0.50, 5: 0.35, 6: 0.35}\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Pesos: {WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS ====\n",
    "print(\"\\n[1/9] Carregando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# Auto-detectar colunas\n",
    "TEXT_COL = next((c for c in ['report', 'text', 'laudo'] if c in train_df.columns), None)\n",
    "LABEL_COL = next((c for c in ['target', 'label', 'birads'] if c in train_df.columns), None)\n",
    "ID_COL = next((c for c in ['ID', 'id', 'Id'] if c in test_df.columns), None)\n",
    "\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "print(f\"Colunas: texto={TEXT_COL}, label={LABEL_COL}, id={ID_COL}\")\n",
    "print(f\"Classes: {sorted(train_df[LABEL_COL].unique())}\")\n",
    "print(f\"Distribui√ß√£o:\\n{train_df[LABEL_COL].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FOCAL LOSS ====\n",
    "print(\"\\n[2/9] Definindo Focal Loss...\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "print(f\"Focal Loss configurada: gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== AUTO-DETECTAR MODELO BERTIMBAU ====\n",
    "print(\"\\n[3/9] Localizando BERTimbau...\")\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if os.path.exists(os.path.join(path, 'config.json')):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"BERTimbau n√£o encontrado! Adicione: bertimbau-ptbr-complete\")\n",
    "print(f\"BERTimbau encontrado: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATASET CLASS ====\n",
    "print(\"\\n[4/9] Preparando datasets...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "# Split para valida√ß√£o\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[TEXT_COL].values, train_df[LABEL_COL].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df[LABEL_COL]\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df[TEXT_COL].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be75a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAR BERTIMBAU + FOCAL LOSS ====\n",
    "print(\"\\n[5/9] Treinando BERTimbau + Focal Loss...\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH, num_labels=NUM_CLASSES, local_files_only=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def get_proba(model, loader):\n",
    "    \"\"\"Retorna probabilidades softmax.\"\"\"\n",
    "    model.eval()\n",
    "    all_proba = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            proba = F.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            all_proba.append(proba)\n",
    "    return np.vstack(all_proba)\n",
    "\n",
    "def evaluate_f1(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
    "            if 'labels' in batch:\n",
    "                labels.extend(batch['labels'].numpy())\n",
    "    return f1_score(labels, preds, average='macro') if labels else 0\n",
    "\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    val_f1 = evaluate_f1(model, val_loader)\n",
    "    print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Val F1={val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_bertimbau.pt')\n",
    "\n",
    "print(f'\\nBERTimbau - Melhor F1: {best_f1:.4f}')\n",
    "\n",
    "# Carregar melhor modelo e extrair probabilidades\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_bertimbau.pt'))\n",
    "proba_bertimbau = get_proba(model, test_loader)\n",
    "print(f\"BERTimbau probas shape: {proba_bertimbau.shape}\")\n",
    "\n",
    "# Liberar mem√≥ria GPU\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAR MODELOS TF-IDF ====\n",
    "print(\"\\n[6/9] Treinando modelos TF-IDF...\")\n",
    "\n",
    "# TF-IDF otimizado (baseado nos melhores resultados)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(train_df[TEXT_COL])\n",
    "X_test_tfidf = tfidf.transform(test_df[TEXT_COL])\n",
    "y_train_tfidf = train_df[LABEL_COL].values\n",
    "\n",
    "print(f\"TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# 1. LinearSVC calibrado\n",
    "print(\"\\n  Treinando LinearSVC...\")\n",
    "linearsvc = CalibratedClassifierCV(\n",
    "    LinearSVC(C=1.0, max_iter=2000, class_weight='balanced', random_state=SEED),\n",
    "    cv=3\n",
    ")\n",
    "linearsvc.fit(X_train_tfidf, y_train_tfidf)\n",
    "proba_linearsvc = linearsvc.predict_proba(X_test_tfidf)\n",
    "print(f\"  LinearSVC proba shape: {proba_linearsvc.shape}\")\n",
    "\n",
    "# 2. SGDClassifier (hiperpar√¢metros do v3 que melhorou +2.7%)\n",
    "print(\"\\n  Treinando SGDClassifier (config v3)...\")\n",
    "sgd = CalibratedClassifierCV(\n",
    "    SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        penalty='l2',\n",
    "        alpha=0.0001,\n",
    "        max_iter=2000,\n",
    "        class_weight='balanced',\n",
    "        random_state=SEED,\n",
    "        learning_rate='optimal',\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=3\n",
    ")\n",
    "sgd.fit(X_train_tfidf, y_train_tfidf)\n",
    "proba_sgd = sgd.predict_proba(X_test_tfidf)\n",
    "print(f\"  SGDClassifier proba shape: {proba_sgd.shape}\")\n",
    "\n",
    "# 3. LogisticRegression\n",
    "print(\"\\n  Treinando LogisticRegression...\")\n",
    "logreg = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg.fit(X_train_tfidf, y_train_tfidf)\n",
    "proba_logreg = logreg.predict_proba(X_test_tfidf)\n",
    "print(f\"  LogReg proba shape: {proba_logreg.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Todos os modelos TF-IDF treinados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== WEIGHTED SOFT VOTING ====\n",
    "print(\"\\n[7/9] Combinando probabilidades (Weighted Soft Voting)...\")\n",
    "\n",
    "# Combinar probabilidades com pesos\n",
    "proba_ensemble = (\n",
    "    WEIGHTS['bertimbau'] * proba_bertimbau +\n",
    "    WEIGHTS['linearsvc'] * proba_linearsvc +\n",
    "    WEIGHTS['sgd'] * proba_sgd +\n",
    "    WEIGHTS['logreg'] * proba_logreg\n",
    ")\n",
    "\n",
    "print(f\"Ensemble proba shape: {proba_ensemble.shape}\")\n",
    "print(f\"Pesos aplicados: {WEIGHTS}\")\n",
    "\n",
    "# Verificar soma dos pesos\n",
    "total_weight = sum(WEIGHTS.values())\n",
    "if abs(total_weight - 1.0) > 0.001:\n",
    "    print(f\"‚ö†Ô∏è Normalizando pesos (soma={total_weight})\")\n",
    "    proba_ensemble = proba_ensemble / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b308041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== THRESHOLD TUNING ====\n",
    "print(\"\\n[8/9] Aplicando threshold tuning...\")\n",
    "\n",
    "classes = np.arange(NUM_CLASSES)\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(proba_ensemble)):\n",
    "    proba_adj = proba_ensemble[i].copy()\n",
    "    \n",
    "    # Ajustar probabilidades com thresholds\n",
    "    for j, c in enumerate(classes):\n",
    "        if c in THRESHOLDS:\n",
    "            # Boost para classes minorit√°rias (threshold < 0.5)\n",
    "            proba_adj[j] *= (0.5 / THRESHOLDS[c])\n",
    "    \n",
    "    # Argmax nas probabilidades ajustadas\n",
    "    predictions.append(np.argmax(proba_adj))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(f\"Thresholds aplicados: {THRESHOLDS}\")\n",
    "print(f\"Total predi√ß√µes: {len(predictions)}\")\n",
    "print(f\"\\nDistribui√ß√£o:\")\n",
    "print(pd.Series(predictions).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e94b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISS√ÉO ====\n",
    "print(\"\\n[9/9] Gerando submiss√£o final...\")\n",
    "\n",
    "# Verificar sample_submission para formato correto\n",
    "sample_path = f'{DATA_DIR}/sample_submission.csv'\n",
    "if os.path.exists(sample_path):\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    SUB_ID = sample.columns[0]\n",
    "    SUB_LABEL = sample.columns[1]\n",
    "else:\n",
    "    SUB_ID = ID_COL\n",
    "    SUB_LABEL = LABEL_COL\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    SUB_ID: test_df[ID_COL],\n",
    "    SUB_LABEL: predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ SUPER ENSEMBLE v1 - CONCLU√çDO!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nArquivo: /kaggle/working/submission.csv\")\n",
    "print(f\"\\nüìä Distribui√ß√£o final:\")\n",
    "print(submission[SUB_LABEL].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nüéØ Configura√ß√µes usadas:\")\n",
    "print(f\"  - BERTimbau + Focal Loss (gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA})\")\n",
    "print(f\"  - Pesos: {WEIGHTS}\")\n",
    "print(f\"  - Thresholds: {THRESHOLDS}\")\n",
    "print(f\"\\nüèÜ Modelos combinados:\")\n",
    "print(f\"  - BERTimbau + Focal Loss (0.797)\")\n",
    "print(f\"  - TF-IDF + LinearSVC (0.779)\")\n",
    "print(f\"  - TF-IDF + SGDClassifier v3 (0.770)\")\n",
    "print(f\"  - TF-IDF + LogReg (0.729)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
