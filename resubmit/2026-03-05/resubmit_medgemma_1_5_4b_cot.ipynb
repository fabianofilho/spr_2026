{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a09a66",
   "metadata": {},
   "source": [
    "# SPR 2026 - MedGemma 1.5 4B Chain-of-Thought\n",
    "\n",
    "**Modelo:** google/medgemma-1.5-4b-it\n",
    "\n",
    "**Características:**\n",
    "- Chain-of-Thought reasoning antes da classificação\n",
    "- Modelo pensa passo-a-passo antes de responder\n",
    "- 4B parâmetros - cabe em T4\n",
    "\n",
    "**Hipótese:** Raciocínio explícito melhora a classificação\n",
    "\n",
    "---\n",
    "## CONFIGURAÇÃO KAGGLE:\n",
    "1. **Add Input** → **Models** → `medgemma-1.5-4b-it`\n",
    "2. **Add Input** → **Competition** → `spr-2026-mammography-report-classification`\n",
    "3. **Settings** → Internet → **OFF**, GPU → **T4 x2**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15473f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MEDGEMMA 1.5 4B COT =====\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - MedGemma 1.5 4B Chain-of-Thought\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SEED = 42\n",
    "MAX_NEW_TOKENS = 150  # Mais tokens para CoT\n",
    "DATA_DIR = '/kaggle/input/competitions/spr-2026-mammography-report-classification'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth: return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                    return path\n",
    "                result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                if result: return result\n",
    "        except: pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8233a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHAIN-OF-THOUGHT PROMPT =====\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a senior breast radiologist expert in BI-RADS classification.\n",
    "\n",
    "## BI-RADS Categories:\n",
    "- **0**: Incomplete - needs additional imaging\n",
    "- **1**: Negative - normal mammogram  \n",
    "- **2**: Benign - definitely benign findings (0% malignancy)\n",
    "- **3**: Probably Benign - <2% malignancy, 6-month follow-up\n",
    "- **4**: Suspicious - 2-95% malignancy, biopsy recommended\n",
    "- **5**: Highly Suggestive of Malignancy - >95% malignancy\n",
    "- **6**: Known Biopsy-Proven Malignancy\n",
    "\n",
    "## Instructions:\n",
    "Analyze the mammography report step by step:\n",
    "1. Identify key findings (masses, calcifications, distortions)\n",
    "2. Assess morphology and margins\n",
    "3. Consider associated features\n",
    "4. Determine the most appropriate BI-RADS category\n",
    "\n",
    "Think through your reasoning, then provide your final answer as:\n",
    "FINAL: [number]\"\"\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Mammography Report:\n",
    "{report}\n",
    "\n",
    "Analyze step by step and provide your BI-RADS classification:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2813cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR MODELO =====\n",
    "print(\"Carregando modelo...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, local_files_only=True,\n",
    "    torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Modelo: {model.config.architectures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07387ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c70beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CLASSIFICAÇÃO COM COT =====\n",
    "import re\n",
    "\n",
    "def classify_report(report, model, tokenizer):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(report=report)}\"}]\n",
    "    \n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(report=report)}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False,\n",
    "                                  pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Procura por FINAL: X primeiro\n",
    "    final_match = re.search(r'FINAL[:\\s]*([0-6])', response, re.IGNORECASE)\n",
    "    if final_match:\n",
    "        return int(final_match.group(1))\n",
    "    \n",
    "    # Fallback: último dígito 0-6 mencionado\n",
    "    digits = re.findall(r'[0-6]', response)\n",
    "    if digits:\n",
    "        return int(digits[-1])\n",
    "    \n",
    "    return 2\n",
    "\n",
    "# Teste\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"Real: {sample['target']}, Pred: {classify_report(sample['report'], model, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d125b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDAÇÃO =====\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "val_sample = train_df.groupby('target', group_keys=False).apply(lambda x: x.sample(min(20, len(x)), random_state=SEED))\n",
    "val_preds = [classify_report(row['report'], model, tokenizer) for _, row in tqdm(val_sample.iterrows(), total=len(val_sample))]\n",
    "val_labels = val_sample['target'].values\n",
    "\n",
    "print(f\"\\nF1-Macro: {f1_score(val_labels, val_preds, average='macro'):.5f}\")\n",
    "print(classification_report(val_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SUBMISSION =====\n",
    "test_preds = [classify_report(row['report'], model, tokenizer) for _, row in tqdm(test_df.iterrows(), total=len(test_df))]\n",
    "\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission salva!\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
