# NEXT.md - Pr√≥ximos Passos

> Baseado nos insights de 35+ submiss√µes (incluindo resubmiss√µes v2/v3)

## üìä Resumo do Estado Atual

| M√©trica | Valor |
|---------|-------|
| Melhor Score | **0.79696** (BERTimbau + Focal Loss) |
| 2¬∫ Melhor | 0.79505 (BERTimbau + Focal Loss v2) |
| üöÄ **√önica Melhoria** | **0.77036** (SGDClassifier v3, +2.7%) |
| Total Submiss√µes | 35+ |
| Submiss√µes com Falha | 7+ (LoRA, mDeBERTa, BioBERTpt v2, Custom v2, Qwen3, LLMs) |

---

## ‚úÖ O Que Funciona (Replicar)

### Modelos que Mantiveram/Melhoraram
1. **BERTimbau + Focal Loss** ‚Üí 0.79696 (CAMPE√ÉO)
2. **BERTimbau + Focal Loss v2** ‚Üí 0.79505 (99.8% do original)
3. **Ensemble Soft Voting** ‚Üí 0.78049 (baseline est√°vel)
4. **TF-IDF + LinearSVC** ‚Üí 0.77885 (baseline est√°vel)
5. üöÄ **SGDClassifier v3** ‚Üí **0.77036** (√öNICO QUE MELHOROU! +2.7%)

### T√©cnicas Comprovadas
- **Focal Loss** com Œ≥=2 funciona bem para classes desbalanceadas
- **RandomizedSearchCV** com 20+ iter ‚Üí SGD v3 melhorou 2.7%!
- **Class weights** como fallback para modelos cl√°ssicos
- **Soft Voting** entre modelos TF-IDF diversos
- **BERTimbau** supera modelos multilingual

---

## ‚ùå O Que N√£o Funciona (Evitar)

### Modelos que Falharam
| Modelo | Score | Motivo |
|--------|-------|--------|
| BioBERTpt + Focal v2 | 0.26099 | Focal Loss mal calibrada |
| Custom Transformer v2 | 0.41721 | Altera√ß√µes quebraram tokenizer |
| BERTimbau + LoRA | 0.13261 | Offline n√£o funciona |
| Qwen3 Zero-Shot | 0.13261 | LLM n√£o entende contexto m√©dico |
| Qwen3 One-Shot | 0.13261 | Mesmo com exemplo, n√£o funciona |
| mDeBERTa | 0.01008 | Bug fp16 no Kaggle |

### Resubmiss√µes que Regrediram
| Modelo | Baseline | Resubmit | Delta |
|--------|----------|----------|-------|
| LinearSVC v3 | 0.77885 | 0.75966 | -2.5% |
| LogisticRegression v3 | 0.72935 | 0.71303 | -2.2% |
| BERTimbau + Focal v3 | 0.79696 | 0.72625 | -8.9% |

### Anti-patterns
- ‚ö†Ô∏è **Muitas altera√ß√µes de uma vez** ‚Üí Quebraram 3 de 5 resubmiss√µes
- ‚ö†Ô∏è **LoRA offline** ‚Üí N√£o funciona no Kaggle
- ‚ö†Ô∏è **Modelos multilingual** ‚Üí ~30% piores que PT nativo
- ‚ö†Ô∏è **LLMs zero/one-shot** ‚Üí N√£o funcionam para este problema
- ‚ö†Ô∏è **RandomSearch em LinearSVC/LogReg** ‚Üí Regrediu, n√£o melhorou

---

## üéØ Pr√≥ximos Experimentos Priorit√°rios

### 1. SGDClassifier v4: Replicar Sucesso (ALTA PRIORIDADE)

**Hip√≥tese:** RandomSearch intensivo + SMOTE pode melhorar ainda mais

```python
N_SEARCH_ITER = 50          # vs 20 no v3 que j√° melhorou
USE_SMOTE = True            # Oversample classes 5/6 para 500 amostras
sampling_strategy = {5: 500, 6: 500}
```

**Cuidados:**
- Manter mesma seed e estrutura do v3
- SMOTE apenas no treino, nunca no val/test

---

### 2. Ensemble BERTimbau + SGD (ALTA PRIORIDADE)

**Hip√≥tese:** Combinar os 2 melhores pode superar 0.80

```python
# Weighted Blend
final_proba = 0.6 * bertimbau_proba + 0.4 * sgd_v3_proba
```

**Cuidados:**
- N√ÉO re-treinar BERTimbau, usar probabilidades salvas
- Testar pesos: 0.5/0.5, 0.6/0.4, 0.7/0.3

---

### 3. BERTimbau v4: Threshold Tuning Apenas (M√âDIA PRIORIDADE)

**Hip√≥tese:** Ajustar thresholds na infer√™ncia pode melhorar F1-Macro

```python
# N√ÉO MEXER NO MODELO - apenas p√≥s-processamento
thresholds = {
    0: 0.50, 1: 0.50, 2: 0.50, 
    3: 0.50, 4: 0.50, 
    5: 0.30,  # Mais sens√≠vel para classe minorit√°ria
    6: 0.25   # Muito mais sens√≠vel
}
```

**Cuidados:**
- Usar modelo EXATAMENTE como est√°
- Apenas ajustar thresholds na predi√ß√£o final

---

### 4. Focal Loss em Transformers (M√âDIA PRIORIDADE)

**Hip√≥tese:** Copiar config EXATA do BERTimbau em outros transformers

**Candidatos:**
| Modelo | Score Atual | Score Esperado |
|--------|-------------|----------------|
| XLM-RoBERTa | 0.68767 | ~0.74+ |
| ModernBERT | 0.68578 | ~0.74+ |

**Cuidados:**
- COPIAR EXATAMENTE config do BERTimbau, n√£o inventar
- Œ≥=2, sem altera√ß√µes
- Uma altera√ß√£o por vez

---

### 4. Pre-training com MLM M√©dico (BAIXA PRIORIDADE)

**Hip√≥tese:** Continuar pr√©-treino do BERTimbau com textos m√©dicos PT

**Datasets dispon√≠veis:**
- Medical Transcriptions
- PubMed 200k RCT
- CBIS-DDSM BI-RADS

**Notebook:** `tests/pretrain/submit_bertimbau_pretrain.ipynb`

**Cuidados:**
- Requer muito compute (2-4h no Kaggle P100)
- Risco de catastrophic forgetting
- Testar com learning rate menor (1e-5)

---

### 5. NER + Regras Determin√≠sticas (BAIXA PRIORIDADE)

**Motiva√ß√£o:** Alguns BI-RADS (0 e 6) t√™m padr√µes textuais √≥bvios

**Pipeline:**
1. Regex para extrair "BIRADS 0", "BIRADS 6" expl√≠citos
2. NER para identificar "nega√ß√£o" + "achado"
3. Fallback para modelo ML

**Notebook:** `tests/ner/submit_ner_rules.ipynb`

---

## üìã Checklist de Boas Pr√°ticas

### Antes de Submeter
- [ ] Testar localmente com CV (5-fold)
- [ ] Verificar seed fixa para reprodutibilidade
- [ ] Validar `local_files_only=True` para modelos HuggingFace
- [ ] Confirmar que n√£o h√° altera√ß√µes vs vers√£o que funcionou

### Durante Resubmiss√£o
- [ ] UMA altera√ß√£o por vez
- [ ] Copiar notebook original, n√£o editar
- [ ] Documentar exatamente o que mudou
- [ ] Comparar scores antes de submeter

### Debugging de Falhas
- [ ] Verificar logs do Kaggle para errors
- [ ] Testar com inference local
- [ ] Comparar tokeniza√ß√£o de inputs
- [ ] Verificar shapes dos outputs

---

## üìà Roadmap Sugerido

| Semana | Foco | Objetivo |
|--------|------|----------|
| 1 | Ensemble BERTimbau + TF-IDF | Superar 0.80 |
| 2 | Data Augmentation | +0.5-1% no F1-Macro |
| 3 | Focal Loss em outros modelos | Diversificar top 5 |
| 4 | Fine-tuning & Otimiza√ß√£o | Consolidar ganhos |

---

## üîë Conclus√µes Principais

1. **BERTimbau + Focal Loss √© o padr√£o ouro** - N√£o alterar sem necessidade
2. **Resubmiss√µes s√£o arriscadas** - 3 de 5 falharam (60%)
3. **Modelos PT nativos > Multilingual** - Sempre priorizar
4. **Focal Loss > Class Weights** - Para classes desbalanceadas
5. **Ensemble simples funciona** - Soft Voting supera Stacking
6. **TF-IDF ainda √© forte** - Baseline dif√≠cil de bater

---

*Criado em: 25/02/2026*
*Baseado em: 32 submiss√µes no Kaggle SPR 2026*
