# NEXT.md - Pr√≥ximos Passos

> Baseado nos insights de 32 submiss√µes (incluindo 5 resubmiss√µes)

## üìä Resumo do Estado Atual

| M√©trica | Valor |
|---------|-------|
| Melhor Score | **0.79696** (BERTimbau + Focal Loss) |
| 2¬∫ Melhor | 0.79505 (BERTimbau + Focal Loss v2) |
| Total Submiss√µes | 32 |
| Submiss√µes com Falha | 5 (LoRA, mDeBERTa, BioBERTpt v2, Custom v2) |

---

## ‚úÖ O Que Funciona (N√£o Mexer)

### Modelos Est√°veis
1. **BERTimbau + Focal Loss** ‚Üí 0.79696 (CAMPE√ÉO)
2. **BERTimbau + Focal Loss v2** ‚Üí 0.79505 (99.8% do original)
3. **Ensemble Soft Voting** ‚Üí 0.78049
4. **TF-IDF + LinearSVC** ‚Üí 0.77885

### T√©cnicas Comprovadas
- **Focal Loss** com Œ≥=2 funciona bem para classes desbalanceadas
- **Class weights** como fallback para modelos cl√°ssicos
- **Soft Voting** entre modelos TF-IDF diversos
- **BERTimbau** supera modelos multilingual

---

## ‚ùå O Que N√£o Funciona (Evitar)

### Modelos que Falharam
| Modelo | Score | Motivo |
|--------|-------|--------|
| BioBERTpt + Focal v2 | 0.26099 | Focal Loss mal calibrada |
| Custom Transformer v2 | 0.41721 | Altera√ß√µes quebraram tokenizer |
| BERTimbau + LoRA | 0.13261 | Offline n√£o funciona |
| mDeBERTa | 0.01008 | Bug fp16 no Kaggle |

### Anti-patterns
- ‚ö†Ô∏è **Muitas altera√ß√µes de uma vez** ‚Üí Quebraram 3 de 5 resubmiss√µes
- ‚ö†Ô∏è **LoRA offline** ‚Üí N√£o funciona no Kaggle
- ‚ö†Ô∏è **Modelos multilingual** ‚Üí ~30% piores que PT nativo
- ‚ö†Ô∏è **Word2Vec m√©dia** ‚Üí Dilui informa√ß√£o discriminativa

---

## üéØ Pr√≥ximos Experimentos Priorit√°rios

### 1. Ensemble com BERTimbau (ALTA PRIORIDADE)

**Hip√≥tese:** Combinar BERTimbau + Focal com TF-IDF Ensemble pode superar 0.80

```python
# Proposta: Weighted Blend
final_pred = 0.6 * bertimbau_probs + 0.4 * tfidf_ensemble_probs
```

**Notebook:** `submit/ensemble/submit_ensemble_bertimbau_tfidf.ipynb`

**Cuidados:**
- Usar mesma seed do BERTimbau original
- N√£o alterar hiperpar√¢metros do Focal Loss
- Testar pesos: 0.5/0.5, 0.6/0.4, 0.7/0.3

---

### 2. Data Augmentation para Classes 5 e 6 (M√âDIA PRIORIDADE)

**Hip√≥tese:** Aumentar samples das classes minorit√°rias pode melhorar F1-Macro

**T√©cnicas a testar:**
- [ ] EDA (Easy Data Augmentation) - synonym replacement
- [ ] Back-translation PT‚ÜíEN‚ÜíPT
- [ ] SMOTE no espa√ßo de embeddings do BERTimbau

**Notebook:** `tests/augmented/submit_augmented_bertimbau.ipynb`

**Cuidados:**
- Augmentar APENAS treino, n√£o valida√ß√£o
- Monitorar overfitting nas classes aumentadas
- Usar augmentation conservadora (max 2x samples)

---

### 3. Focal Loss em Outros Modelos (M√âDIA PRIORIDADE)

**Hip√≥tese:** Focal Loss pode melhorar modelos abaixo de 0.78

**Candidatos:**
| Modelo | Score Atual | Score Esperado |
|--------|-------------|----------------|
| BioBERTpt | 0.72480 | ~0.76+ |
| XLM-RoBERTa | 0.68767 | ~0.72+ |
| ModernBERT | 0.68578 | ~0.72+ |

**Cuidados:**
- Copiar EXATAMENTE a configura√ß√£o do BERTimbau + Focal
- Œ≥=2, sem altera√ß√µes
- Uma altera√ß√£o por vez

---

### 4. Pre-training com MLM M√©dico (BAIXA PRIORIDADE)

**Hip√≥tese:** Continuar pr√©-treino do BERTimbau com textos m√©dicos PT

**Datasets dispon√≠veis:**
- Medical Transcriptions
- PubMed 200k RCT
- CBIS-DDSM BI-RADS

**Notebook:** `tests/pretrain/submit_bertimbau_pretrain.ipynb`

**Cuidados:**
- Requer muito compute (2-4h no Kaggle P100)
- Risco de catastrophic forgetting
- Testar com learning rate menor (1e-5)

---

### 5. NER + Regras Determin√≠sticas (BAIXA PRIORIDADE)

**Motiva√ß√£o:** Alguns BI-RADS (0 e 6) t√™m padr√µes textuais √≥bvios

**Pipeline:**
1. Regex para extrair "BIRADS 0", "BIRADS 6" expl√≠citos
2. NER para identificar "nega√ß√£o" + "achado"
3. Fallback para modelo ML

**Notebook:** `tests/ner/submit_ner_rules.ipynb`

---

## üìã Checklist de Boas Pr√°ticas

### Antes de Submeter
- [ ] Testar localmente com CV (5-fold)
- [ ] Verificar seed fixa para reprodutibilidade
- [ ] Validar `local_files_only=True` para modelos HuggingFace
- [ ] Confirmar que n√£o h√° altera√ß√µes vs vers√£o que funcionou

### Durante Resubmiss√£o
- [ ] UMA altera√ß√£o por vez
- [ ] Copiar notebook original, n√£o editar
- [ ] Documentar exatamente o que mudou
- [ ] Comparar scores antes de submeter

### Debugging de Falhas
- [ ] Verificar logs do Kaggle para errors
- [ ] Testar com inference local
- [ ] Comparar tokeniza√ß√£o de inputs
- [ ] Verificar shapes dos outputs

---

## üìà Roadmap Sugerido

| Semana | Foco | Objetivo |
|--------|------|----------|
| 1 | Ensemble BERTimbau + TF-IDF | Superar 0.80 |
| 2 | Data Augmentation | +0.5-1% no F1-Macro |
| 3 | Focal Loss em outros modelos | Diversificar top 5 |
| 4 | Fine-tuning & Otimiza√ß√£o | Consolidar ganhos |

---

## üîë Conclus√µes Principais

1. **BERTimbau + Focal Loss √© o padr√£o ouro** - N√£o alterar sem necessidade
2. **Resubmiss√µes s√£o arriscadas** - 3 de 5 falharam (60%)
3. **Modelos PT nativos > Multilingual** - Sempre priorizar
4. **Focal Loss > Class Weights** - Para classes desbalanceadas
5. **Ensemble simples funciona** - Soft Voting supera Stacking
6. **TF-IDF ainda √© forte** - Baseline dif√≠cil de bater

---

*Criado em: 25/02/2026*
*Baseado em: 32 submiss√µes no Kaggle SPR 2026*
