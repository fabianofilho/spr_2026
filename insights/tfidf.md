# TF-IDF - An√°lise Metodol√≥gica

## Resultados

| Rank | Modelo | Score | Observa√ß√£o |
|------|--------|-------|------------|
| 1 | LinearSVC | **0.77885** | üèÜ Melhor geral |
| 2 | SGDClassifier | 0.75019 | -0.029 |
| 3 | Logistic Regression | 0.72935 | -0.050 |
| 4 | LightGBM + SVD | 0.70273 | -0.076 |
| 5 | XGBoost | 0.69482 | -0.084 |
| 6 | SVD + XGBoost | 0.66897 | -0.110 |
| 7 | CatBoost | 0.48202 | -0.297 |
| 8 | TabPFN v0.1.9 | 0.39074 | -0.388 |

---

## Por que TF-IDF funciona t√£o bem?

### 1. Textos Curtos e T√©cnicos

O dataset cont√©m laudos m√©dicos com caracter√≠sticas:
- **Textos curtos:** poucas senten√ßas
- **Vocabul√°rio t√©cnico restrito:** termos BI-RADS, anatomia, patologia
- **Alta discriminatividade l√©xica:** presen√ßa de "benigno" vs "maligno" √© decisiva

TF-IDF captura exatamente isso: **quais palavras aparecem e com que frequ√™ncia**.

### 2. Esparsidade √© Feature, n√£o Bug

Em textos curtos t√©cnicos, a matriz esparsa do TF-IDF √© vantajosa:
- Cada termo m√©dico vira uma "feature bin√°ria" impl√≠cita
- Modelos lineares (SVM, LogReg) exploram isso diretamente
- N√£o h√° "ru√≠do sem√¢ntico" de embeddings densos

### 3. Classificadores Lineares Dominam

| Tipo | Melhor Score | Observa√ß√£o |
|------|--------------|------------|
| Linear (SVC, SGD, LogReg) | 0.77885 | ‚úÖ Domina |
| Ensemble (LGBM, XGB) | 0.70273 | ‚ùå Perde 7.6 pontos |
| Boosting + SVD | 0.66897 | ‚ùå SVD piora |

**Por qu√™?** 
- TF-IDF j√° √© linearmente separ√°vel neste problema
- Boosting adiciona complexidade desnecess√°ria
- SVD (redu√ß√£o de dimensionalidade) descarta informa√ß√£o √∫til

---

## An√°lise por Modelo

### LinearSVC (0.77885) üèÜ

**Por que √© o melhor:**
- SVM com kernel linear √© otimizado para dados esparsos de alta dimens√£o
- Margem m√°xima funciona bem quando classes s√£o linearmente separ√°veis
- Regulariza√ß√£o L2 impl√≠cita previne overfitting

**Configura√ß√£o prov√°vel ideal:**
```python
LinearSVC(C=1.0, max_iter=10000, class_weight='balanced')
```

### SGDClassifier (0.75019)

**Gap de 2.9 pontos:**
- SGD √© aproxima√ß√£o estoc√°stica do SVM
- Menos est√°vel, mais sens√≠vel a learning rate
- √ötil para datasets maiores, mas aqui perde para LinearSVC

### Logistic Regression (0.72935)

**Gap de 5 pontos:**
- Boa baseline, interpret√°vel
- Menos robusto que SVM para margens apertadas

### Modelos de Boosting (LGBM, XGBoost) 

**Por que perdem 7-8 pontos:**
1. Boosting √© √≥timo para features densas, n√£o esparsas
2. √Årvores cortam features TF-IDF de forma sub√≥tima
3. Overfitting mais prov√°vel em alta dimensionalidade

### CatBoost (0.48202) e TabPFN (0.39074)

**Falhas severas:**
- CatBoost: n√£o otimizado para texto esparso
- TabPFN: n√£o roda bem offline, pode ter tido problemas de execu√ß√£o

---

## Pr√≥ximos Passos

### 1. Otimiza√ß√£o do LinearSVC
- Grid search em C: [0.1, 1, 10]
- Testar `class_weight` ajustado
- Experimentar `loss='squared_hinge'`

### 2. Ensemble com TF-IDF
- Voting: LinearSVC + SGD + LogReg
- Stacking com meta-learner simples

### 3. TF-IDF + Transformers
- Usar TF-IDF como feature adicional em fine-tuning

### 4. N-grams e Preprocessing
- Testar bi-grams e tri-grams
- Remover stopwords m√©dicas espec√≠ficas
- Lematiza√ß√£o com spaCy

---

## Conclus√£o

TF-IDF + LinearSVC √© o **baseline imbat√≠vel** para este problema porque:

1. **Dados s√£o linearmente separ√°veis** em espa√ßo TF-IDF
2. **Termos m√©dicos espec√≠ficos** s√£o altamente discriminativos
3. **Complexidade adicional** (boosting, embeddings) prejudica

**Veredicto:** Focar em otimiza√ß√µes marginais do LinearSVC e testar Transformers como alternativa sem√¢ntica.

---

*Atualizado em: 20/02/2026*
