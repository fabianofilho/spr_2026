# Transformers - An√°lise Metodol√≥gica

## Resultados

| Rank | Modelo | Score | Status |
|------|--------|-------|--------|
| üèÜ | **BERTimbau + Focal Loss** | **0.79696** | ‚úÖ Submetido |
| 2 | ModernBERT base | 0.68578 | ‚úÖ Submetido |
| 3 | BERTimbau base | 0.64319 | ‚úÖ Submetido |
| 4 | BERT Multilingual | 0.56095 | ‚úÖ Submetido |
| 5 | DistilBERT Multilingual | 0.55229 | ‚úÖ Submetido |
| ‚ùå | BERTimbau + LoRA (Offline) | 0.13261 | ‚ö†Ô∏è Falhou |
| ‚ùå | mDeBERTa + class weights | 0.01008 | ‚ö†Ô∏è BUG |
| - | BioBERTpt | - | ‚è≥ Pendente |
| - | mDeBERTa-v3 (sem class weights) | - | ‚è≥ Pendente |
| - | XLM-RoBERTa + Mean Pool | - | ‚è≥ Pendente |
| - | Custom Transformer | - | ‚è≥ Pendente |

---

## üèÜ An√°lise: BERTimbau + Focal Loss (0.79696) - MELHOR SCORE!

**Primeiro transformer a superar TF-IDF!** BERTimbau com Focal Loss alcan√ßou **0.79696**, superando o baseline TF-IDF (0.77885) em **+2.3%**.

### Por que funcionou?

1. **Focal Loss:** Œ≥=2 foca nos exemplos dif√≠ceis, melhorando classes minorit√°rias
2. **BERTimbau base:** Especializado em portugu√™s, vocabul√°rio adequado
3. **Fine-tuning adequado:** Par√¢metros bem calibrados para o dataset
4. **Sem overfitting:** Focal Loss tem efeito regularizador impl√≠cito

### Por que superou TF-IDF?

- **Contexto sem√¢ntico:** Entende rela√ß√µes complexas como "sem sinais de malignidade"
- **Transfer learning:** Conhecimento pr√©vio de portugu√™s m√©dico
- **Focal Loss:** Resolve o problema de desbalanceamento de classes

### Compara√ß√£o com baseline TF-IDF

- BERTimbau + Focal: **0.79696** (+2.3% acima)
- TF-IDF baseline: 0.77885
- **Breakthrough!** Primeiro transformer a vencer.

---

## ‚ö†Ô∏è An√°lise: BERTimbau + LoRA (0.13261) - FALHA

**LoRA offline falhou completamente.** Score de 0.13261 indica predi√ß√µes quase aleat√≥rias.

### Por que falhou?

1. **Offline execution:** Kaggle offline pode ter problemas com bibliotecas PEFT
2. **Rank muito baixo:** LoRA com r=4 ou r=8 pode ser insuficiente para este task
3. **Adapter n√£o treinou:** Poss√≠vel problema no salvamento/carregamento dos pesos
4. **Incompatibilidade:** Vers√£o do PEFT pode n√£o ser compat√≠vel com ambiente Kaggle

### Li√ß√µes aprendidas

- Full fine-tuning funciona melhor que LoRA para datasets pequenos
- Testar sempre online antes de submeter offline
- LoRA economiza mem√≥ria mas pode perder performance

---

## An√°lise: ModernBERT (0.68578)

**Segundo melhor transformer.** ModernBERT ficou **12% abaixo** do TF-IDF baseline (0.77885).

### Por que funcionou melhor?

1. **Arquitetura moderna:** RoPE embeddings, Flash Attention 2, GeGLU
2. **Contexto longo:** Suporta at√© 8192 tokens nativamente
3. **Treinamento eficiente:** Mais dados, melhor curriculum
4. **Tokeniza√ß√£o robusta:** Melhor handling de tokens subword

### Compara√ß√£o com baseline TF-IDF

- ModernBERT: **0.68578** (88% do baseline)
- Gap ainda significativo, mas promissor para ensembles

---

## An√°lise: BERTimbau (0.64319)

**Resultado intermedi√°rio.** BERTimbau ficou abaixo do ModernBERT apesar de ser especializado em portugu√™s.

### Por que n√£o superou ModernBERT?

1. **Arquitetura mais antiga:** BERT original (2018) vs ModernBERT (2024)
2. **Sem otimiza√ß√µes modernas:** Sem Flash Attention, RoPE
3. **Pr√©-treinamento limitado:** Menos dados que ModernBERT
4. **Contexto m√°ximo:** 512 tokens vs 8192 do ModernBERT

### Pontos positivos

- Modelo PT-BR ainda √© **14% melhor** que BERT Multilingual (0.56095)
- Base s√≥lida para fine-tuning adicional

---

## An√°lise: DistilBERT Multilingual (0.55229)

**Resultado esperado.** DistilBERT ficou pr√≥ximo ao BERT Multilingual (0.56095), apenas 1.5% abaixo.

### Por que ficou similar ao BERT Multilingual?

1. **Destila√ß√£o preservou conhecimento:** DistilBERT mant√©m ~97% da performance do BERT original
2. **Mesmo problema:** Modelo gen√©rico multilingual, sem especializa√ß√£o PT-BR
3. **Vantagem:** 40% mais r√°pido e 60% menor que BERT full

### Quando usar DistilBERT?

- Quando tempo de infer√™ncia √© cr√≠tico
- Como modelo r√°pido para ensembles
- Trade-off aceit√°vel: -1.5% score vs 2x mais r√°pido

---

## ‚ö†Ô∏è BUG: mDeBERTa + Class Weights (0.01008)

**Score praticamente zero indica bug cr√≠tico!** Modelo completamente quebrado.

### Diagn√≥stico

Score de 0.01 em F1-macro significa:
- Modelo predizendo **sempre a mesma classe**
- Ou labels **invertidas/mapeadas incorretamente**
- Ou problema no **training loop**

### Poss√≠veis causas

1. **Mixed precision + class_weights:** Adicionamos `.float()` nos logits, mas pode ter outro problema
2. **Label mismatch:** Verificar se labels do test set batem com train
3. **Peso de classes extremo:** Class weights muito altos podem causar gradientes inst√°veis
4. **Learning rate alta demais:** Com class weights, LR precisa ser menor

### Pr√≥ximos passos

1. Testar `submit_deberta.ipynb` (sem class weights) para isolar o problema
2. Verificar distribui√ß√£o de predi√ß√µes no output
3. Se necess√°rio, remover class weights e usar Focal Loss em vez disso

---

## An√°lise: BERT Multilingual (0.56095)

**Resultado decepcionante.** BERT Multilingual ficou **28% abaixo** do TF-IDF baseline (0.77885).

### Por que falhou?

1. **Modelo gen√©rico:** BERT Multilingual √© treinado em 104 idiomas, diluindo conhecimento de portugu√™s
2. **Sem dom√≠nio m√©dico:** Vocabul√°rio m√©dico/radiol√≥gico n√£o est√° bem representado
3. **Tokeniza√ß√£o sub√≥tima:** Warning de regex do Mistral indica problemas no tokenizer
4. **Epoch insuficientes:** F1 ainda estava subindo (0.43 ‚Üí 0.50 ‚Üí 0.56), precisava de mais epochs
5. **Incompatibilidade LayerNorm:** Warnings de `gamma/beta` vs `weight/bias` indicam checkpoint com formato antigo

### Li√ß√µes aprendidas

- Transformers gen√©ricos **n√£o** superam TF-IDF automaticamente
- Precisa de modelo especializado em portugu√™s (BERTimbau) ou dom√≠nio m√©dico (BioBERTpt)
- Hiperpar√¢metros precisam de tuning (mais epochs, learning rate schedule)

---

## Inputs Kaggle Necess√°rios

Todos os modelos transformer precisam ser adicionados como **Input** no Kaggle:

| Modelo | Kaggle Input |
|--------|--------------|
| BERTimbau base | `neuralmind/bert-base-portuguese-cased` |
| BERTimbau large | `neuralmind/bert-large-portuguese-cased` |
| BioBERTpt | `pucpr/biobertpt-all` |
| mDeBERTa-v3 | `microsoft/mdeberta-v3-base` |
| DistilBERT | `distilbert-base-multilingual-cased` |
| XLM-RoBERTa | `xlm-roberta-large` |
| ModernBERT | `answerdotai/ModernBERT-base` |

---

## Hip√≥teses

### 1. Potencial Vantagens sobre TF-IDF

- **Contexto sem√¢ntico:** Transformers entendem "aus√™ncia de malignidade" vs "presen√ßa de malignidade"
- **Transfer learning:** Pr√©-treinamento em portugu√™s pode ajudar
- **Embeddings contextuais:** Mesma palavra em contextos diferentes tem representa√ß√µes diferentes

### 2. Riscos

- **Overfitting:** Dataset pequeno + modelo grande = alto risco
- **Tempo de infer√™ncia:** Kaggle tem limite de tempo
- **GPU requirements:** Pode falhar offline

### 3. Modelos Priorit√°rios

1. **BERTimbau base:** Melhor modelo PT-BR, baseline obrigat√≥rio
2. **BioBERTpt:** Dom√≠nio m√©dico, pode ter vantagem
3. **mDeBERTa:** Estado da arte em NLU

---

## Configura√ß√µes Recomendadas

### Fine-tuning b√°sico
```python
{
    "learning_rate": 2e-5,
    "epochs": 3-5,
    "batch_size": 16,
    "max_length": 128,  # textos s√£o curtos
    "warmup_ratio": 0.1
}
```

### Para dataset desbalanceado
- Focal Loss (Œ≥=2)
- Class weights inversamente proporcionais
- Oversampling da classe minorit√°ria

### Para poucos dados
- LoRA (r=8, alpha=16)
- Gradient checkpointing
- Early stopping agressivo

---

## Resumo

| Modelo | Score | vs Baseline |
|--------|-------|-------------|
| **BERTimbau + Focal** | **0.79696** | **+2.3%** üèÜ |
| ModernBERT | 0.68578 | -12% |
| BERTimbau | 0.64319 | -17% |
| BERT Multilingual | 0.56095 | -28% |
| DistilBERT | 0.55229 | -29% |
| BERTimbau + LoRA | 0.13261 | ‚ùå Falhou |
| mDeBERTa + CW | 0.01008 | ‚ùå BUG |

**Conclus√£o:** üéâ **BERTimbau + Focal Loss superou o baseline TF-IDF (0.77885)!** Primeiro transformer a vencer.

**Pr√≥ximos passos:**
1. Investigar por que LoRA falhou (0.13261)
2. Testar BioBERTpt com Focal Loss
3. Testar mDeBERTa SEM class weights

---

*Atualizado em: 23/02/2026*
