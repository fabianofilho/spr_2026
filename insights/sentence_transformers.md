# Sentence Transformers - An√°lise Metodol√≥gica

## Resultados

| Rank | Modelo | Score | Status |
|------|--------|-------|--------|
| üî• | **Custom Transformer Encoder** | **0.77272** | ‚úÖ Submetido |
| 2 | SBERT + LightGBM | 0.48376 | ‚úÖ Submetido |

---

## üî• An√°lise: Custom Transformer Encoder (0.77272) - TOP 4!

**Surprise hit!** Um transformer from scratch competiu com modelos pr√©-treinados!

### Por que funcionou t√£o bem?

1. **Tokenizer espec√≠fico:** Vocabul√°rio constru√≠do do dataset m√©dico
2. **Sem transfer gap:** Treinado diretamente no task
3. **Arquitetura otimizada:** Modelo menor, mais eficiente
4. **Embeddings limpos:** Sem ru√≠do de dom√≠nios irrelevantes

### Compara√ß√£o com pr√©-treinados

| Modelo | Score | Pr√©-treino |
|--------|-------|------------|
| Custom Transformer | **0.77272** | ‚ùå Nenhum |
| BioBERTpt | 0.72480 | ‚úÖ M√©dico PT |
| ModernBERT | 0.68578 | ‚úÖ Geral EN |
| BERT Multilingual | 0.56095 | ‚úÖ Multi |

**Insight:** Para datasets pequenos e espec√≠ficos, treinar from scratch pode superar transfer learning!

### Configura√ß√£o

```python
# Arquitetura
vocab_size = ~5000  # Vocabul√°rio do dataset
embedding_dim = 256
num_heads = 4
num_layers = 2
max_len = 256

# Treinamento
epochs = 50
batch_size = 32
lr = 1e-3
```

### Por que superou modelos multilingual?

1. **Vocabul√°rio ajustado:** "BIRADS" √© um token √∫nico, n√£o "BI" + "##RADS"
2. **Sem overhead:** Modelo 50x menor que BERT
3. **R√°pido para iterar:** Treina em minutos, n√£o horas

---

## ‚ö†Ô∏è An√°lise: SBERT + LightGBM (0.48376) - DECEP√á√ÉO

**SBERT multilingual falhou.** Score de 0.48376 √© pior que Word2Vec.

### Por que falhou?

1. **Embeddings gen√©ricos:** Otimizado para similaridade, n√£o classifica√ß√£o
2. **Vocabul√°rio multilingual:** Perde especificidade de termos m√©dicos PT
3. **Representa√ß√£o densa:** Mesmo problema do Word2Vec

### Compara√ß√£o com Word2Vec

| Modelo | Score | Representa√ß√£o |
|--------|-------|---------------|
| Word2Vec + XGBoost | 0.66385 | M√©dia 300D |
| SBERT + LightGBM | 0.48376 | Senten√ßa 384D |

**Insight:** SBERT multilingual √© pior que Word2Vec para este task!

### Li√ß√µes aprendidas

- SBERT funciona para similaridade sem√¢ntica, n√£o classifica√ß√£o
- Embeddings pr√©-treinados multilingual perdem especificidade
- Para classifica√ß√£o, TF-IDF ou custom transformer s√£o melhores

---

## üí° Pr√≥ximos Passos

### Para melhorar SBERT:
1. Usar modelo PT-BR espec√≠fico (se existir)
2. Adicionar TF-IDF features h√≠bridas
3. Fine-tuning contrastivo no dataset

### Para melhorar Custom Transformer:
1. Adicionar Focal Loss (como BERTimbau)
2. Aumentar profundidade (4-6 layers)
3. Data augmentation (EDA)

---

*Atualizado em: 24/02/2026*
