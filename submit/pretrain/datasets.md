# Datasets para Pr√©-Treinamento

## Objetivo
Usar datasets externos do Kaggle para pr√©-treinar modelos antes de fine-tuning no dataset da competi√ß√£o SPR 2026 (classifica√ß√£o de laudos mamogr√°ficos em categorias BI-RADS).

## Como usar no Kaggle
1. Add Input ‚Üí **Datasets** ‚Üí pesquisar o dataset
2. O dataset fica dispon√≠vel em `/kaggle/input/{dataset-slug}/`

---

## Datasets Recomendados

### üè• Radiologia / Mamografia

| Dataset | Slug Kaggle | Descri√ß√£o | Tamanho |
|---------|-------------|-----------|---------|
| CBIS-DDSM | `awsaf49/cbis-ddsm-breast-cancer-image-dataset` | Mammography dataset com labels BI-RADS | ~3GB |
| VinDr-Mammo | `maedemaftouni/vindr-mammo` | Laudos mamogr√°ficos em ingl√™s | ~1GB |
| Breast Cancer Wisconsin | `uciml/breast-cancer-wisconsin-data` | Classifica√ß√£o bin√°rio (benigno/maligno) | 24KB |
| INbreast | `ramanathansp20/inbreast-dataset` | Mamografias com BI-RADS | ~2GB |

### üìù NLP M√©dico (Ingl√™s)

| Dataset | Slug Kaggle | Descri√ß√£o | Tamanho |
|---------|-------------|-----------|---------|
| Medical Transcriptions | `tboyle10/medicaltranscriptions` | 5000+ transcri√ß√µes m√©dicas | 20MB |
| Medical NER | `finalepoch/medical-ner` | Named Entity Recognition m√©dico | 5MB |
| PubMed 200k RCT | `owaiskhan9654/pubmed-200k-rct` | Abstracts de artigos m√©dicos | ~100MB |
| Medical QA | `jpmiller/layoutlm` | Perguntas e respostas m√©dicas | 10MB |
| MIMIC-III Clinical Notes | `drscarlat/mimic3` | Notas cl√≠nicas (requer aprova√ß√£o) | ~4GB |
| Medical Specialty Classification | `chaitanyakck/medical-text` | Classifica√ß√£o de especialidade m√©dica | 5MB |
| Symptom2Disease | `itachi9604/disease-symptom-description-dataset` | Sintomas ‚Üí Diagn√≥stico | 1MB |
| Drug Review | `jessicali9530/kuc-hackathon-winter-2018` | Reviews de medicamentos | 50MB |

### üáßüá∑ Portugu√™s

| Dataset | Slug Kaggle | Descri√ß√£o | Tamanho |
|---------|-------------|-----------|---------|
| Portuguese Medical Corpus | `a pesquisar` | Corpus m√©dico em PT-BR | - |
| Wikipedia PT | `ltcmdrdata/wikipedia-ptbr` | Para domain adaptation | ~2GB |
| Brazilian News | `diogocaliman/brazilian-news` | Not√≠cias em PT-BR | 100MB |
| B2W Reviews | `olistbr/brazilian-ecommerce` | Reviews em PT-BR | 50MB |

### üéØ Classifica√ß√£o de Texto (Transfer Learning)

| Dataset | Slug Kaggle | Descri√ß√£o | Tamanho |
|---------|-------------|-----------|---------|
| IMDB Reviews | `lakshmi25npathi/imdb-dataset-of-50k-movie-reviews` | Sentimento (bom para base) | 60MB |
| AG News | `amananandrai/ag-news-classification-dataset` | Classifica√ß√£o de not√≠cias | 30MB |
| Yelp Reviews | `yelp-dataset/yelp-dataset` | Reviews multi-classe | 5GB |
| Amazon Reviews | `bittlingmayer/amazonreviews` | Reviews multi-classe | 600MB |

---

## Estrat√©gias de Pr√©-Treinamento

### 1. MLM (Masked Language Modeling)
- Usar corpus m√©dico/radiol√≥gico para continuar pr√©-treino de BERT/BERTimbau
- Datasets: Medical Transcriptions, PubMed, MIMIC-III

### 2. Domain Adaptation
- Fine-tune em tarefa similar antes do dataset alvo
- Datasets: Medical Specialty Classification, Breast Cancer Wisconsin

### 3. Multi-Task Learning
- Treinar em m√∫ltiplas tarefas m√©dicas simultaneamente
- Datasets: Medical NER + Classification + QA

### 4. Data Augmentation
- Usar dados similares para aumentar o treino
- Datasets: VinDr-Mammo, CBIS-DDSM

---

## Modelos a Testar (aguardando sele√ß√£o)

| Modelo | Tipo | Status |
|--------|------|--------|
| BERTimbau + MLM m√©dico | Transformer | ‚è≥ Aguardando |
| mDeBERTa + Domain Adaptation | Transformer | ‚è≥ Aguardando |
| Sentence-BERT + Contrastive | Sentence Transformer | ‚è≥ Aguardando |
| LLM Fine-tuned | LLM | ‚è≥ Aguardando |

---

## Notas

- ‚ö†Ô∏è Verificar licen√ßas antes de usar
- ‚ö†Ô∏è Alguns datasets requerem aprova√ß√£o (MIMIC-III)
- ‚ö†Ô∏è Datasets grandes podem estourar mem√≥ria no Kaggle (16GB RAM)
- üí° Preferir datasets em portugu√™s ou multil√≠ngues quando poss√≠vel
- üí° Come√ßar com datasets menores para validar a abordagem
