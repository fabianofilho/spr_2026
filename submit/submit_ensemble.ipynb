{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b54846f",
   "metadata": {},
   "source": [
    "# SPR 2026 - Ensemble (TF-IDF + Word2Vec + LightGBM)\n",
    "\n",
    "**Notebook para submissão offline no Kaggle.**\n",
    "\n",
    "Combina múltiplas estratégias que não requerem modelos externos:\n",
    "- TF-IDF + Logistic Regression\n",
    "- TF-IDF + LightGBM\n",
    "- Word2Vec + LightGBM\n",
    "\n",
    "---\n",
    "**CONFIGURAÇÃO OFFLINE:**\n",
    "1. No Kaggle, vá em Settings → Internet → **OFF**\n",
    "2. Não requer datasets externos de modelos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.stats import mode\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "EMBEDDING_DIM = 100\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "np.random.seed(SEED)\n",
    "print('Bibliotecas carregadas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c83403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train: {train.shape}')\n",
    "print(f'Test: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f83c6a",
   "metadata": {},
   "source": [
    "## Modelo 1: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "X_tfidf_train = tfidf.fit_transform(train['report'])\n",
    "X_tfidf_test = tfidf.transform(test['report'])\n",
    "y = train['target'].values\n",
    "\n",
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(C=1.0, max_iter=1000, class_weight='balanced', random_state=SEED, n_jobs=-1)\n",
    "model_lr.fit(X_tfidf_train, y)\n",
    "preds_lr = model_lr.predict(X_tfidf_test)\n",
    "print(f'Modelo 1 (TF-IDF + LR): {len(preds_lr)} predições')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f00d93",
   "metadata": {},
   "source": [
    "## Modelo 2: TF-IDF + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe42fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF com mais features\n",
    "tfidf2 = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "X_tfidf2_train = tfidf2.fit_transform(train['report'])\n",
    "X_tfidf2_test = tfidf2.transform(test['report'])\n",
    "\n",
    "# LightGBM\n",
    "model_lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "model_lgbm.fit(X_tfidf2_train, y)\n",
    "preds_lgbm = model_lgbm.predict(X_tfidf2_test)\n",
    "print(f'Modelo 2 (TF-IDF + LGBM): {len(preds_lgbm)} predições')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2d062",
   "metadata": {},
   "source": [
    "## Modelo 3: Word2Vec + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessamento\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-záàâãéèêíïóôõöúçñ\\s]', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "train['tokens'] = train['report'].apply(preprocess)\n",
    "test['tokens'] = test['report'].apply(preprocess)\n",
    "\n",
    "# Word2Vec\n",
    "all_texts = train['tokens'].tolist() + test['tokens'].tolist()\n",
    "w2v = Word2Vec(sentences=all_texts, vector_size=EMBEDDING_DIM, window=5, min_count=2, workers=4, epochs=10, seed=SEED)\n",
    "\n",
    "# Embeddings\n",
    "def text_to_embedding(tokens, model, dim):\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_w2v_train = np.array([text_to_embedding(t, w2v, EMBEDDING_DIM) for t in train['tokens']])\n",
    "X_w2v_test = np.array([text_to_embedding(t, w2v, EMBEDDING_DIM) for t in test['tokens']])\n",
    "\n",
    "# LightGBM\n",
    "model_w2v = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    verbose=-1\n",
    ")\n",
    "model_w2v.fit(X_w2v_train, y)\n",
    "preds_w2v = model_w2v.predict(X_w2v_test)\n",
    "print(f'Modelo 3 (Word2Vec + LGBM): {len(preds_w2v)} predições')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cac9f",
   "metadata": {},
   "source": [
    "## Ensemble: Votação Majoritária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar predições\n",
    "preds_matrix = np.column_stack([preds_lr, preds_lgbm, preds_w2v])\n",
    "print(f'Matriz de predições: {preds_matrix.shape}')\n",
    "\n",
    "# Votação majoritária\n",
    "ensemble_preds = mode(preds_matrix, axis=1).mode.flatten()\n",
    "\n",
    "# Submissão\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'target': ensemble_preds.astype(int)\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('submission.csv criado!')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar distribuições\n",
    "print('\\nDistribuição por modelo:')\n",
    "print(f'LR:       {np.bincount(preds_lr, minlength=7)}')\n",
    "print(f'LGBM:     {np.bincount(preds_lgbm, minlength=7)}')\n",
    "print(f'W2V:      {np.bincount(preds_w2v, minlength=7)}')\n",
    "print(f'Ensemble: {np.bincount(ensemble_preds.astype(int), minlength=7)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
