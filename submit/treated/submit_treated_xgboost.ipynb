{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a938d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# SPR 2026 - TF-IDF + XGBoost (TRATADO)\n",
    "\n",
    "**Versão tratada do 5º melhor modelo (0.69482)**\n",
    "\n",
    "**Tratamento aplicado:**\n",
    "- ✅ Normalização de texto (lowercase, acentos)\n",
    "- ✅ Remoção de stop words (preservando termos médicos)\n",
    "- ✅ Preservação de termos BI-RADS\n",
    "- ✅ Limpeza de caracteres especiais\n",
    "\n",
    "**Optimizações:**\n",
    "- ✅ TruncatedSVD: 15k features → 500 features densas\n",
    "- ✅ GPU acceleration\n",
    "\n",
    "**Hipótese:** Laudos muito parecidos → tratamento cuidadoso pode ajudar a capturar diferenças sutis.\n",
    "\n",
    "---\n",
    "**CONFIGURAÇÃO KAGGLE:**\n",
    "1. Settings → Internet → **OFF**\n",
    "2. Settings → Accelerator → **GPU T4 x2** (recomendado) ou **CPU**\n",
    "3. **IMPORTANTE:** Execute \"Run All\" após commit\n",
    "---\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# =============================================================================\n",
    "# SPR 2026 - TFIDF + XGBOOST (VERSÃO TRATADA)\n",
    "# =============================================================================\n",
    "# Teste de hipótese: tratamento de texto melhora performance?\n",
    "# Baseline: 0.69482 (sem tratamento)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - TF-IDF + XGBoost (TRATADO)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==== CONFIGURAÇÕES ====\n",
    "SEED = 42\n",
    "SVD_COMPONENTS = 500\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"✓ GPU disponível: {USE_GPU}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FUNÇÕES DE TRATAMENTO DE TEXTO\n",
    "# =============================================================================\n",
    "\n",
    "# Stop words em português (excluindo termos médicos importantes)\n",
    "STOP_WORDS_PT = {\n",
    "    'a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo',\n",
    "    'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele',\n",
    "    'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em',\n",
    "    'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estas',\n",
    "    'este', 'estes', 'eu', 'foi', 'fomos', 'for', 'foram', 'há', 'isso',\n",
    "    'isto', 'já', 'lhe', 'lhes', 'lo', 'mas', 'me', 'mesmo', 'meu', 'meus',\n",
    "    'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa',\n",
    "    'nossas', 'nosso', 'nossos', 'num', 'numa', 'nuns', 'o', 'os', 'ou',\n",
    "    'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando',\n",
    "    'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sem', 'seu', 'seus',\n",
    "    'só', 'sua', 'suas', 'também', 'te', 'tem', 'tendo', 'tenho', 'ter',\n",
    "    'teu', 'teus', 'ti', 'tive', 'tivemos', 'tiveram', 'tu', 'tua', 'tuas',\n",
    "    'um', 'uma', 'umas', 'uns', 'você', 'vocês', 'vos'\n",
    "}\n",
    "\n",
    "# Termos BI-RADS importantes (NÃO remover)\n",
    "BIRADS_TERMS = {\n",
    "    'birads', 'bi-rads', 'categoria', 'calcificacao', 'calcificacoes',\n",
    "    'nodulo', 'nodulos', 'massa', 'massas', 'assimetria', 'assimetrias',\n",
    "    'distorcao', 'densidade', 'benigno', 'benigna', 'maligno', 'maligna',\n",
    "    'suspeito', 'suspeita', 'provavelmente', 'tipicamente', 'achado',\n",
    "    'achados', 'mama', 'mamas', 'mamografia', 'ultrassom', 'ecografia',\n",
    "    'axila', 'axilar', 'linfonodo', 'linfonodos', 'pele', 'mamilo',\n",
    "    'areola', 'parenquima', 'fibroglandular', 'adiposo', 'heterogeneo',\n",
    "    'homogeneo', 'denso', 'densa', 'espiculado', 'circunscrito', 'irregular',\n",
    "    'oval', 'redondo', 'lobulado', 'microlobulado', 'obscurecido',\n",
    "    'parcialmente', 'totalmente', 'grosseiras', 'finas', 'pleomorficas',\n",
    "    'amorfas', 'puntiformes', 'lineares', 'ramificadas', 'segmentar',\n",
    "    'regional', 'difuso', 'agrupadas', 'clusters', 'estavel', 'novo',\n",
    "    'aumentou', 'diminuiu', 'inalterado', 'recomenda', 'biopsia',\n",
    "    'controle', 'seguimento', 'rotina', 'complementar', 'negativo',\n",
    "    'positivo', 'inconclusivo'\n",
    "}\n",
    "\n",
    "def remove_accents(text):\n",
    "    \"\"\"Remove acentos do texto.\"\"\"\n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join(c for c in nfkd if not unicodedata.combining(c))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Pré-processa texto de laudo de mamografia.\n",
    "    - Lowercase\n",
    "    - Remove acentos\n",
    "    - Remove caracteres especiais (mantém hífen para BI-RADS)\n",
    "    - Remove stop words (preservando termos médicos)\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove acentos\n",
    "    text = remove_accents(text)\n",
    "    \n",
    "    # Normaliza BI-RADS\n",
    "    text = re.sub(r'bi[\\s-]*rads?', 'birads', text)\n",
    "    \n",
    "    # Remove caracteres especiais (mantém letras, números e espaços)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove números isolados (mas mantém medidas como contexto)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    # Remove espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stop words (preservando termos BI-RADS)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in STOP_WORDS_PT or w in BIRADS_TERMS]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# ==== CARREGAR DADOS ====\n",
    "print(\"\\n[1/6] Carregando dados...\")\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f\"    Train: {train.shape} | Test: {test.shape}\")\n",
    "\n",
    "# ==== TRATAMENTO DE TEXTO ====\n",
    "print(\"\\n[2/6] Aplicando tratamento de texto...\")\n",
    "train['report_treated'] = train['report'].apply(preprocess_text)\n",
    "test['report_treated'] = test['report'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar exemplo\n",
    "print(\"\\n    Exemplo de tratamento:\")\n",
    "print(f\"    Original: {train['report'].iloc[0][:100]}...\")\n",
    "print(f\"    Tratado:  {train['report_treated'].iloc[0][:100]}...\")\n",
    "\n",
    "# ==== TF-IDF ====\n",
    "print(\"\\n[3/6] Aplicando TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(train['report_treated'])\n",
    "X_test_tfidf = tfidf.transform(test['report_treated'])\n",
    "y_train = train['target'].values\n",
    "print(f\"    TF-IDF esparso: {X_train_tfidf.shape}\")\n",
    "\n",
    "# ==== SVD ====\n",
    "print(f\"\\n[4/6] Aplicando SVD: {X_train_tfidf.shape[1]} → {SVD_COMPONENTS} features...\")\n",
    "svd = TruncatedSVD(n_components=SVD_COMPONENTS, random_state=SEED)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "print(f\"    Variância explicada: {svd.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"    ✅ Shape denso: {X_train_svd.shape}\")\n",
    "\n",
    "# Normalizar\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_svd)\n",
    "X_test = scaler.transform(X_test_svd)\n",
    "\n",
    "# ==== XGBOOST ====\n",
    "print(f\"\\n[5/6] Treinando XGBoost ({'GPU' if USE_GPU else 'CPU'})...\")\n",
    "sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    device='cuda' if USE_GPU else 'cpu',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "print(\"    ✓ Modelo treinado!\")\n",
    "\n",
    "# ==== SUBMISSÃO ====\n",
    "print(\"\\n[6/6] Gerando submissão...\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✅ CONCLUÍDO - submission.csv criado!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nDistribuição das predições:\")\n",
    "print(submission['target'].value_counts().sort_index())\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
