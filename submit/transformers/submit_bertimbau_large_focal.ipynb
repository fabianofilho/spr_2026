{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06febf2b",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau-Large + Focal Loss\n",
    "\n",
    "**BERTimbau-Large com Focal Loss para desbalanceamento**\n",
    "\n",
    "- ✅ Focal Loss (gamma=2.0, alpha=0.25)\n",
    "- ✅ Model Large (mais parâmetros)\n",
    "- ✅ Tempo esperado: ~25-30 min\n",
    "\n",
    "---\n",
    "**CONFIGURAÇÃO KAGGLE:**\n",
    "1. Settings → Internet → **OFF**\n",
    "2. Settings → Accelerator → **GPU T4 x2** ou **P100**\n",
    "3. Add Data → Models → `neuralmind/bert-large-portuguese-cased`\n",
    "4. **IMPORTANTE:** Execute \"Run All\" após commit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - BERTIMBAU-LARGE + FOCAL LOSS (CONSOLIDADO) =====\n",
    "\n",
    "# ==== SETUP E IMPORTS ====\n",
    "print(\"[1/8] Configurando ambiente...\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "MODEL_PATHS = [\n",
    "    '/kaggle/input/bert-large-portuguese-cased',\n",
    "    '/kaggle/input/bertimbau-large-portuguese-cased',\n",
    "    '/kaggle/input/neuralmind-bert-large-portuguese-cased',\n",
    "    'neuralmind/bert-large-portuguese-cased'\n",
    "]\n",
    "\n",
    "MODEL_PATH = MODEL_PATHS[-1]\n",
    "for path in MODEL_PATHS[:-1]:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Usando modelo: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ==== FOCAL LOSS ====\n",
    "print(\"[2/8] Definindo Focal Loss...\")\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# ==== CARREGAR DADOS ====\n",
    "print(\"[3/8] Carregando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values,\n",
    "    train_df['target'].values,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['target']\n",
    ")\n",
    "print(f'Train: {len(train_texts)}, Val: {len(val_texts)}')\n",
    "\n",
    "# ==== DATASET ====\n",
    "print(\"[4/8] Preparando datasets...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_df['report'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ==== MODELO ====\n",
    "print(\"[5/8] Carregando modelo...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_CLASSES).to(device)\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
    "\n",
    "# ==== FUNÇÕES DE TREINO ====\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        preds.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    return total_loss / len(loader), f1_score(targets, preds, average='macro')\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    return f1_score(targets, preds, average='macro')\n",
    "\n",
    "# ==== TREINAMENTO ====\n",
    "print(\"[6/8] Treinando...\")\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}')\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f'Novo melhor modelo! F1: {best_f1:.4f}')\n",
    "\n",
    "print(f'\\nMelhor F1: {best_f1:.4f}')\n",
    "\n",
    "# ==== PREDIÇÃO ====\n",
    "print(\"[7/8] Gerando predições...\")\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# ==== SUBMISSÃO ====\n",
    "print(\"[8/8] Criando submissão...\")\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'target': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✅ CONCLUÍDO - submission.csv criado!\")\n",
    "print(\"=\"*60)\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
