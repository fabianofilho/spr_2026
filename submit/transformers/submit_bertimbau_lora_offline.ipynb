{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b827bac0",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau-Large + LoRA (Offline)\n",
    "\n",
    "**Fine-tuning eficiente com LoRA implementado manualmente (SEM peft)**\n",
    "\n",
    "- ✅ LoRA (r=16, alpha=32) implementado do zero\n",
    "- ✅ **100% OFFLINE** - não precisa de pip install\n",
    "- ✅ Menor uso de memória (~10% dos parâmetros treináveis)\n",
    "- ✅ Tempo esperado: ~15-20 min\n",
    "\n",
    "---\n",
    "**CONFIGURAÇÃO KAGGLE:**\n",
    "1. Settings → Internet → **OFF** ✅\n",
    "2. Settings → Accelerator → **GPU T4 x2**\n",
    "3. Add Data → Models → `neuralmind/bert-large-portuguese-cased`\n",
    "4. **IMPORTANTE:** Execute \"Run All\" após commit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - BERTIMBAU-LARGE + LORA (OFFLINE - SEM PEFT) =====\n",
    "\n",
    "# ==== SETUP E IMPORTS ====\n",
    "print(\"[1/8] Configurando ambiente...\")\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# LoRA Config\n",
    "LORA_R = 16          # rank\n",
    "LORA_ALPHA = 32      # scaling\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# Paths para modelo (verificar qual está disponível)\n",
    "MODEL_PATHS = [\n",
    "    '/kaggle/input/bert-large-portuguese-cased',\n",
    "    '/kaggle/input/bertimbau-large-portuguese-cased',\n",
    "    '/kaggle/input/neuralmind-bert-large-portuguese-cased',\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for path in MODEL_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\n⚠️ Modelo não encontrado. Datasets disponíveis:\")\n",
    "    for item in os.listdir('/kaggle/input'):\n",
    "        print(f\"  - {item}\")\n",
    "    raise FileNotFoundError(\"Adicione neuralmind/bert-large-portuguese-cased!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Modelo: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== LORA IMPLEMENTATION (SEM BIBLIOTECA EXTERNA) ====\n",
    "print(\"[2/8] Implementando LoRA...\")\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA: Low-Rank Adaptation of Large Language Models\n",
    "    Paper: https://arxiv.org/abs/2106.09685\n",
    "    \n",
    "    W' = W + BA onde B ∈ R^(d×r), A ∈ R^(r×k)\n",
    "    r << min(d, k) para eficiência\n",
    "    \"\"\"\n",
    "    def __init__(self, original_layer, r=16, alpha=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # Dimensões\n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # Matrizes LoRA (A e B)\n",
    "        # A: projeta para espaço de baixa dimensão\n",
    "        # B: projeta de volta para dimensão original\n",
    "        self.lora_A = nn.Parameter(torch.zeros(r, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        \n",
    "        # Inicialização (A com Kaiming, B com zeros)\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Congelar pesos originais\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output original (frozen)\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA path: x → dropout → A → B → scale\n",
    "        lora_output = self.dropout(x)\n",
    "        lora_output = F.linear(lora_output, self.lora_A)  # x @ A.T\n",
    "        lora_output = F.linear(lora_output, self.lora_B)  # (x @ A.T) @ B.T\n",
    "        lora_output = lora_output * self.scaling\n",
    "        \n",
    "        return original_output + lora_output\n",
    "\n",
    "\n",
    "def apply_lora_to_model(model, r=16, alpha=32, dropout=0.1, target_modules=['query', 'key', 'value']):\n",
    "    \"\"\"\n",
    "    Aplica LoRA às camadas especificadas do modelo BERT.\n",
    "    Retorna o número de parâmetros treináveis.\n",
    "    \"\"\"\n",
    "    lora_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Verificar se é uma camada alvo\n",
    "        if any(target in name for target in target_modules):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Encontrar o módulo pai e o nome do atributo\n",
    "                parts = name.split('.')\n",
    "                parent = model\n",
    "                for part in parts[:-1]:\n",
    "                    parent = getattr(parent, part)\n",
    "                \n",
    "                # Substituir por LoRALayer\n",
    "                attr_name = parts[-1]\n",
    "                lora_layer = LoRALayer(module, r=r, alpha=alpha, dropout=dropout)\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "                lora_layers.append(name)\n",
    "    \n",
    "    # Contar parâmetros\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"LoRA aplicado a {len(lora_layers)} camadas\")\n",
    "    print(f\"Parâmetros totais: {total_params:,}\")\n",
    "    print(f\"Parâmetros treináveis: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "    \n",
    "    return lora_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b524ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== MODELO COM CLASSIFICADOR ====\n",
    "\n",
    "class BertWithLoRA(nn.Module):\n",
    "    def __init__(self, model_path, num_classes, r=16, alpha=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Carregar BERT base\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Aplicar LoRA às camadas de atenção\n",
    "        apply_lora_to_model(\n",
    "            self.bert, \n",
    "            r=r, \n",
    "            alpha=alpha, \n",
    "            dropout=dropout,\n",
    "            target_modules=['query', 'key', 'value']\n",
    "        )\n",
    "        \n",
    "        # Classificador (treinável)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bae8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS ====\n",
    "print(\"[3/8] Carregando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values,\n",
    "    train_df['target'].values,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['target']\n",
    ")\n",
    "print(f'Train: {len(train_texts)}, Val: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATASET ====\n",
    "print(\"[4/8] Preparando datasets...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_df['report'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eed23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CRIAR MODELO ====\n",
    "print(\"[5/8] Criando modelo com LoRA...\")\n",
    "model = BertWithLoRA(\n",
    "    MODEL_PATH, \n",
    "    NUM_CLASSES, \n",
    "    r=LORA_R, \n",
    "    alpha=LORA_ALPHA, \n",
    "    dropout=LORA_DROPOUT\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer e Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=LR, \n",
    "    weight_decay=0.01\n",
    ")\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * total_steps), \n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FUNÇÕES DE TREINO ====\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), f1_score(targets, preds, average='macro')\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return f1_score(targets, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAMENTO ====\n",
    "print(\"[6/8] Treinando...\")\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_lora_model.pt')\n",
    "        print(f'Novo melhor modelo! F1: {best_f1:.4f}')\n",
    "\n",
    "print(f'\\nMelhor F1: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR MELHOR MODELO ====\n",
    "print(\"[7/8] Carregando melhor modelo...\")\n",
    "model.load_state_dict(torch.load('best_lora_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# ==== PREDIÇÃO ====\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fa7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISSÃO ====\n",
    "print(\"[8/8] Criando submissão...\")\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✅ CONCLUÍDO - submission.csv criado!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Melhor F1 validação: {best_f1:.4f}\")\n",
    "print(f\"\\nDistribuição das predições:\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
