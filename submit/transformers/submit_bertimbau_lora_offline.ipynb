{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau + LoRA (Offline)\n",
    "\n",
    "**Fine-tuning eficiente com LoRA implementado manualmente (SEM peft)**\n",
    "\n",
    "- LoRA (r=16, alpha=32) implementado do zero\n",
    "- **100% OFFLINE** - nao precisa de pip install\n",
    "- Menor uso de memoria (~10% dos parametros treinaveis)\n",
    "- Tempo esperado: ~15-20 min\n",
    "\n",
    "---\n",
    "**CONFIGURACAO KAGGLE:**\n",
    "\n",
    "1. **Add Input** -> **Models** -> buscar `bertimbau-ptbr-complete` (fabianofilho)\n",
    "   - Ou `bertimbau-large-ptbr` para versao Large\n",
    "2. **Settings** -> Internet -> **OFF**\n",
    "3. **Settings** -> Accelerator -> **GPU T4 x2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - BERTIMBAU + LORA (OFFLINE) =====\n",
    "\n",
    "# ==== SETUP E IMPORTS ====\n",
    "print(\"[1/8] Configurando ambiente...\")\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# LoRA Config\n",
    "LORA_R = 16          # rank\n",
    "LORA_ALPHA = 32      # scaling\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# ==== AUTO-DETECTAR PATH DO MODELO ====\n",
    "def find_model_path():\n",
    "    \"\"\"Encontra automaticamente o path do modelo (busca recursiva ate 10 niveis).\"\"\"\n",
    "    base = '/kaggle/input'\n",
    "    \n",
    "    def has_config(path):\n",
    "        return os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json'))\n",
    "    \n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if has_config(path):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(\"\\nModelo nao encontrado. Estrutura de /kaggle/input:\")\n",
    "    def show_tree(path, prefix=\"\", depth=0):\n",
    "        if depth > 4:\n",
    "            return\n",
    "        try:\n",
    "            items = os.listdir(path)[:10]\n",
    "            for item in items:\n",
    "                full = os.path.join(path, item)\n",
    "                print(f\"{prefix}{item}\")\n",
    "                if os.path.isdir(full):\n",
    "                    show_tree(full, prefix + \"  \", depth + 1)\n",
    "        except:\n",
    "            pass\n",
    "    show_tree('/kaggle/input')\n",
    "    raise FileNotFoundError(\"Adicione o modelo BERTimbau ao notebook!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== LORA LAYER (IMPLEMENTACAO MANUAL) ====\n",
    "print(\"[2/8] Definindo LoRA layers...\")\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"LoRA layer implementado do zero.\"\"\"\n",
    "    def __init__(self, in_features, out_features, r=16, alpha=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # Matrizes A e B do LoRA\n",
    "        self.lora_A = nn.Linear(in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, out_features, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Inicializacao\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"Linear layer com LoRA.\"\"\"\n",
    "    def __init__(self, linear, r=16, alpha=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, r, alpha, dropout\n",
    "        )\n",
    "        # Congelar pesos originais\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "print(f'LoRA config: r={LORA_R}, alpha={LORA_ALPHA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== MODELO COM LORA ====\n",
    "print(\"[3/8] Carregando modelo com LoRA...\")\n",
    "\n",
    "class BertWithLoRA(nn.Module):\n",
    "    def __init__(self, model_path, num_labels, r=16, alpha=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Aplicar LoRA nas camadas de atencao\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            # Query e Value (padrao do paper)\n",
    "            layer.attention.self.query = LinearWithLoRA(\n",
    "                layer.attention.self.query, r, alpha, dropout\n",
    "            )\n",
    "            layer.attention.self.value = LinearWithLoRA(\n",
    "                layer.attention.self.value, r, alpha, dropout\n",
    "            )\n",
    "        \n",
    "        # Classificador\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "model = BertWithLoRA(MODEL_PATH, NUM_CLASSES, LORA_R, LORA_ALPHA, LORA_DROPOUT)\n",
    "model.to(device)\n",
    "\n",
    "# Contar parametros treinaveis\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total params: {total_params:,}')\n",
    "print(f'Trainable params: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS ====\n",
    "print(\"[4/8] Carregando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATASET CLASS ====\n",
    "print(\"[5/8] Preparando dataset...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "print('Tokenizer carregado!')\n",
    "\n",
    "# Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['text'].values, train_df['label'].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df['label']\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df['text'].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAMENTO ====\n",
    "print(\"[6/8] Treinando modelo...\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad], lr=LR\n",
    ")\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            if 'labels' in batch:\n",
    "                labels.extend(batch['labels'].numpy())\n",
    "    if labels:\n",
    "        return f1_score(labels, preds, average='macro')\n",
    "    return preds\n",
    "\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Val F1={val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_lora_model.pt')\n",
    "        print(f'  -> Melhor modelo salvo! F1={best_f1:.4f}')\n",
    "\n",
    "print(f'\\nMelhor F1 validacao: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PREDICAO ====\n",
    "print(\"[7/8] Gerando predicoes...\")\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_lora_model.pt'))\n",
    "predictions = evaluate(model, test_loader)\n",
    "print(f'Predicoes geradas: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISSAO ====\n",
    "print(\"[8/8] Criando submissao...\")\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'label': predictions\n",
    "})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print('Submissao salva!')\n",
    "print(submission['label'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
