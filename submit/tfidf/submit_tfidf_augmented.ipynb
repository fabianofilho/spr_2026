{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e05d69b",
   "metadata": {},
   "source": [
    "# SPR 2026 - TF-IDF + Data Augmentation + Ensemble\n",
    "\n",
    "## ✅ Funciona 100% OFFLINE\n",
    "\n",
    "**Este notebook combina os 3 melhores modelos com Data Augmentation:**\n",
    "- LinearSVC (0.77885)\n",
    "- SGDClassifier (0.75019)\n",
    "- LogisticRegression (0.72935)\n",
    "\n",
    "---\n",
    "### Estratégias de Augmentation:\n",
    "1. **EDA (Easy Data Augmentation)** - Random swap, delete, insert\n",
    "2. **Noise Injection** - Simula erros de digitação (comum em laudos reais)\n",
    "3. **Segment Shuffle** - Embaralha sentenças mantendo semântica\n",
    "\n",
    "---\n",
    "### Configuração Kaggle:\n",
    "1. Settings → **Internet OFF**\n",
    "2. Settings → Accelerator → **None** (não precisa GPU)\n",
    "3. Run All\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758be3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - TF-IDF + DATA AUGMENTATION + ENSEMBLE =====\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - TF-IDF + Data Augmentation + Ensemble\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==== CONFIGURAÇÕES ====\n",
    "SEED = 42\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# Augmentation config\n",
    "AUG_FACTOR = 2  # Quantas vezes multiplicar dados de classes minoritárias\n",
    "MIN_SAMPLES = 500  # Mínimo de amostras por classe após augmentation\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS ====\n",
    "print(\"\\n[1/7] Carregando dados...\")\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"    Train original: {train.shape}\")\n",
    "print(f\"    Test: {test.shape}\")\n",
    "\n",
    "# Distribuição original\n",
    "print(\"\\nDistribuição das classes (original):\")\n",
    "class_dist = train['label'].value_counts().sort_index()\n",
    "for label, count in class_dist.items():\n",
    "    print(f\"    Classe {label}: {count:5d} ({100*count/len(train):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FUNÇÕES DE AUGMENTATION ====\n",
    "print(\"\\n[2/7] Definindo funções de augmentation...\")\n",
    "\n",
    "# Dicionário de sinônimos médicos (mamografia/radiologia)\n",
    "MEDICAL_SYNONYMS = {\n",
    "    'nodulo': ['nódulo', 'lesão nodular', 'formação nodular'],\n",
    "    'nódulo': ['nodulo', 'lesão nodular', 'formação nodular'],\n",
    "    'mama': ['glândula mamária', 'tecido mamário'],\n",
    "    'calcificação': ['microcalcificação', 'calcificações'],\n",
    "    'calcificações': ['microcalcificações', 'calcificação'],\n",
    "    'benigno': ['benigna', 'aspecto benigno'],\n",
    "    'benigna': ['benigno', 'aspecto benigno'],\n",
    "    'maligno': ['maligna', 'suspeito de malignidade'],\n",
    "    'maligna': ['maligno', 'suspeito de malignidade'],\n",
    "    'densas': ['densa', 'densidade aumentada'],\n",
    "    'densa': ['densas', 'densidade aumentada'],\n",
    "    'bilateral': ['em ambas as mamas', 'bilateralmente'],\n",
    "    'unilateral': ['em uma mama', 'unilateralmente'],\n",
    "    'assimetria': ['assimétrico', 'área assimétrica'],\n",
    "    'contornos': ['margens', 'bordas', 'limites'],\n",
    "    'margens': ['contornos', 'bordas', 'limites'],\n",
    "    'espiculado': ['espiculada', 'com espículas'],\n",
    "    'circunscrito': ['circunscrita', 'bem delimitado'],\n",
    "    'irregular': ['irregulares', 'de contornos irregulares'],\n",
    "    'regular': ['regulares', 'de contornos regulares'],\n",
    "    'direita': ['D', 'dir'],\n",
    "    'esquerda': ['E', 'esq'],\n",
    "    'superior': ['sup', 'parte superior'],\n",
    "    'inferior': ['inf', 'parte inferior'],\n",
    "    'lateral': ['lat', 'região lateral'],\n",
    "    'medial': ['med', 'região medial'],\n",
    "    'axila': ['região axilar', 'axilar'],\n",
    "    'linfonodo': ['gânglio', 'linfonodomegalia'],\n",
    "    'gânglio': ['linfonodo', 'linfonodomegalia'],\n",
    "    'parênquima': ['tecido', 'parênquima mamário'],\n",
    "    'estroma': ['tecido conjuntivo', 'estroma mamário'],\n",
    "    'cm': ['centímetros', 'centimetros'],\n",
    "    'mm': ['milímetros', 'milimetros'],\n",
    "    'apresenta': ['mostra', 'evidencia', 'revela'],\n",
    "    'observa-se': ['nota-se', 'visualiza-se', 'identifica-se'],\n",
    "    'nota-se': ['observa-se', 'visualiza-se', 'identifica-se'],\n",
    "    'sem': ['ausência de', 'não há'],\n",
    "    'com': ['apresentando', 'evidenciando'],\n",
    "}\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokeniza texto em palavras.\"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def detokenize(tokens, original_text):\n",
    "    \"\"\"Reconstrói texto a partir de tokens (mantém pontuação original).\"\"\"\n",
    "    result = original_text\n",
    "    orig_tokens = re.findall(r'\\b\\w+\\b', original_text.lower())\n",
    "    \n",
    "    for i, (orig, new) in enumerate(zip(orig_tokens, tokens)):\n",
    "        if orig != new:\n",
    "            # Preserva capitalização original\n",
    "            pattern = re.compile(re.escape(orig), re.IGNORECASE)\n",
    "            result = pattern.sub(new, result, count=1)\n",
    "    return result\n",
    "\n",
    "def synonym_replacement(text, n=2):\n",
    "    \"\"\"Substitui n palavras por sinônimos médicos.\"\"\"\n",
    "    words = tokenize(text)\n",
    "    candidates = [(i, w) for i, w in enumerate(words) if w in MEDICAL_SYNONYMS]\n",
    "    \n",
    "    if not candidates:\n",
    "        return text\n",
    "    \n",
    "    n = min(n, len(candidates))\n",
    "    selected = random.sample(candidates, n)\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    for i, word in selected:\n",
    "        synonyms = MEDICAL_SYNONYMS[word]\n",
    "        new_words[i] = random.choice(synonyms)\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_swap(text, n=2):\n",
    "    \"\"\"Troca n pares de palavras aleatoriamente.\"\"\"\n",
    "    words = tokenize(text)\n",
    "    if len(words) < 4:\n",
    "        return text\n",
    "    \n",
    "    n = min(n, len(words) // 2)\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_deletion(text, p=0.1):\n",
    "    \"\"\"Deleta palavras com probabilidade p.\"\"\"\n",
    "    words = tokenize(text)\n",
    "    if len(words) <= 5:\n",
    "        return text\n",
    "    \n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    \n",
    "    # Garantir mínimo de palavras\n",
    "    if len(new_words) < 3:\n",
    "        return text\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def noise_injection(text, p=0.05):\n",
    "    \"\"\"Simula erros de digitação (comum em laudos reais).\"\"\"\n",
    "    chars = list(text)\n",
    "    typo_map = {\n",
    "        'a': 'as', 'e': 'er', 'i': 'io', 'o': 'op',\n",
    "        's': 'sa', 'r': 're', 'n': 'nm', 'm': 'mn',\n",
    "        'c': 'cv', 'd': 'df', 'l': 'lk'\n",
    "    }\n",
    "    \n",
    "    result = []\n",
    "    for c in chars:\n",
    "        if c.lower() in typo_map and random.random() < p:\n",
    "            # 50% chance: duplicar letra, 50%: trocar por vizinha\n",
    "            if random.random() < 0.5:\n",
    "                result.append(c + c)  # duplicar\n",
    "            else:\n",
    "                result.append(random.choice(typo_map.get(c.lower(), c)))  # vizinha\n",
    "        else:\n",
    "            result.append(c)\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "def sentence_shuffle(text):\n",
    "    \"\"\"Embaralha frases do laudo (mantém semântica geral).\"\"\"\n",
    "    sentences = re.split(r'[.!?]\\s*', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) < 2:\n",
    "        return text\n",
    "    \n",
    "    random.shuffle(sentences)\n",
    "    return '. '.join(sentences) + '.'\n",
    "\n",
    "def eda_augment(text, alpha=0.3):\n",
    "    \"\"\"EDA completo: aplica todas as técnicas com probabilidade.\"\"\"\n",
    "    augmented = text\n",
    "    \n",
    "    if random.random() < alpha:\n",
    "        augmented = synonym_replacement(augmented, n=random.randint(1, 3))\n",
    "    \n",
    "    if random.random() < alpha:\n",
    "        augmented = random_swap(augmented, n=random.randint(1, 2))\n",
    "    \n",
    "    if random.random() < alpha * 0.5:  # Menos agressivo\n",
    "        augmented = random_deletion(augmented, p=0.1)\n",
    "    \n",
    "    if random.random() < alpha * 0.3:  # Ainda mais conservador\n",
    "        augmented = noise_injection(augmented, p=0.03)\n",
    "    \n",
    "    if random.random() < alpha * 0.5:\n",
    "        augmented = sentence_shuffle(augmented)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Testar\n",
    "sample = train.iloc[0]['text']\n",
    "print(f\"Original: {sample[:100]}...\")\n",
    "print(f\"Augmented: {eda_augment(sample)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== APLICAR AUGMENTATION ====\n",
    "print(\"\\n[3/7] Aplicando data augmentation...\")\n",
    "\n",
    "# Calcular quantas amostras gerar por classe\n",
    "class_counts = train['label'].value_counts()\n",
    "max_count = class_counts.max()\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "for label in sorted(train['label'].unique()):\n",
    "    class_data = train[train['label'] == label]\n",
    "    current_count = len(class_data)\n",
    "    \n",
    "    # Target: mínimo entre max_count e MIN_SAMPLES * AUG_FACTOR\n",
    "    target_count = max(min(max_count, current_count * AUG_FACTOR), MIN_SAMPLES)\n",
    "    samples_needed = target_count - current_count\n",
    "    \n",
    "    if samples_needed > 0:\n",
    "        # Gerar amostras augmentadas\n",
    "        for _ in range(samples_needed):\n",
    "            idx = random.randint(0, current_count - 1)\n",
    "            original_row = class_data.iloc[idx]\n",
    "            aug_text = eda_augment(original_row['text'])\n",
    "            augmented_data.append({\n",
    "                'id': f\"aug_{original_row['id']}_{len(augmented_data)}\",\n",
    "                'text': aug_text,\n",
    "                'label': label\n",
    "            })\n",
    "        print(f\"    Classe {label}: {current_count} → {current_count + samples_needed} (+{samples_needed})\")\n",
    "    else:\n",
    "        print(f\"    Classe {label}: {current_count} (sem augmentation)\")\n",
    "\n",
    "# Combinar dados originais + augmentados\n",
    "if augmented_data:\n",
    "    aug_df = pd.DataFrame(augmented_data)\n",
    "    train_aug = pd.concat([train, aug_df], ignore_index=True)\n",
    "else:\n",
    "    train_aug = train.copy()\n",
    "\n",
    "print(f\"\\nTrain após augmentation: {len(train_aug)} ({len(train_aug) - len(train)} novas amostras)\")\n",
    "\n",
    "# Nova distribuição\n",
    "print(\"\\nNova distribuição:\")\n",
    "new_dist = train_aug['label'].value_counts().sort_index()\n",
    "for label, count in new_dist.items():\n",
    "    print(f\"    Classe {label}: {count:5d} ({100*count/len(train_aug):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea29565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PREPROCESSAMENTO ====\n",
    "print(\"\\n[4/7] Preprocessamento de texto...\")\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocessamento leve mantendo termos médicos.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    # Remove números isolados mas mantém medidas (ex: 2,3cm)\n",
    "    text = re.sub(r'\\b\\d{4,}\\b', '', text)  # Remove IDs longos\n",
    "    # Normaliza espaços\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "X_train_aug = train_aug['text'].apply(preprocess)\n",
    "y_train_aug = train_aug['label'].values\n",
    "X_test = test['text'].apply(preprocess)\n",
    "\n",
    "print(f\"    X_train_aug: {len(X_train_aug)}\")\n",
    "print(f\"    X_test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TF-IDF VETORIZAÇÃO ====\n",
    "print(\"\\n[5/7] Vetorização TF-IDF...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_aug)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"    Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"    Train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"    Test shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d899306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAR MODELOS TOP 3 ====\n",
    "print(\"\\n[6/7] Treinando Top 3 modelos...\")\n",
    "\n",
    "# 1. LinearSVC (melhor single model: 0.77885)\n",
    "print(\"\\n--- LinearSVC ---\")\n",
    "svc = LinearSVC(\n",
    "    C=0.5,\n",
    "    loss='squared_hinge',\n",
    "    max_iter=3000,\n",
    "    class_weight='balanced',\n",
    "    dual='auto',\n",
    "    random_state=SEED\n",
    ")\n",
    "# Calibrar para ter predict_proba\n",
    "svc_calibrated = CalibratedClassifierCV(svc, cv=3)\n",
    "svc_calibrated.fit(X_train_tfidf, y_train_aug)\n",
    "print(\"    Treinado!\")\n",
    "\n",
    "# 2. SGDClassifier (segundo melhor: 0.75019)\n",
    "print(\"\\n--- SGDClassifier ---\")\n",
    "sgd = SGDClassifier(\n",
    "    loss='modified_huber',  # Permite predict_proba\n",
    "    alpha=1e-4,\n",
    "    penalty='l2',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "sgd.fit(X_train_tfidf, y_train_aug)\n",
    "print(\"    Treinado!\")\n",
    "\n",
    "# 3. LogisticRegression (terceiro: 0.72935)\n",
    "print(\"\\n--- LogisticRegression ---\")\n",
    "lr = LogisticRegression(\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    multi_class='multinomial',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr.fit(X_train_tfidf, y_train_aug)\n",
    "print(\"    Treinado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== VALIDAÇÃO CRUZADA ====\n",
    "print(\"\\n--- Cross-Validation (5-fold) ---\")\n",
    "\n",
    "# Usar dados originais para CV (sem augmentation) para avaliação justa\n",
    "X_train_orig = train['text'].apply(preprocess)\n",
    "y_train_orig = train['label'].values\n",
    "X_train_orig_tfidf = tfidf.transform(X_train_orig)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Treinar modelos no dataset original para CV justa\n",
    "svc_cv = CalibratedClassifierCV(LinearSVC(C=0.5, class_weight='balanced', dual='auto', random_state=SEED), cv=3)\n",
    "sgd_cv = SGDClassifier(loss='modified_huber', alpha=1e-4, class_weight='balanced', random_state=SEED)\n",
    "lr_cv = LogisticRegression(C=1.0, class_weight='balanced', multi_class='multinomial', random_state=SEED)\n",
    "\n",
    "svc_scores = cross_val_score(svc_cv, X_train_orig_tfidf, y_train_orig, cv=cv, scoring='f1_macro')\n",
    "sgd_scores = cross_val_score(sgd_cv, X_train_orig_tfidf, y_train_orig, cv=cv, scoring='f1_macro')\n",
    "lr_scores = cross_val_score(lr_cv, X_train_orig_tfidf, y_train_orig, cv=cv, scoring='f1_macro')\n",
    "\n",
    "print(f\"\\n    LinearSVC:     {svc_scores.mean():.5f} ± {svc_scores.std():.5f}\")\n",
    "print(f\"    SGDClassifier: {sgd_scores.mean():.5f} ± {sgd_scores.std():.5f}\")\n",
    "print(f\"    LogisticReg:   {lr_scores.mean():.5f} ± {lr_scores.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2790e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ENSEMBLE PREDICTIONS ====\n",
    "print(\"\\n[7/7] Gerando predições ensemble...\")\n",
    "\n",
    "# Predições individuais\n",
    "pred_svc = svc_calibrated.predict(X_test_tfidf)\n",
    "pred_sgd = sgd.predict(X_test_tfidf)\n",
    "pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "# Probabilidades\n",
    "proba_svc = svc_calibrated.predict_proba(X_test_tfidf)\n",
    "proba_sgd = sgd.predict_proba(X_test_tfidf)\n",
    "proba_lr = lr.predict_proba(X_test_tfidf)\n",
    "\n",
    "# ==== ESTRATÉGIAS DE ENSEMBLE ====\n",
    "\n",
    "# 1. Hard Voting (maioria)\n",
    "pred_hard = []\n",
    "for i in range(len(test)):\n",
    "    votes = [pred_svc[i], pred_sgd[i], pred_lr[i]]\n",
    "    pred_hard.append(Counter(votes).most_common(1)[0][0])\n",
    "pred_hard = np.array(pred_hard)\n",
    "\n",
    "# 2. Soft Voting (média das probabilidades)\n",
    "proba_avg = (proba_svc + proba_sgd + proba_lr) / 3\n",
    "pred_soft = np.argmax(proba_avg, axis=1)\n",
    "\n",
    "# 3. Weighted Voting (pesos baseados no score público)\n",
    "# LinearSVC: 0.77885, SGD: 0.75019, LR: 0.72935\n",
    "weights = np.array([0.77885, 0.75019, 0.72935])\n",
    "weights = weights / weights.sum()  # Normalizar\n",
    "\n",
    "proba_weighted = (proba_svc * weights[0] + proba_sgd * weights[1] + proba_lr * weights[2])\n",
    "pred_weighted = np.argmax(proba_weighted, axis=1)\n",
    "\n",
    "print(\"\\n--- Comparação das estratégias ---\")\n",
    "print(f\"Hard Voting - distribuição:  {dict(Counter(pred_hard))}\")\n",
    "print(f\"Soft Voting - distribuição:  {dict(Counter(pred_soft))}\")\n",
    "print(f\"Weighted    - distribuição:  {dict(Counter(pred_weighted))}\")\n",
    "print(f\"LinearSVC   - distribuição:  {dict(Counter(pred_svc))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081633ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CRIAR MÚLTIPLAS SUBMISSÕES ====\n",
    "print(\"\\n--- Gerando submissões ---\")\n",
    "\n",
    "# 1. LinearSVC puro (augmented) - provavelmente o melhor\n",
    "submission_svc = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_svc\n",
    "})\n",
    "submission_svc.to_csv('/kaggle/working/submission_linearsvc_aug.csv', index=False)\n",
    "print(\"    submission_linearsvc_aug.csv\")\n",
    "\n",
    "# 2. Weighted Ensemble\n",
    "submission_weighted = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_weighted\n",
    "})\n",
    "submission_weighted.to_csv('/kaggle/working/submission_weighted_ensemble.csv', index=False)\n",
    "print(\"    submission_weighted_ensemble.csv\")\n",
    "\n",
    "# 3. Soft Voting\n",
    "submission_soft = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_soft\n",
    "})\n",
    "submission_soft.to_csv('/kaggle/working/submission_soft_ensemble.csv', index=False)\n",
    "print(\"    submission_soft_ensemble.csv\")\n",
    "\n",
    "# PRINCIPAL: usar weighted (melhor teórico)\n",
    "submission_weighted.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUÍDO - submission.csv = Weighted Ensemble\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nArquivos gerados:\")\n",
    "print(\"  - submission.csv (weighted ensemble)\")\n",
    "print(\"  - submission_linearsvc_aug.csv (LinearSVC + augmentation)\")\n",
    "print(\"  - submission_weighted_ensemble.csv\")\n",
    "print(\"  - submission_soft_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ANÁLISE FINAL ====\n",
    "print(\"\\n--- Resumo Final ---\")\n",
    "\n",
    "print(f\"\\nDados de treino: {len(train)} → {len(train_aug)} (augmented)\")\n",
    "print(f\"Dados de teste: {len(test)}\")\n",
    "\n",
    "print(\"\\nModelos treinados:\")\n",
    "print(\"  1. LinearSVC (C=0.5, balanced)\")\n",
    "print(\"  2. SGDClassifier (modified_huber, balanced)\")\n",
    "print(\"  3. LogisticRegression (multinomial, balanced)\")\n",
    "\n",
    "print(\"\\nTécnicas de Augmentation:\")\n",
    "print(\"  - Synonym replacement (termos médicos)\")\n",
    "print(\"  - Random word swap\")\n",
    "print(\"  - Random word deletion\")\n",
    "print(\"  - Noise injection (typos)\")\n",
    "print(\"  - Sentence shuffle\")\n",
    "\n",
    "print(\"\\nEstrategias de Ensemble:\")\n",
    "print(\"  - Hard Voting (maioria)\")\n",
    "print(\"  - Soft Voting (média proba)\")\n",
    "print(\"  - Weighted Voting (pesos por score)\")\n",
    "\n",
    "print(\"\\n✅ Pronto para submissão!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
