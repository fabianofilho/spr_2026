{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b8ad54",
   "metadata": {},
   "source": [
    "# SPR 2026 - TF-IDF Pretrain (Domain Adaptation)\n",
    "\n",
    "## ✅ Funciona OFFLINE (com dataset adicional)\n",
    "\n",
    "**Este notebook usa dataset médico externo para domain adaptation:**\n",
    "- Treina TF-IDF em corpus médico expandido\n",
    "- Melhora representação de termos médicos raros\n",
    "- Combina vocabulário de múltiplas fontes\n",
    "\n",
    "---\n",
    "### Configuração Kaggle:\n",
    "1. **Add Input** → **Datasets** → buscar `medicaltranscriptions`\n",
    "   - Dataset: `tboyle10/medicaltranscriptions`\n",
    "2. Settings → **Internet OFF**\n",
    "3. Settings → Accelerator → **None** (não precisa GPU)\n",
    "4. Run All\n",
    "\n",
    "---\n",
    "### Estratégia:\n",
    "1. Carregar Medical Transcriptions (5000+ textos médicos)\n",
    "2. Combinar com dados de treino SPR 2026\n",
    "3. Treinar TF-IDF no corpus combinado (vocabulário expandido)\n",
    "4. Fine-tune classificadores apenas nos dados SPR 2026\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - TF-IDF PRETRAIN (DOMAIN ADAPTATION) =====\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - TF-IDF Pretrain (Domain Adaptation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==== CONFIGURAÇÕES ====\n",
    "SEED = 42\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# Paths possíveis para Medical Transcriptions\n",
    "PRETRAIN_PATHS = [\n",
    "    '/kaggle/input/medicaltranscriptions/mtsamples.csv',\n",
    "    '/kaggle/input/medical-transcriptions/mtsamples.csv',\n",
    "    '/kaggle/input/tboyle10-medicaltranscriptions/mtsamples.csv',\n",
    "]\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9704838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS COMPETIÇÃO ====\n",
    "print(\"\\n[1/7] Carregando dados da competição...\")\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"    Train SPR: {train.shape}\")\n",
    "print(f\"    Test SPR: {test.shape}\")\n",
    "\n",
    "# Distribuição\n",
    "print(\"\\nDistribuição das classes:\")\n",
    "for label, count in train['label'].value_counts().sort_index().items():\n",
    "    print(f\"    Classe {label}: {count:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0235e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DATASET DE PRÉ-TREINO ====\n",
    "print(\"\\n[2/7] Carregando dataset de pré-treino...\")\n",
    "\n",
    "pretrain_df = None\n",
    "\n",
    "# Tentar encontrar o dataset\n",
    "for path in PRETRAIN_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        pretrain_df = pd.read_csv(path)\n",
    "        print(f\"    Encontrado: {path}\")\n",
    "        break\n",
    "\n",
    "# Se não encontrou, buscar em /kaggle/input\n",
    "if pretrain_df is None:\n",
    "    print(\"    Buscando em /kaggle/input...\")\n",
    "    for root, dirs, files in os.walk('/kaggle/input'):\n",
    "        for f in files:\n",
    "            if 'mtsample' in f.lower() or 'transcription' in f.lower():\n",
    "                if f.endswith('.csv'):\n",
    "                    path = os.path.join(root, f)\n",
    "                    pretrain_df = pd.read_csv(path)\n",
    "                    print(f\"    Encontrado: {path}\")\n",
    "                    break\n",
    "        if pretrain_df is not None:\n",
    "            break\n",
    "\n",
    "# Verificar se encontrou\n",
    "if pretrain_df is None:\n",
    "    print(\"\\n⚠️ Dataset de pré-treino não encontrado!\")\n",
    "    print(\"\\nPara usar este notebook:\")\n",
    "    print(\"1. Vá em 'Add Data'\")\n",
    "    print(\"2. Busque por 'medicaltranscriptions'\")\n",
    "    print(\"3. Adicione o dataset 'tboyle10/medicaltranscriptions'\")\n",
    "    print(\"\\nContinuando sem pré-treino (usando apenas dados da competição)...\")\n",
    "    USE_PRETRAIN = False\n",
    "else:\n",
    "    USE_PRETRAIN = True\n",
    "    print(f\"\\n    Pretrain dataset shape: {pretrain_df.shape}\")\n",
    "    print(f\"    Colunas: {list(pretrain_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PREPARAR CORPUS COMBINADO ====\n",
    "print(\"\\n[3/7] Preparando corpus combinado...\")\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocessamento básico.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d{5,}', '', text)  # Remove IDs longos\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Textos da competição\n",
    "spr_texts = train['text'].apply(preprocess).tolist()\n",
    "test_texts = test['text'].apply(preprocess).tolist()\n",
    "\n",
    "if USE_PRETRAIN:\n",
    "    # Identificar coluna de texto no dataset de pré-treino\n",
    "    text_col = None\n",
    "    for col in ['transcription', 'text', 'description', 'medical_specialty']:\n",
    "        if col in pretrain_df.columns:\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    if text_col is None:\n",
    "        # Usar a coluna com mais texto\n",
    "        text_col = pretrain_df.select_dtypes(include='object').columns[0]\n",
    "    \n",
    "    print(f\"    Usando coluna: '{text_col}'\")\n",
    "    \n",
    "    # Filtrar textos válidos\n",
    "    pretrain_texts = pretrain_df[text_col].dropna().apply(preprocess).tolist()\n",
    "    pretrain_texts = [t for t in pretrain_texts if len(t) > 50]  # Mínimo 50 chars\n",
    "    \n",
    "    # Combinar todos os textos para treinar TF-IDF\n",
    "    all_texts = spr_texts + pretrain_texts + test_texts\n",
    "    \n",
    "    print(f\"\\n    Textos SPR (train): {len(spr_texts)}\")\n",
    "    print(f\"    Textos pretrain: {len(pretrain_texts)}\")\n",
    "    print(f\"    Textos SPR (test): {len(test_texts)}\")\n",
    "    print(f\"    Total corpus: {len(all_texts)}\")\n",
    "else:\n",
    "    # Sem pré-treino: usar apenas dados da competição\n",
    "    all_texts = spr_texts + test_texts\n",
    "    print(f\"    Corpus (sem pretrain): {len(all_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TF-IDF COM VOCABULÁRIO EXPANDIDO ====\n",
    "print(\"\\n[4/7] Treinando TF-IDF com vocabulário expandido...\")\n",
    "\n",
    "# Treinar TF-IDF no corpus completo (domain adaptation)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,  # Mais features para capturar vocabulário médico\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "# Fit no corpus completo (transfere conhecimento do pretrain)\n",
    "tfidf.fit(all_texts)\n",
    "\n",
    "print(f\"    Vocabulário: {len(tfidf.vocabulary_)} termos\")\n",
    "\n",
    "# Transform apenas nos dados da competição\n",
    "X_train_tfidf = tfidf.transform(spr_texts)\n",
    "X_test_tfidf = tfidf.transform(test_texts)\n",
    "\n",
    "y_train = train['label'].values\n",
    "\n",
    "print(f\"    X_train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"    X_test shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ANÁLISE DO VOCABULÁRIO ====\n",
    "print(\"\\n--- Análise do vocabulário ---\")\n",
    "\n",
    "# Termos médicos importantes no vocabulário\n",
    "medical_terms = [\n",
    "    'nodulo', 'nódulo', 'calcificação', 'calcificacao', 'mama', \n",
    "    'benigno', 'maligno', 'birads', 'bi-rads', 'categoria',\n",
    "    'assimetria', 'espiculado', 'massa', 'lesão', 'linfonodo',\n",
    "    'mamografia', 'ultrassom', 'biópsia', 'axila', 'parênquima',\n",
    "]\n",
    "\n",
    "found_terms = [t for t in medical_terms if t in tfidf.vocabulary_]\n",
    "missing_terms = [t for t in medical_terms if t not in tfidf.vocabulary_]\n",
    "\n",
    "print(f\"\\n    Termos médicos encontrados: {len(found_terms)}/{len(medical_terms)}\")\n",
    "if missing_terms:\n",
    "    print(f\"    Faltando: {missing_terms[:10]}...\")\n",
    "\n",
    "# Top palavras por frequência\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "idf_scores = tfidf.idf_\n",
    "\n",
    "# Palavras mais comuns (menor IDF)\n",
    "common_idx = np.argsort(idf_scores)[:20]\n",
    "print(f\"\\n    Top 20 termos mais comuns:\")\n",
    "print(f\"    {[feature_names[i] for i in common_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16051ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAR MODELOS TOP 3 ====\n",
    "print(\"\\n[5/7] Treinando Top 3 modelos...\")\n",
    "\n",
    "# 1. LinearSVC (melhor single model: 0.77885)\n",
    "print(\"\\n--- LinearSVC ---\")\n",
    "svc = LinearSVC(\n",
    "    C=0.5,\n",
    "    loss='squared_hinge',\n",
    "    max_iter=3000,\n",
    "    class_weight='balanced',\n",
    "    dual='auto',\n",
    "    random_state=SEED\n",
    ")\n",
    "svc_calibrated = CalibratedClassifierCV(svc, cv=3)\n",
    "svc_calibrated.fit(X_train_tfidf, y_train)\n",
    "print(\"    Treinado!\")\n",
    "\n",
    "# 2. SGDClassifier (segundo melhor: 0.75019)\n",
    "print(\"\\n--- SGDClassifier ---\")\n",
    "sgd = SGDClassifier(\n",
    "    loss='modified_huber',\n",
    "    alpha=1e-4,\n",
    "    penalty='l2',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "sgd.fit(X_train_tfidf, y_train)\n",
    "print(\"    Treinado!\")\n",
    "\n",
    "# 3. LogisticRegression (terceiro: 0.72935)\n",
    "print(\"\\n--- LogisticRegression ---\")\n",
    "lr = LogisticRegression(\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    multi_class='multinomial',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "print(\"    Treinado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CROSS-VALIDATION ====\n",
    "print(\"\\n[6/7] Cross-Validation (5-fold)...\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "svc_cv = CalibratedClassifierCV(\n",
    "    LinearSVC(C=0.5, class_weight='balanced', dual='auto', random_state=SEED), cv=3\n",
    ")\n",
    "sgd_cv = SGDClassifier(\n",
    "    loss='modified_huber', alpha=1e-4, class_weight='balanced', random_state=SEED\n",
    ")\n",
    "lr_cv = LogisticRegression(\n",
    "    C=1.0, class_weight='balanced', multi_class='multinomial', random_state=SEED\n",
    ")\n",
    "\n",
    "svc_scores = cross_val_score(svc_cv, X_train_tfidf, y_train, cv=cv, scoring='f1_macro')\n",
    "sgd_scores = cross_val_score(sgd_cv, X_train_tfidf, y_train, cv=cv, scoring='f1_macro')\n",
    "lr_scores = cross_val_score(lr_cv, X_train_tfidf, y_train, cv=cv, scoring='f1_macro')\n",
    "\n",
    "print(f\"\\n    LinearSVC:     {svc_scores.mean():.5f} ± {svc_scores.std():.5f}\")\n",
    "print(f\"    SGDClassifier: {sgd_scores.mean():.5f} ± {sgd_scores.std():.5f}\")\n",
    "print(f\"    LogisticReg:   {lr_scores.mean():.5f} ± {lr_scores.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ENSEMBLE PREDICTIONS ====\n",
    "print(\"\\n[7/7] Gerando predições ensemble...\")\n",
    "\n",
    "# Predições individuais\n",
    "pred_svc = svc_calibrated.predict(X_test_tfidf)\n",
    "pred_sgd = sgd.predict(X_test_tfidf)\n",
    "pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "# Probabilidades\n",
    "proba_svc = svc_calibrated.predict_proba(X_test_tfidf)\n",
    "proba_sgd = sgd.predict_proba(X_test_tfidf)\n",
    "proba_lr = lr.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Weighted Voting\n",
    "weights = np.array([0.77885, 0.75019, 0.72935])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "proba_weighted = (\n",
    "    proba_svc * weights[0] + \n",
    "    proba_sgd * weights[1] + \n",
    "    proba_lr * weights[2]\n",
    ")\n",
    "pred_weighted = np.argmax(proba_weighted, axis=1)\n",
    "\n",
    "print(f\"\\n    LinearSVC dist:  {dict(Counter(pred_svc))}\")\n",
    "print(f\"    Weighted dist:   {dict(Counter(pred_weighted))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d400e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISSÕES ====\n",
    "print(\"\\n--- Gerando submissões ---\")\n",
    "\n",
    "# 1. LinearSVC com pretrain\n",
    "submission_svc = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_svc\n",
    "})\n",
    "submission_svc.to_csv('/kaggle/working/submission_linearsvc_pretrain.csv', index=False)\n",
    "print(\"    submission_linearsvc_pretrain.csv\")\n",
    "\n",
    "# 2. Weighted Ensemble com pretrain\n",
    "submission_weighted = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_weighted\n",
    "})\n",
    "submission_weighted.to_csv('/kaggle/working/submission_weighted_pretrain.csv', index=False)\n",
    "print(\"    submission_weighted_pretrain.csv\")\n",
    "\n",
    "# PRINCIPAL: LinearSVC (best single model)\n",
    "submission_svc.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "pretrain_status = \"COM\" if USE_PRETRAIN else \"SEM\"\n",
    "print(f\"CONCLUÍDO - submission.csv ({pretrain_status} pretrain)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nDomain Adaptation:\")\n",
    "if USE_PRETRAIN:\n",
    "    print(f\"  - Corpus expandido: {len(all_texts)} textos\")\n",
    "    print(f\"  - Vocabulário: {len(tfidf.vocabulary_)} termos\")\n",
    "    print(\"  - Melhor representação de termos médicos raros\")\n",
    "else:\n",
    "    print(\"  - Usando apenas dados da competição\")\n",
    "    print(\"  - Adicione 'medicaltranscriptions' para domain adaptation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
