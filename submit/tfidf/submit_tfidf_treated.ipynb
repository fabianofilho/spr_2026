{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c9f38d",
   "metadata": {},
   "source": [
    "# SPR 2026 - TF-IDF Treated (Processamento Avançado)\n",
    "\n",
    "## ✅ Funciona 100% OFFLINE\n",
    "\n",
    "**Este notebook aplica pipeline completo de tratamento de texto:**\n",
    "- Limpeza e normalização\n",
    "- Stop words customizadas (mantendo termos médicos)\n",
    "- Extração de features BI-RADS\n",
    "- Class weights para desbalanceamento\n",
    "\n",
    "---\n",
    "### Modelos combinados:\n",
    "1. LinearSVC (0.77885)\n",
    "2. SGDClassifier (0.75019)\n",
    "3. LogisticRegression (0.72935)\n",
    "\n",
    "---\n",
    "### Configuração Kaggle:\n",
    "1. Settings → **Internet OFF**\n",
    "2. Settings → Accelerator → **None** (não precisa GPU)\n",
    "3. Run All\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78763cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPR 2026 - TF-IDF TREATED (PROCESSAMENTO AVANÇADO) =====\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - TF-IDF Treated (Processamento Avançado)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==== CONFIGURAÇÕES ====\n",
    "SEED = 42\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR DADOS ====\n",
    "print(\"\\n[1/8] Carregando dados...\")\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"    Train: {train.shape}\")\n",
    "print(f\"    Test: {test.shape}\")\n",
    "\n",
    "# Distribuição original\n",
    "print(\"\\nDistribuição das classes:\")\n",
    "class_dist = train['label'].value_counts().sort_index()\n",
    "for label, count in class_dist.items():\n",
    "    print(f\"    Classe {label}: {count:5d} ({100*count/len(train):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STOP WORDS CUSTOMIZADAS ====\n",
    "print(\"\\n[2/8] Definindo stop words customizadas...\")\n",
    "\n",
    "# Stop words genéricas em português (MANTER termos médicos!)\n",
    "STOP_WORDS = {\n",
    "    # Artigos\n",
    "    'o', 'a', 'os', 'as', 'um', 'uma', 'uns', 'umas',\n",
    "    # Preposições\n",
    "    'de', 'da', 'do', 'das', 'dos', 'em', 'na', 'no', 'nas', 'nos',\n",
    "    'para', 'por', 'pela', 'pelo', 'pelas', 'pelos',\n",
    "    'com', 'sem', 'sob', 'sobre', 'entre', 'ate', 'até',\n",
    "    # Conjunções\n",
    "    'e', 'ou', 'mas', 'porem', 'porém', 'contudo', 'entretanto',\n",
    "    'que', 'quando', 'como', 'se', 'porque', 'pois',\n",
    "    # Pronomes\n",
    "    'eu', 'tu', 'ele', 'ela', 'nos', 'nós', 'vos', 'eles', 'elas',\n",
    "    'me', 'te', 'se', 'lhe', 'lhes',\n",
    "    'meu', 'minha', 'meus', 'minhas', 'seu', 'sua', 'seus', 'suas',\n",
    "    'esse', 'essa', 'esses', 'essas', 'este', 'esta', 'estes', 'estas',\n",
    "    'isso', 'isto', 'aquele', 'aquela', 'aqueles', 'aquelas', 'aquilo',\n",
    "    # Verbos auxiliares\n",
    "    'ser', 'estar', 'ter', 'haver', 'foi', 'era', 'é', 'são',\n",
    "    'está', 'estão', 'tem', 'têm', 'há',\n",
    "    # Advérbios comuns\n",
    "    'não', 'nao', 'sim', 'já', 'ja', 'ainda', 'também', 'tambem',\n",
    "    'muito', 'pouco', 'mais', 'menos', 'bem', 'mal',\n",
    "    'aqui', 'ali', 'lá', 'la', 'onde', 'quando',\n",
    "    # Outros\n",
    "    'ao', 'aos', 'pela', 'pelo', 'pelas', 'pelos',\n",
    "    'qual', 'quais', 'quanto', 'quanta', 'quantos', 'quantas',\n",
    "    'todo', 'toda', 'todos', 'todas', 'outro', 'outra', 'outros', 'outras',\n",
    "    'mesmo', 'mesma', 'mesmos', 'mesmas',\n",
    "    'sendo', 'sido', 'tendo', 'tido',\n",
    "    # Pontuação e números isolados\n",
    "    'ii', 'iii', 'iv', 'vi', 'vii', 'viii', 'ix', 'xi', 'xii',\n",
    "}\n",
    "\n",
    "# Termos médicos para NUNCA remover\n",
    "MEDICAL_TERMS = {\n",
    "    # BI-RADS\n",
    "    'birads', 'bi-rads', 'bi', 'rads', 'categoria',\n",
    "    # Achados\n",
    "    'nodulo', 'nódulo', 'massa', 'lesão', 'lesao', 'tumor',\n",
    "    'calcificação', 'calcificacao', 'calcificações', 'microcalcificação',\n",
    "    'assimetria', 'assimétrica', 'distorção', 'distorcao',\n",
    "    'densidade', 'espessamento', 'retração', 'retracao',\n",
    "    # Características\n",
    "    'benigno', 'benigna', 'maligno', 'maligna', 'suspeito', 'suspeita',\n",
    "    'espiculado', 'espiculada', 'irregular', 'circunscrito', 'circunscrita',\n",
    "    'lobulado', 'lobulada', 'oval', 'redondo', 'redonda',\n",
    "    'indistinto', 'indistinta', 'obscurecido', 'microlobulado',\n",
    "    # Localização\n",
    "    'mama', 'mamas', 'mamária', 'mamario', 'bilateral', 'unilateral',\n",
    "    'direita', 'esquerda', 'superior', 'inferior', 'lateral', 'medial',\n",
    "    'retroareolar', 'axilar', 'axila', 'quadrante',\n",
    "    # Tecido\n",
    "    'parênquima', 'parenquima', 'fibroglandular', 'adiposo', 'gorduroso',\n",
    "    'denso', 'densa', 'heterogêneo', 'heterogeneo', 'homogêneo',\n",
    "    # Linfonodos\n",
    "    'linfonodo', 'linfonodos', 'gânglio', 'ganglio', 'adenopatia',\n",
    "    # Procedimentos\n",
    "    'mamografia', 'ultrassom', 'ultrassonografia', 'biópsia', 'biopsia',\n",
    "    'aspiração', 'aspiracao', 'punção', 'puncao', 'core',\n",
    "    # Conclusões\n",
    "    'negativo', 'negativa', 'normal', 'inconclusivo',\n",
    "    'provavelmente', 'altamente', 'confirmado', 'confirmada',\n",
    "    # Medidas\n",
    "    'cm', 'mm', 'centímetros', 'milímetros',\n",
    "}\n",
    "\n",
    "# Remover termos médicos das stop words\n",
    "STOP_WORDS = STOP_WORDS - MEDICAL_TERMS\n",
    "print(f\"    Stop words: {len(STOP_WORDS)} palavras\")\n",
    "print(f\"    Termos médicos protegidos: {len(MEDICAL_TERMS)} palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FUNÇÕES DE PROCESSAMENTO ====\n",
    "print(\"\\n[3/8] Definindo pipeline de processamento...\")\n",
    "\n",
    "# Dicionário de abreviações médicas\n",
    "ABBREVIATIONS = {\n",
    "    r'\\bqse\\b': 'quadrante superior externo',\n",
    "    r'\\bqsi\\b': 'quadrante superior interno',\n",
    "    r'\\bqie\\b': 'quadrante inferior externo',\n",
    "    r'\\bqii\\b': 'quadrante inferior interno',\n",
    "    r'\\busg\\b': 'ultrassonografia',\n",
    "    r'\\bus\\b': 'ultrassom',\n",
    "    r'\\bmmg\\b': 'mamografia',\n",
    "    r'\\bmamo\\b': 'mamografia',\n",
    "    r'\\bln\\b': 'linfonodo',\n",
    "    r'\\blns\\b': 'linfonodos',\n",
    "    r'\\bca\\b': 'câncer',\n",
    "    r'\\bcm\\b': 'centímetros',\n",
    "    r'\\bmm\\b': 'milímetros',\n",
    "    r'\\bd\\b': 'direita',\n",
    "    r'\\be\\b': 'esquerda',\n",
    "    r'\\bdir\\b': 'direita',\n",
    "    r'\\besq\\b': 'esquerda',\n",
    "    r'\\bsup\\b': 'superior',\n",
    "    r'\\binf\\b': 'inferior',\n",
    "    r'\\blat\\b': 'lateral',\n",
    "    r'\\bmed\\b': 'medial',\n",
    "    r'\\bc/\\b': 'com',\n",
    "    r'\\bs/\\b': 'sem',\n",
    "}\n",
    "\n",
    "# Padrões BI-RADS para extração\n",
    "BIRADS_PATTERNS = [\n",
    "    (r'bi-?rads?\\s*:?\\s*([0-6])', 'birads_explicit'),\n",
    "    (r'categoria\\s*:?\\s*([0-6])', 'birads_categoria'),\n",
    "    (r'classe\\s*:?\\s*([0-6])', 'birads_classe'),\n",
    "]\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    \"\"\"Expande abreviações médicas comuns.\"\"\"\n",
    "    for pattern, replacement in ABBREVIATIONS.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    \"\"\"Normaliza números e medidas.\"\"\"\n",
    "    # Converte medidas para padrão\n",
    "    text = re.sub(r'(\\d+)[,\\.](\\d+)\\s*(cm|mm)', r'\\1\\2\\3', text)\n",
    "    # Remove IDs e números longos (> 4 dígitos)\n",
    "    text = re.sub(r'\\b\\d{5,}\\b', '', text)\n",
    "    # Mantém medidas mas remove outros números isolados\n",
    "    text = re.sub(r'(?<!\\d)\\b\\d{1,2}\\b(?!\\s*(cm|mm|anos?))', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_boilerplate(text):\n",
    "    \"\"\"Remove seções padrão de laudos.\"\"\"\n",
    "    # Remove datas\n",
    "    text = re.sub(r'\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}', '', text)\n",
    "    # Remove horários\n",
    "    text = re.sub(r'\\d{1,2}:\\d{2}(:\\d{2})?', '', text)\n",
    "    # Remove cabeçalhos comuns\n",
    "    patterns = [\n",
    "        r'laudo\\s*:',\n",
    "        r'paciente\\s*:',\n",
    "        r'data\\s*:',\n",
    "        r'médico\\s*:',\n",
    "        r'crm\\s*:?\\s*\\d+',\n",
    "        r'\\bdr\\.?\\s+\\w+',\n",
    "        r'assinatura\\s*:?',\n",
    "        r'\\bobs\\s*:?',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def extract_birads_features(text):\n",
    "    \"\"\"Extrai menções explícitas de BI-RADS.\"\"\"\n",
    "    features = {\n",
    "        'birads_mentioned': 0,\n",
    "        'birads_value': -1,  # -1 = não encontrado\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern, name in BIRADS_PATTERNS:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            features['birads_mentioned'] = 1\n",
    "            features['birads_value'] = int(match.group(1))\n",
    "            break\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_medical_features(text):\n",
    "    \"\"\"Extrai features estruturadas do texto.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    features = {\n",
    "        # Achados principais\n",
    "        'has_nodulo': 1 if re.search(r'n[óo]dulo|massa|les[ãa]o', text_lower) else 0,\n",
    "        'has_calcificacao': 1 if re.search(r'calcifica[çc][ãa]o|microcalc', text_lower) else 0,\n",
    "        'has_assimetria': 1 if re.search(r'assim[ée]tr', text_lower) else 0,\n",
    "        'has_distorcao': 1 if re.search(r'distor[çc][ãa]o', text_lower) else 0,\n",
    "        \n",
    "        # Características\n",
    "        'is_benigno': 1 if re.search(r'benign[oa]|provavelmente\\s+benign', text_lower) else 0,\n",
    "        'is_maligno': 1 if re.search(r'malign[oa]|suspeito|alta\\s+suspeita', text_lower) else 0,\n",
    "        'is_normal': 1 if re.search(r'normal|negativ[oa]|sem\\s+(achados|altera)', text_lower) else 0,\n",
    "        'is_inconclusivo': 1 if re.search(r'inconclusiv|adicional|complement', text_lower) else 0,\n",
    "        \n",
    "        # Morfologia\n",
    "        'has_espiculado': 1 if re.search(r'espiculad|irregular|indistint', text_lower) else 0,\n",
    "        'has_circunscrito': 1 if re.search(r'circunscrit|bem\\s+delimitad|regular', text_lower) else 0,\n",
    "        \n",
    "        # Localização\n",
    "        'is_bilateral': 1 if re.search(r'bilateral|ambas', text_lower) else 0,\n",
    "        'has_axila': 1 if re.search(r'axil|linfono', text_lower) else 0,\n",
    "        \n",
    "        # Densidade\n",
    "        'has_denso': 1 if re.search(r'dens[oa]|heterog[êe]ne', text_lower) else 0,\n",
    "        'has_adiposo': 1 if re.search(r'adipos|gordur|lipom', text_lower) else 0,\n",
    "        \n",
    "        # Tamanho do texto\n",
    "        'text_length': len(text),\n",
    "        'word_count': len(text.split()),\n",
    "    }\n",
    "    \n",
    "    # Adicionar features de BI-RADS\n",
    "    birads = extract_birads_features(text)\n",
    "    features.update(birads)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words mantendo termos médicos.\"\"\"\n",
    "    words = text.split()\n",
    "    return ' '.join([w for w in words if w.lower() not in STOP_WORDS])\n",
    "\n",
    "def full_preprocess(text):\n",
    "    \"\"\"Pipeline completo de processamento.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 1. Remove boilerplate (cabeçalhos, datas)\n",
    "    text = remove_boilerplate(text)\n",
    "    \n",
    "    # 2. Expande abreviações\n",
    "    text = expand_abbreviations(text)\n",
    "    \n",
    "    # 3. Normaliza números\n",
    "    text = normalize_numbers(text)\n",
    "    \n",
    "    # 4. Remove caracteres especiais (manter hífen e ponto)\n",
    "    text = re.sub(r'[^a-záàâãéêíóôõúüç\\s\\-\\.]', ' ', text)\n",
    "    \n",
    "    # 5. Normaliza espaços\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 6. Remove stop words\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Testar\n",
    "sample = train.iloc[0]['text']\n",
    "print(f\"Original ({len(sample)} chars): {sample[:80]}...\")\n",
    "processed = full_preprocess(sample)\n",
    "print(f\"Processado ({len(processed)} chars): {processed[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf877458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== APLICAR PROCESSAMENTO ====\n",
    "print(\"\\n[4/8] Aplicando processamento...\")\n",
    "\n",
    "# Processar textos\n",
    "X_train_text = train['text'].apply(full_preprocess)\n",
    "X_test_text = test['text'].apply(full_preprocess)\n",
    "y_train = train['label'].values\n",
    "\n",
    "# Extrair features estruturadas\n",
    "print(\"    Extraindo features médicas...\")\n",
    "train_features = train['text'].apply(extract_medical_features).apply(pd.Series)\n",
    "test_features = test['text'].apply(extract_medical_features).apply(pd.Series)\n",
    "\n",
    "print(f\"\\n    Textos processados: {len(X_train_text)} train, {len(X_test_text)} test\")\n",
    "print(f\"    Features extraídas: {train_features.shape[1]} colunas\")\n",
    "print(f\"    Features: {list(train_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TF-IDF VETORIZAÇÃO ====\n",
    "print(\"\\n[5/8] Vetorização TF-IDF...\")\n",
    "\n",
    "# TF-IDF com vocabulário otimizado\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\b[a-záàâãéêíóôõúüç]{2,}\\b',  # Mínimo 2 caracteres\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "print(f\"    Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"    TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Combinar TF-IDF + features estruturadas\n",
    "print(\"\\n    Combinando TF-IDF + features estruturadas...\")\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "X_train_combined = hstack([X_train_tfidf, csr_matrix(train_features_scaled)])\n",
    "X_test_combined = hstack([X_test_tfidf, csr_matrix(test_features_scaled)])\n",
    "\n",
    "print(f\"    Shape final: {X_train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CLASS WEIGHTS CUSTOMIZADOS ====\n",
    "print(\"\\n[6/8] Calculando class weights...\")\n",
    "\n",
    "# Class weights baseados na frequência inversa\n",
    "class_counts = train['label'].value_counts().sort_index()\n",
    "total = len(train)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "# Fórmula: balanced weights com boost para classes críticas (5, 6)\n",
    "weights = {}\n",
    "for label, count in class_counts.items():\n",
    "    base_weight = total / (n_classes * count)\n",
    "    # Boost extra para classes críticas (5, 6)\n",
    "    if label >= 5:\n",
    "        base_weight *= 1.5  # 50% extra\n",
    "    weights[label] = base_weight\n",
    "\n",
    "print(\"    Class weights calculados:\")\n",
    "for label, weight in sorted(weights.items()):\n",
    "    print(f\"        Classe {label}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAR MODELOS TOP 3 ====\n",
    "print(\"\\n[7/8] Treinando Top 3 modelos...\")\n",
    "\n",
    "# 1. LinearSVC (melhor single model: 0.77885)\n",
    "print(\"\\n--- LinearSVC ---\")\n",
    "svc = LinearSVC(\n",
    "    C=0.5,\n",
    "    loss='squared_hinge',\n",
    "    max_iter=3000,\n",
    "    class_weight=weights,\n",
    "    dual='auto',\n",
    "    random_state=SEED\n",
    ")\n",
    "svc_calibrated = CalibratedClassifierCV(svc, cv=3)\n",
    "svc_calibrated.fit(X_train_combined, y_train)\n",
    "print(\"    Treinado!\")\n",
    "\n",
    "# 2. SGDClassifier (segundo melhor: 0.75019)\n",
    "print(\"\\n--- SGDClassifier ---\")\n",
    "sgd = SGDClassifier(\n",
    "    loss='modified_huber',\n",
    "    alpha=1e-4,\n",
    "    penalty='l2',\n",
    "    max_iter=1000,\n",
    "    class_weight=weights,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "sgd.fit(X_train_combined, y_train)\n",
    "print(\"    Treinado!\")\n",
    "\n",
    "# 3. LogisticRegression (terceiro: 0.72935)\n",
    "print(\"\\n--- LogisticRegression ---\")\n",
    "lr = LogisticRegression(\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    class_weight=weights,\n",
    "    multi_class='multinomial',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr.fit(X_train_combined, y_train)\n",
    "print(\"    Treinado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CROSS-VALIDATION ====\n",
    "print(\"\\n--- Cross-Validation (5-fold) ---\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Re-treinar para CV\n",
    "svc_cv = CalibratedClassifierCV(\n",
    "    LinearSVC(C=0.5, class_weight=weights, dual='auto', random_state=SEED), cv=3\n",
    ")\n",
    "sgd_cv = SGDClassifier(\n",
    "    loss='modified_huber', alpha=1e-4, class_weight=weights, random_state=SEED\n",
    ")\n",
    "lr_cv = LogisticRegression(\n",
    "    C=1.0, class_weight=weights, multi_class='multinomial', random_state=SEED\n",
    ")\n",
    "\n",
    "svc_scores = cross_val_score(svc_cv, X_train_combined, y_train, cv=cv, scoring='f1_macro')\n",
    "sgd_scores = cross_val_score(sgd_cv, X_train_combined, y_train, cv=cv, scoring='f1_macro')\n",
    "lr_scores = cross_val_score(lr_cv, X_train_combined, y_train, cv=cv, scoring='f1_macro')\n",
    "\n",
    "print(f\"\\n    LinearSVC:     {svc_scores.mean():.5f} ± {svc_scores.std():.5f}\")\n",
    "print(f\"    SGDClassifier: {sgd_scores.mean():.5f} ± {sgd_scores.std():.5f}\")\n",
    "print(f\"    LogisticReg:   {lr_scores.mean():.5f} ± {lr_scores.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32054143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ENSEMBLE PREDICTIONS ====\n",
    "print(\"\\n[8/8] Gerando predições ensemble...\")\n",
    "\n",
    "# Predições individuais\n",
    "pred_svc = svc_calibrated.predict(X_test_combined)\n",
    "pred_sgd = sgd.predict(X_test_combined)\n",
    "pred_lr = lr.predict(X_test_combined)\n",
    "\n",
    "# Probabilidades\n",
    "proba_svc = svc_calibrated.predict_proba(X_test_combined)\n",
    "proba_sgd = sgd.predict_proba(X_test_combined)\n",
    "proba_lr = lr.predict_proba(X_test_combined)\n",
    "\n",
    "# Weighted Voting (pesos baseados no score público)\n",
    "weights_voting = np.array([0.77885, 0.75019, 0.72935])\n",
    "weights_voting = weights_voting / weights_voting.sum()\n",
    "\n",
    "proba_weighted = (\n",
    "    proba_svc * weights_voting[0] + \n",
    "    proba_sgd * weights_voting[1] + \n",
    "    proba_lr * weights_voting[2]\n",
    ")\n",
    "pred_weighted = np.argmax(proba_weighted, axis=1)\n",
    "\n",
    "print(f\"\\n    LinearSVC dist:  {dict(Counter(pred_svc))}\")\n",
    "print(f\"    Weighted dist:   {dict(Counter(pred_weighted))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d188e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISSÕES ====\n",
    "print(\"\\n--- Gerando submissões ---\")\n",
    "\n",
    "# 1. LinearSVC tratado\n",
    "submission_svc = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_svc\n",
    "})\n",
    "submission_svc.to_csv('/kaggle/working/submission_linearsvc_treated.csv', index=False)\n",
    "print(\"    submission_linearsvc_treated.csv\")\n",
    "\n",
    "# 2. Weighted Ensemble tratado\n",
    "submission_weighted = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_weighted\n",
    "})\n",
    "submission_weighted.to_csv('/kaggle/working/submission_weighted_treated.csv', index=False)\n",
    "print(\"    submission_weighted_treated.csv\")\n",
    "\n",
    "# PRINCIPAL: LinearSVC (best single model)\n",
    "submission_svc.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUÍDO - submission.csv = LinearSVC Treated\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nProcessamento aplicado:\")\n",
    "print(\"  - Stop words customizadas (preservando termos médicos)\")\n",
    "print(\"  - Expansão de abreviações médicas\")\n",
    "print(\"  - Remoção de boilerplate (cabeçalhos, datas)\")\n",
    "print(\"  - Normalização de números\")\n",
    "print(\"  - Extração de features estruturadas (17 features)\")\n",
    "print(\"  - Class weights com boost para classes 5/6\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
