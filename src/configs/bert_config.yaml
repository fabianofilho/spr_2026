# BERT Model Configuration
# Inherits from base_config.yaml

model:
  name: "neuralmind/bert-base-portuguese-cased"  # BERTimbau - Portuguese BERT
  max_length: 512
  hidden_dropout_prob: 0.1
  pooling: "cls"
  
# Alternative models to try:
# - "bert-base-multilingual-cased"
# - "neuralmind/bert-large-portuguese-cased"
# - "xlm-roberta-base"
# - "xlm-roberta-large"

training:
  batch_size: 16
  learning_rate: 2e-5
  num_epochs: 5
