# T5 Model Configuration
# Sequence-to-sequence approach for classification

model:
  name: "unicamp-dl/ptt5-base-portuguese-vocab"  # Portuguese T5
  max_length: 512
  max_target_length: 8  # Output: "0", "1", ..., "6"
  pooling: null  # Not used for seq2seq
  
# Alternative models to try:
# - "google/flan-t5-base"
# - "google/flan-t5-large"
# - "google/mt5-base"
# - "google/mt5-large"

training:
  batch_size: 8  # T5 requires more memory
  learning_rate: 3e-5
  num_epochs: 5
  gradient_accumulation_steps: 4
  
# T5 specific settings
t5:
  input_prefix: "classificar mamografia: "
  label_format: "class"  # Output format: just the number
