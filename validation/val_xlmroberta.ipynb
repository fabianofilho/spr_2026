{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e874ac5",
   "metadata": {},
   "source": [
    "# ValidaÃ§Ã£o: XLM-RoBERTa\n",
    "\n",
    "**XLM-RoBERTa Large - Multilingual**\n",
    "\n",
    "## ðŸ“Š HistÃ³rico\n",
    "- Score 0.68767 com Mean Pooling (3Âº melhor transformer)\n",
    "- CLS pooling nÃ£o testado\n",
    "\n",
    "## ðŸŽ¯ Objetivo\n",
    "Testar diferentes estratÃ©gias de pooling e Focal Loss.\n",
    "\n",
    "## ðŸ“Š ConfiguraÃ§Ãµes Testadas\n",
    "1. **Pooling:** CLS, Mean, Max, Weighted Mean\n",
    "2. **Loss:** CrossEntropy vs Focal Loss\n",
    "3. **Model Size:** base vs large\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "    def find_model_path():\n",
    "        base = '/kaggle/input'\n",
    "        def search_dir(directory, depth=0, max_depth=10):\n",
    "            if depth > max_depth: return None\n",
    "            try:\n",
    "                for item in os.listdir(directory):\n",
    "                    path = os.path.join(directory, item)\n",
    "                    if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                    if result: return result\n",
    "            except: pass\n",
    "            return None\n",
    "        return search_dir(base)\n",
    "    MODEL_PATH = find_model_path()\n",
    "else:\n",
    "    DATA_DIR = '../data'\n",
    "    MODEL_PATH = 'xlm-roberta-large'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].tolist(),\n",
    "    train_df['target'].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=train_df['target'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_texts)}, Val: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODELO COM DIFERENTES POOLINGS =====\n",
    "class XLMRoBERTaWithPooling(nn.Module):\n",
    "    def __init__(self, model_path, num_labels=7, pooling='cls'):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "        self.encoder = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling == 'cls':\n",
    "            pooled = hidden[:, 0, :]\n",
    "        elif self.pooling == 'mean':\n",
    "            mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float()\n",
    "            pooled = (hidden * mask).sum(1) / mask.sum(1)\n",
    "        elif self.pooling == 'max':\n",
    "            mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float()\n",
    "            hidden[mask == 0] = -1e9\n",
    "            pooled = hidden.max(1)[0]\n",
    "        else:\n",
    "            pooled = hidden[:, 0, :]\n",
    "        \n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return type('Output', (), {'loss': loss, 'logits': logits})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FOCAL LOSS =====\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde65dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATASET =====\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TREINO MANUAL =====\n",
    "def train_model(pooling='cls', use_focal=False, epochs=3, lr=2e-5, batch_size=8):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Pooling: {pooling}, Focal: {use_focal}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "    \n",
    "    train_ds = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    val_ds = TextDataset(val_texts, val_labels, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size*2)\n",
    "    \n",
    "    model = XLMRoBERTaWithPooling(MODEL_PATH, num_labels=7, pooling=pooling).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    if use_focal:\n",
    "        criterion = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # ValidaÃ§Ã£o\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop('labels')\n",
    "                outputs = model(**batch)\n",
    "                preds = outputs.logits.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, F1={f1:.5f}')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "    \n",
    "    print(f'\\nMelhor F1: {best_f1:.5f}')\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return best_f1, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b976509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENTOS DE POOLING =====\n",
    "results = {}\n",
    "\n",
    "# CLS Pooling\n",
    "f1_cls, _ = train_model(pooling='cls', use_focal=False)\n",
    "results['CLS'] = f1_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling\n",
    "f1_mean, _ = train_model(pooling='mean', use_focal=False)\n",
    "results['Mean'] = f1_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling\n",
    "f1_max, _ = train_model(pooling='max', use_focal=False)\n",
    "results['Max'] = f1_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean + Focal Loss\n",
    "f1_mean_focal, _ = train_model(pooling='mean', use_focal=True)\n",
    "results['Mean+Focal'] = f1_mean_focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RESUMO =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š RESUMO - XLM-RoBERTa Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, f1 in sorted(results.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{name:<20} {f1:.5f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Score anterior (Mean): 0.68767\")\n",
    "print(f\"ðŸ“ ReferÃªncia (BERTimbau v4): 0.82073\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
