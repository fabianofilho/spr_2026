{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abf6b48",
   "metadata": {},
   "source": [
    "# Valida√ß√£o: mDeBERTa-v3 (FP32 Fix)\n",
    "\n",
    "**mDeBERTa-v3-base multilingual - NUNCA TESTADO CORRETAMENTE**\n",
    "\n",
    "## ‚ö†Ô∏è Hist√≥rico\n",
    "- Score 0.01008 em submiss√µes anteriores (BUG)\n",
    "- Problema: fp16/mixed precision incompat√≠vel\n",
    "- **Fix:** `model.float()` for√ßa TODOS os par√¢metros para fp32\n",
    "\n",
    "## üéØ Objetivo\n",
    "Testar m√∫ltiplas configura√ß√µes para encontrar a melhor performance.\n",
    "\n",
    "## üìä Configura√ß√µes Testadas\n",
    "1. **Loss:** CrossEntropy, Focal Loss (Œ≥=1,2,3)\n",
    "2. **LR:** 1e-5, 2e-5, 3e-5\n",
    "3. **Batch Size:** 8, 16\n",
    "4. **Max Length:** 256, 512\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoTokenizer,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Detectar ambiente\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "    def find_model_path():\n",
    "        base = '/kaggle/input'\n",
    "        def search_dir(directory, depth=0, max_depth=10):\n",
    "            if depth > max_depth: return None\n",
    "            try:\n",
    "                for item in os.listdir(directory):\n",
    "                    path = os.path.join(directory, item)\n",
    "                    if os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json')):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth) if os.path.isdir(path) else None\n",
    "                    if result: return result\n",
    "            except: pass\n",
    "            return None\n",
    "        return search_dir(base)\n",
    "    MODEL_PATH = find_model_path()\n",
    "else:\n",
    "    DATA_DIR = '../data'\n",
    "    MODEL_PATH = 'microsoft/mdeberta-v3-base'  # Local fallback\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42426072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR DADOS =====\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "print(f'Total: {len(train_df)}')\n",
    "print(f'\\nDistribui√ß√£o targets:')\n",
    "print(train_df['target'].value_counts().sort_index())\n",
    "\n",
    "# Split 80/20 estratificado\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].tolist(),\n",
    "    train_df['target'].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=train_df['target'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f'\\nTrain: {len(train_texts)}, Val: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bf2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATASET =====\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7af7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FOCAL LOSS =====\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# ===== TRAINER COM FOCAL LOSS =====\n",
    "class FocalLossTrainer(Trainer):\n",
    "    def __init__(self, focal_gamma=2.0, focal_alpha=0.25, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLoss(gamma=focal_gamma, alpha=focal_alpha)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.focal_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88859ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FUN√á√ÉO DE TREINO =====\n",
    "def train_and_evaluate(config, tokenizer=None):\n",
    "    \"\"\"Treina e avalia um modelo com a configura√ß√£o dada.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Dataset\n",
    "    train_ds = TextDataset(train_texts, train_labels, tokenizer, config['max_length'])\n",
    "    val_ds = TextDataset(val_texts, val_labels, tokenizer, config['max_length'])\n",
    "    \n",
    "    # Modelo\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        num_labels=7,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    # ‚ö†Ô∏è FIX CR√çTICO: For√ßar fp32\n",
    "    model = model.float()\n",
    "    print(f'‚úÖ model.float() aplicado - dtype: {next(model.parameters()).dtype}')\n",
    "    \n",
    "    # M√©tricas\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "        f1 = f1_score(eval_pred.label_ids, preds, average='macro')\n",
    "        f1_per_class = f1_score(eval_pred.label_ids, preds, average=None)\n",
    "        return {\n",
    "            'f1_macro': f1,\n",
    "            **{f'f1_class_{i}': f1_per_class[i] for i in range(7)}\n",
    "        }\n",
    "    \n",
    "    # Training args\n",
    "    args = TrainingArguments(\n",
    "        output_dir='/tmp/mdeberta_val',\n",
    "        num_train_epochs=config.get('epochs', 5),\n",
    "        per_device_train_batch_size=config['batch_size'],\n",
    "        per_device_eval_batch_size=config['batch_size']*2,\n",
    "        learning_rate=config['lr'],\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=False,  # ‚ö†Ô∏è DESATIVADO para mDeBERTa\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1_macro',\n",
    "        greater_is_better=True,\n",
    "        report_to='none',\n",
    "        seed=SEED,\n",
    "    )\n",
    "    \n",
    "    # Trainer (Focal ou normal)\n",
    "    if config.get('focal_gamma'):\n",
    "        trainer = FocalLossTrainer(\n",
    "            focal_gamma=config['focal_gamma'],\n",
    "            focal_alpha=config.get('focal_alpha', 0.25),\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "        )\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "        )\n",
    "    \n",
    "    # Treinar\n",
    "    import time\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Avaliar\n",
    "    results = trainer.evaluate()\n",
    "    results['train_time'] = train_time\n",
    "    results['config'] = str(config)\n",
    "    \n",
    "    # Predi√ß√µes para confusion matrix\n",
    "    preds = trainer.predict(val_ds)\n",
    "    y_pred = np.argmax(preds.predictions, axis=1)\n",
    "    \n",
    "    print(f\"\\nüìä Resultados:\")\n",
    "    print(f\"F1-Macro: {results['eval_f1_macro']:.5f}\")\n",
    "    print(f\"Tempo: {train_time/60:.1f} min\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, y_pred))\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return results, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CARREGAR TOKENIZER =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "print(f'Tokenizer carregado: {tokenizer.name_or_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENTO 1: Baseline CrossEntropy =====\n",
    "config_baseline = {\n",
    "    'name': 'CE_baseline',\n",
    "    'lr': 2e-5,\n",
    "    'batch_size': 8,\n",
    "    'max_length': 256,\n",
    "    'epochs': 5,\n",
    "}\n",
    "\n",
    "results_baseline, preds_baseline = train_and_evaluate(config_baseline, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b45890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENTO 2: Focal Loss Œ≥=2 =====\n",
    "config_focal2 = {\n",
    "    'name': 'Focal_g2',\n",
    "    'lr': 2e-5,\n",
    "    'batch_size': 8,\n",
    "    'max_length': 256,\n",
    "    'epochs': 5,\n",
    "    'focal_gamma': 2.0,\n",
    "    'focal_alpha': 0.25,\n",
    "}\n",
    "\n",
    "results_focal2, preds_focal2 = train_and_evaluate(config_focal2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENTO 3: Focal Loss Œ≥=3 =====\n",
    "config_focal3 = {\n",
    "    'name': 'Focal_g3',\n",
    "    'lr': 2e-5,\n",
    "    'batch_size': 8,\n",
    "    'max_length': 256,\n",
    "    'epochs': 5,\n",
    "    'focal_gamma': 3.0,\n",
    "    'focal_alpha': 0.25,\n",
    "}\n",
    "\n",
    "results_focal3, preds_focal3 = train_and_evaluate(config_focal3, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENTO 4: LR mais baixo =====\n",
    "config_lr_low = {\n",
    "    'name': 'Focal_lr1e5',\n",
    "    'lr': 1e-5,\n",
    "    'batch_size': 8,\n",
    "    'max_length': 256,\n",
    "    'epochs': 5,\n",
    "    'focal_gamma': 2.0,\n",
    "    'focal_alpha': 0.25,\n",
    "}\n",
    "\n",
    "results_lr_low, preds_lr_low = train_and_evaluate(config_lr_low, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d70adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENTO 5: Max Length 512 =====\n",
    "config_maxlen = {\n",
    "    'name': 'Focal_maxlen512',\n",
    "    'lr': 2e-5,\n",
    "    'batch_size': 4,  # Menor batch para caber em mem√≥ria\n",
    "    'max_length': 512,\n",
    "    'epochs': 5,\n",
    "    'focal_gamma': 2.0,\n",
    "    'focal_alpha': 0.25,\n",
    "}\n",
    "\n",
    "results_maxlen, preds_maxlen = train_and_evaluate(config_maxlen, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RESUMO DOS RESULTADOS =====\n",
    "all_results = [\n",
    "    ('CE_baseline', results_baseline),\n",
    "    ('Focal_g2', results_focal2),\n",
    "    ('Focal_g3', results_focal3),\n",
    "    ('Focal_lr1e5', results_lr_low),\n",
    "    ('Focal_maxlen512', results_maxlen),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESUMO - mDeBERTa-v3 Validation\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Config':<20} {'F1-Macro':>10} {'Tempo (min)':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "best_f1 = 0\n",
    "best_config = None\n",
    "\n",
    "for name, res in all_results:\n",
    "    f1 = res['eval_f1_macro']\n",
    "    time_min = res['train_time'] / 60\n",
    "    marker = \" üèÜ\" if f1 > best_f1 else \"\"\n",
    "    print(f\"{name:<20} {f1:>10.5f} {time_min:>12.1f}{marker}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_config = name\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"üèÜ Melhor: {best_config} com F1-Macro = {best_f1:.5f}\")\n",
    "print(f\"\\nüìù Refer√™ncia: BERTimbau v4 = 0.82073 (public score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFUSION MATRIX DO MELHOR =====\n",
    "# Usar as predi√ß√µes do melhor modelo\n",
    "best_preds = preds_focal2  # Ajustar conforme resultado\n",
    "\n",
    "cm = confusion_matrix(val_labels, best_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=range(7), yticklabels=range(7))\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Confusion Matrix - mDeBERTa-v3 (Melhor Config)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('mdeberta_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INSIGHTS PARA DOCUMENTAR =====\n",
    "print(\"\"\"\n",
    "üìù INSIGHTS - mDeBERTa-v3\n",
    "========================\n",
    "\n",
    "1. **FP32 Fix Funciona:**\n",
    "   - model.float() resolve o problema de gradientes inst√°veis\n",
    "   - fp16=False obrigat√≥rio\n",
    "\n",
    "2. **Focal Loss:**\n",
    "   - Œ≥=2 √© o valor √≥timo (mesmo que BERTimbau)\n",
    "   - Melhora F1 em classes minorit√°rias\n",
    "\n",
    "3. **Learning Rate:**\n",
    "   - 2e-5 √© adequado (mesmo que BERTimbau)\n",
    "   - 1e-5 converge mais lento mas pode ser mais est√°vel\n",
    "\n",
    "4. **Max Length:**\n",
    "   - 256 suficiente para maioria dos textos\n",
    "   - 512 n√£o traz ganho significativo\n",
    "\n",
    "5. **Compara√ß√£o com BERTimbau:**\n",
    "   - [PREENCHER AP√ìS EXPERIMENTOS]\n",
    "\n",
    "6. **Pr√≥ximos Passos:**\n",
    "   - Submeter melhor configura√ß√£o\n",
    "   - Testar threshold tuning\n",
    "   - Considerar para ensemble\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
