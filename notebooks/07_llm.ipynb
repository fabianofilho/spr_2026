{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3345e268",
   "metadata": {},
   "source": [
    "# 07 - LLMs (Zero-Shot Classification)\n",
    "\n",
    "**Notebook de referência para classificação zero-shot com LLMs**\n",
    "\n",
    "Este notebook demonstra como usar modelos de linguagem grandes para classificação sem fine-tuning:\n",
    "- Qwen3 (1.7B, 4B)\n",
    "- Gemma 3 (4B)\n",
    "- Llama 3.2 (3B)\n",
    "\n",
    "---\n",
    "\n",
    "## Configuração Kaggle\n",
    "\n",
    "| Modelo | Kaggle Input | VRAM |\n",
    "|--------|--------------|------|\n",
    "| Qwen3 1.7B | `QwenLM/Qwen3` → `1.7B` | ~4GB |\n",
    "| Qwen3 4B | `QwenLM/Qwen3` → `4B` | ~9GB |\n",
    "| Gemma 3 4B | `google/gemma-3` → `4b` | ~9GB |\n",
    "| Llama 3.2 3B | `meta-llama/Llama-3.2` → `3B` | ~7GB |\n",
    "\n",
    "**Settings:**\n",
    "- Internet → **OFF**\n",
    "- Accelerator → **GPU T4 x2** ou **P100**\n",
    "- Add Data → Models → escolher modelo acima\n",
    "\n",
    "**Atenção:** LLMs funcionam offline mas precisam do modelo como Input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85beeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SETUP E IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPR 2026 - LLM Zero-Shot Classification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# Escolher modelo (descomentar o desejado)\n",
    "# =========================================\n",
    "MODEL_NAME = 'qwen3-1.7b'  # <-- ALTERAR AQUI\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'qwen3-1.7b': {\n",
    "        'paths': [\n",
    "            '/kaggle/input/qwen3/transformers/1.7b/1',\n",
    "            '/kaggle/input/qwen3/transformers/1.7b',\n",
    "        ],\n",
    "        'description': 'Qwen3 1.7B - compacto e eficiente'\n",
    "    },\n",
    "    'qwen3-4b': {\n",
    "        'paths': [\n",
    "            '/kaggle/input/qwen3/transformers/4b/1',\n",
    "            '/kaggle/input/qwen3/transformers/4b',\n",
    "        ],\n",
    "        'description': 'Qwen3 4B - mais capacidade'\n",
    "    },\n",
    "    'gemma3-4b': {\n",
    "        'paths': [\n",
    "            '/kaggle/input/gemma-3/transformers/4b/1',\n",
    "            '/kaggle/input/gemma-3/transformers/4b',\n",
    "        ],\n",
    "        'description': 'Gemma 3 4B - Google'\n",
    "    },\n",
    "    'llama3-3b': {\n",
    "        'paths': [\n",
    "            '/kaggle/input/llama-3.2/transformers/3b/1',\n",
    "            '/kaggle/input/llama-3.2/transformers/3b',\n",
    "        ],\n",
    "        'description': 'Llama 3.2 3B - Meta'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Encontrar path do modelo\n",
    "config = MODEL_CONFIGS[MODEL_NAME]\n",
    "MODEL_PATH = None\n",
    "\n",
    "for path in config['paths']:\n",
    "    if os.path.exists(path):\n",
    "        MODEL_PATH = path\n",
    "        break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    print(f\"\\n⚠️ Modelo '{MODEL_NAME}' não encontrado!\")\n",
    "    print(\"Datasets disponíveis:\")\n",
    "    for item in os.listdir('/kaggle/input'):\n",
    "        print(f\"  - {item}\")\n",
    "    raise FileNotFoundError(\"Adicione o modelo ao notebook!\")\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "device = 'cuda' if USE_GPU else 'cpu'\n",
    "\n",
    "print(f\"✓ Modelo: {MODEL_NAME}\")\n",
    "print(f\"✓ Path: {MODEL_PATH}\")\n",
    "print(f\"✓ {config['description']}\")\n",
    "print(f\"✓ GPU: {USE_GPU}\")\n",
    "print(f\"✓ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3acd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR DADOS\n",
    "# =============================================================================\n",
    "print(\"\\n[1/4] Carregando dados...\")\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "\n",
    "print(\"\\nDistribuição das classes no treino:\")\n",
    "print(train['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a389de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR MODELO\n",
    "# =============================================================================\n",
    "print(\"\\n[2/4] Carregando modelo...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if USE_GPU else torch.float32,\n",
    "    device_map='auto' if USE_GPU else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if not USE_GPU:\n",
    "    model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(f\"✓ Modelo carregado!\")\n",
    "print(f\"✓ Parâmetros: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROMPT TEMPLATE\n",
    "# =============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Você é um especialista em radiologia mamária. \n",
    "Sua tarefa é classificar laudos de mamografia na escala BI-RADS (0-6).\n",
    "\n",
    "Categorias BI-RADS:\n",
    "- 0: Avaliação incompleta\n",
    "- 1: Negativo (normal)\n",
    "- 2: Achado benigno\n",
    "- 3: Provavelmente benigno\n",
    "- 4: Suspeita de malignidade\n",
    "- 5: Altamente sugestivo de malignidade\n",
    "- 6: Malignidade conhecida\n",
    "\n",
    "Responda APENAS com um número de 0 a 6.\"\"\"\n",
    "\n",
    "def create_prompt(report):\n",
    "    return f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "<|user|>\n",
    "Classifique o seguinte laudo de mamografia:\n",
    "\n",
    "{report}\n",
    "\n",
    "Categoria BI-RADS (0-6):\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Testar prompt\n",
    "sample_report = train['report'].iloc[0]\n",
    "print(\"Exemplo de prompt:\")\n",
    "print(create_prompt(sample_report[:200] + \"...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÃO DE CLASSIFICAÇÃO\n",
    "# =============================================================================\n",
    "import re\n",
    "\n",
    "def classify_report(report, model, tokenizer, max_new_tokens=10):\n",
    "    \"\"\"Classifica um laudo usando o LLM.\"\"\"\n",
    "    prompt = create_prompt(report)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Extrair número da resposta\n",
    "    numbers = re.findall(r'[0-6]', response)\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    \n",
    "    # Fallback: classe mais comum (2)\n",
    "    return 2\n",
    "\n",
    "# Testar\n",
    "sample_pred = classify_report(train['report'].iloc[0], model, tokenizer)\n",
    "print(f\"Predição de teste: {sample_pred}\")\n",
    "print(f\"Label real: {train['target'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INFERÊNCIA NO TESTE\n",
    "# =============================================================================\n",
    "print(\"\\n[3/4] Classificando...\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(test.iterrows(), total=len(test), desc=\"Classificando\"):\n",
    "    pred = classify_report(row['report'], model, tokenizer)\n",
    "    predictions.append(pred)\n",
    "\n",
    "print(f\"\\n✓ {len(predictions)} predições geradas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUBMISSÃO\n",
    "# =============================================================================\n",
    "print(\"\\n[4/4] Criando submissão...\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✅ CONCLUÍDO - submission.csv criado!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDistribuição das predições:\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31db657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Técnicas Avançadas\n",
    "\n",
    "### Few-Shot Learning\n",
    "\n",
    "Adicionar exemplos no prompt para melhorar a classificação:\n",
    "\n",
    "```python\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "Exemplo 1:\n",
    "Laudo: \"Mamas simétricas, sem nódulos ou calcificações suspeitas.\"\n",
    "Classificação: 1\n",
    "\n",
    "Exemplo 2:\n",
    "Laudo: \"Nódulo sólido de contornos irregulares, medindo 2cm.\"\n",
    "Classificação: 4\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Chain-of-Thought\n",
    "\n",
    "Pedir ao modelo que explique o raciocínio antes de classificar:\n",
    "\n",
    "```python\n",
    "PROMPT = \"\"\"\n",
    "Analise o laudo passo a passo:\n",
    "1. Identifique os achados principais\n",
    "2. Avalie a suspeição de malignidade\n",
    "3. Classifique na escala BI-RADS (0-6)\n",
    "\n",
    "Laudo: {report}\n",
    "\n",
    "Análise:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "Para acelerar a inferência, processar múltiplos laudos por vez (se houver VRAM suficiente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bf894",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validação no Treino (Opcional)\n",
    "\n",
    "Antes de submeter, validar a performance no conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDAÇÃO (OPCIONAL)\n",
    "# =============================================================================\n",
    "# Descomentar para validar no treino\n",
    "\n",
    "'''\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Usar uma amostra para validação rápida\n",
    "n_samples = 100\n",
    "sample = train.sample(n=n_samples, random_state=SEED)\n",
    "\n",
    "val_preds = []\n",
    "for idx, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Validando\"):\n",
    "    pred = classify_report(row['report'], model, tokenizer)\n",
    "    val_preds.append(pred)\n",
    "\n",
    "f1 = f1_score(sample['target'], val_preds, average='macro')\n",
    "print(f\"\\nF1-Macro (amostra de {n_samples}): {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(sample['target'], val_preds))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
