{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4535be78",
   "metadata": {},
   "source": [
    "# 04 - Transformers (BERT-based Models)\n",
    "\n",
    "**Notebook de referência para modelos Transformer encoder-only**\n",
    "\n",
    "Este notebook demonstra o pipeline completo para fine-tuning de modelos BERT-like:\n",
    "- BERTimbau (PT-BR)\n",
    "- mDeBERTa-v3 (multilingual)\n",
    "- DistilBERT (compacto)\n",
    "- XLM-RoBERTa (multilingual)\n",
    "- ModernBERT (Flash Attention + RoPE)\n",
    "- BioBERTpt (domínio médico)\n",
    "\n",
    "---\n",
    "\n",
    "## Configuração Kaggle\n",
    "\n",
    "| Modelo | Kaggle Input | Tamanho |\n",
    "|--------|--------------|----------|\n",
    "| BERTimbau base | `neuralmind/bert-base-portuguese-cased` | ~440MB |\n",
    "| BERTimbau large | `neuralmind/bert-large-portuguese-cased` | ~1.3GB |\n",
    "| mDeBERTa-v3 | `microsoft/mdeberta-v3-base` | ~560MB |\n",
    "| DistilBERT | `distilbert-base-multilingual-cased` | ~280MB |\n",
    "| XLM-RoBERTa | `xlm-roberta-large` | ~1.4GB |\n",
    "| ModernBERT | `answerdotai/ModernBERT-base` | ~450MB |\n",
    "| BioBERTpt | `pucpr/biobertpt-all` | ~440MB |\n",
    "\n",
    "**Settings:**\n",
    "- Internet → **OFF**\n",
    "- Accelerator → **GPU T4 x2**\n",
    "- Add Data → Models → escolher modelo acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7240493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SETUP E IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# Escolher modelo (descomentar o desejado)\n",
    "# =========================================\n",
    "MODEL_NAME = 'bertimbau-base'  # <-- ALTERAR AQUI\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'bertimbau-base': {\n",
    "        'path': '/kaggle/input/bert-base-portuguese-cased',\n",
    "        'description': 'BERTimbau base - melhor modelo PT-BR'\n",
    "    },\n",
    "    'bertimbau-large': {\n",
    "        'path': '/kaggle/input/bert-large-portuguese-cased',\n",
    "        'description': 'BERTimbau large - mais parâmetros'\n",
    "    },\n",
    "    'mdeberta': {\n",
    "        'path': '/kaggle/input/mdeberta-v3-base',\n",
    "        'description': 'mDeBERTa-v3 - disentangled attention'\n",
    "    },\n",
    "    'distilbert': {\n",
    "        'path': '/kaggle/input/distilbert-base-multilingual-cased',\n",
    "        'description': 'DistilBERT - 40% menor, 60% mais rápido'\n",
    "    },\n",
    "    'xlm-roberta': {\n",
    "        'path': '/kaggle/input/xlm-roberta-large',\n",
    "        'description': 'XLM-RoBERTa large - multilingual robusto'\n",
    "    },\n",
    "    'modernbert': {\n",
    "        'path': '/kaggle/input/modernbert-base',\n",
    "        'description': 'ModernBERT - Flash Attention + RoPE'\n",
    "    },\n",
    "    'biobertpt': {\n",
    "        'path': '/kaggle/input/biobertpt-all',\n",
    "        'description': 'BioBERTpt - domínio biomédico PT-BR'\n",
    "    },\n",
    "}\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_NAME]\n",
    "MODEL_PATH = config['path']\n",
    "\n",
    "# Verificar se modelo existe\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"⚠️ Modelo '{MODEL_NAME}' não encontrado em {MODEL_PATH}\")\n",
    "    print(\"Adicione o modelo como Input no Kaggle!\")\n",
    "else:\n",
    "    print(f\"✓ Modelo: {MODEL_NAME}\")\n",
    "    print(f\"✓ Path: {MODEL_PATH}\")\n",
    "    print(f\"✓ {config['description']}\")\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nDevice: {device}')\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR DADOS\n",
    "# =============================================================================\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test: {test_df.shape}')\n",
    "print(f'\\nDistribuição das classes:')\n",
    "print(train_df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e731e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPLIT TREINO/VALIDAÇÃO\n",
    "# =============================================================================\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].tolist(),\n",
    "    train_df['target'].tolist(),\n",
    "    test_size=0.1,\n",
    "    stratify=train_df['target'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_texts)}')\n",
    "print(f'Val: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET CLASS\n",
    "# =============================================================================\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "print(f'Tokenizer carregado: {tokenizer.__class__.__name__}')\n",
    "\n",
    "# Criar datasets\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "test_ds = TextDataset(test_df['report'].tolist(), None, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f'Train dataset: {len(train_ds)}')\n",
    "print(f'Val dataset: {len(val_ds)}')\n",
    "print(f'Test dataset: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f079d",
   "metadata": {},
   "source": [
    "## Opção 1: Trainer API (Recomendado)\n",
    "\n",
    "Usar HuggingFace Trainer para treinamento simplificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af827f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR MODELO\n",
    "# =============================================================================\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    num_labels=NUM_CLASSES,\n",
    "    trust_remote_code=True  # Necessário para ModernBERT\n",
    ")\n",
    "\n",
    "print(f'Modelo carregado: {model.__class__.__name__}')\n",
    "print(f'Parâmetros: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÉTRICAS\n",
    "# =============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "    f1 = f1_score(eval_pred.label_ids, preds, average='macro')\n",
    "    return {'f1_macro': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0db890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING ARGUMENTS\n",
    "# =============================================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/model',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    greater_is_better=True,\n",
    "    report_to='none',\n",
    "    seed=SEED,\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINER\n",
    "# =============================================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# Treinar\n",
    "print(f'\\nIniciando treinamento com {MODEL_NAME}...')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04726eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AVALIAÇÃO\n",
    "# =============================================================================\n",
    "val_results = trainer.evaluate()\n",
    "print(f\"\\nResultados Validação:\")\n",
    "print(f\"F1-Macro: {val_results['eval_f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREDIÇÃO\n",
    "# =============================================================================\n",
    "test_preds = trainer.predict(test_ds)\n",
    "predictions = np.argmax(test_preds.predictions, axis=1)\n",
    "\n",
    "print(f'Predições geradas: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUBMISSÃO\n",
    "# =============================================================================\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('✅ submission.csv criado!')\n",
    "print(f'\\nDistribuição das predições:')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db973c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Opção 2: Training Loop Manual\n",
    "\n",
    "Para mais controle sobre o treinamento (ex: Focal Loss, custom schedulers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING LOOP MANUAL (OPCIONAL)\n",
    "# =============================================================================\n",
    "# Descomentar para usar em vez do Trainer\n",
    "\n",
    "'''\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Optimizer e Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * total_steps), \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss (pode usar Focal Loss para desbalanceamento)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.to(device)\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            \n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(batch['labels'].numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    print(f'Epoch {epoch+1} - Loss: {train_loss/len(train_loader):.4f} - Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "print(f'\\nMelhor F1: {best_f1:.4f}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf4a49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Técnicas Avançadas\n",
    "\n",
    "### Focal Loss (para desbalanceamento)\n",
    "\n",
    "```python\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "```\n",
    "\n",
    "### LoRA (fine-tuning eficiente)\n",
    "Ver notebook `submit/transformers/submit_bertimbau_lora_offline.ipynb`\n",
    "\n",
    "### Mean Pooling (vs CLS token)\n",
    "\n",
    "```python\n",
    "def mean_pooling(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).float()\n",
    "    sum_hidden = torch.sum(last_hidden_state * mask, dim=1)\n",
    "    sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return sum_hidden / sum_mask\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
