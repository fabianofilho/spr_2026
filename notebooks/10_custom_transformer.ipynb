{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e60c66",
   "metadata": {},
   "source": [
    "# SPR 2026 - Custom Transformer Encoder\n",
    "\n",
    "Modelo customizado com Encoder Transformer + camada extra de Self-Attention.\n",
    "\n",
    "**Arquitetura:**\n",
    "- Embedding Layer\n",
    "- Positional Encoding\n",
    "- Transformer Encoder (N layers)\n",
    "- Extra Self-Attention Layer\n",
    "- Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Ambiente e Dados\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Verificar Colab PRIMEIRO (mais confi√°vel)\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input') and not IS_COLAB\n",
    "\n",
    "print(f\"Ambiente: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    DRIVE_BASE = '/content/drive/MyDrive/SPR_2026_outputs'\n",
    "    DATA_DIR = f'{DRIVE_BASE}/data'\n",
    "    OUTPUT_DIR = DRIVE_BASE\n",
    "    \n",
    "    if not os.path.exists(f'{DATA_DIR}/train.csv'):\n",
    "        print(\"‚ö†Ô∏è Dados n√£o encontrados no Drive!\")\n",
    "        print(\"Execute primeiro o notebook 00_download_data.ipynb\")\n",
    "        raise FileNotFoundError(f\"Arquivo n√£o encontrado: {DATA_DIR}/train.csv\")\n",
    "elif IS_KAGGLE:\n",
    "    DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "    OUTPUT_DIR = '/kaggle/working'\n",
    "else:\n",
    "    DATA_DIR = '../data'\n",
    "    OUTPUT_DIR = '../submissions'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias\n",
    "!pip install torch torchtext transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b09821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# Transformer Config\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "N_ENCODER_LAYERS = 4\n",
    "D_FF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a639c7",
   "metadata": {},
   "source": [
    "## 1. Carregar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(f\"\\nDistribui√ß√£o:\")\n",
    "print(train_df['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split treino/valida√ß√£o\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report'].values,\n",
    "    train_df['target'].values,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['target']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3f933",
   "metadata": {},
   "source": [
    "## 2. Tokeniza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar tokenizer do BERTimbau para tokeniza√ß√£o\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"PAD index: {PAD_IDX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aa5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb618adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_df['report'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0dd98",
   "metadata": {},
   "source": [
    "## 3. Modelo: Transformer Encoder + Extra Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d234e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding cl√°ssico do paper 'Attention is All You Need'\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48bc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraSelfAttention(nn.Module):\n",
    "    \"\"\"Camada extra de Self-Attention com residual connection\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_output, attn_weights = self.self_attn(\n",
    "            query=x, key=x, value=x,\n",
    "            key_padding_mask=mask\n",
    "        )\n",
    "        # Residual + LayerNorm\n",
    "        x = self.norm(x + self.dropout(attn_output))\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef592b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    \"\"\"Transformer Encoder com camada extra de Self-Attention para classifica√ß√£o\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=256,\n",
    "        n_heads=8,\n",
    "        n_encoder_layers=4,\n",
    "        d_ff=512,\n",
    "        num_classes=7,\n",
    "        max_len=256,\n",
    "        dropout=0.1,\n",
    "        pad_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=n_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # ====== EXTRA SELF-ATTENTION LAYER ======\n",
    "        self.extra_attention = ExtraSelfAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Criar m√°scara de padding para transformer\n",
    "        padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # ====== EXTRA SELF-ATTENTION ======\n",
    "        x, attn_weights = self.extra_attention(x, mask=padding_mask)\n",
    "        \n",
    "        # Pooling: usar [CLS] token (primeiro) ou mean pooling\n",
    "        # Aqui usamos mean pooling com m√°scara\n",
    "        if attention_mask is not None:\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "            x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfe0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar modelo\n",
    "model = TransformerEncoderClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_encoder_layers=N_ENCODER_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    max_len=MAX_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "# Contar par√¢metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(f\"\\nArquitetura:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c32f7c",
   "metadata": {},
   "source": [
    "## 4. Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights para lidar com imbalance\n",
    "class_counts = train_df['target'].value_counts().sort_index().values\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da448cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    f1 = f1_score(targets, preds, average='macro')\n",
    "    \n",
    "    return avg_loss, f1\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(targets, preds, average='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e52352",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1\n",
    "    })\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"‚úÖ Novo melhor modelo salvo! F1: {best_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Melhor F1 na valida√ß√£o: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4646f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar hist√≥rico\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history_df['epoch'], history_df['train_f1'], 'b-', label='Train F1')\n",
    "axes[1].plot(history_df['epoch'], history_df['val_f1'], 'r-', label='Val F1')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1-Macro')\n",
    "axes[1].set_title('F1 Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a6ec2",
   "metadata": {},
   "source": [
    "## 5. Gera√ß√£o de Submiss√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178167f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar melhor modelo\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Predi√ß√µes no test\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "print(f\"Total predi√ß√µes: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818020eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gera√ß√£o de Submiss√£o\n",
    "# ============================================================\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "# SEMPRE salvar submission.csv no diret√≥rio atual (exigido pelo Kaggle)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"‚úÖ submission.csv salvo no diret√≥rio atual\")\n",
    "\n",
    "# Tamb√©m salvar no OUTPUT_DIR para persist√™ncia (Colab/Local)\n",
    "if not IS_KAGGLE:\n",
    "    submission_path = os.path.join(OUTPUT_DIR, 'submission_transformer.csv')\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"‚úÖ C√≥pia salva em: {submission_path}\")\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o das predi√ß√µes:\")\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download no Colab (opcional)\n",
    "if IS_COLAB and os.path.exists('submission.csv'):\n",
    "    from google.colab import files\n",
    "    files.download('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1502b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar arquivo\n",
    "!ls -la submission.csv\n",
    "!head submission.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
