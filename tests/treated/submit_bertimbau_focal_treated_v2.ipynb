{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a17352",
   "metadata": {},
   "source": [
    "# SPR 2026 - BERTimbau + Focal Loss - Treated v2\n",
    "\n",
    "**Texto tratado: Stop words + Lematiza√ß√£o + Normaliza√ß√£o BI-RADS**\n",
    "\n",
    "Base: BERTimbau + Focal Loss (0.79696)\n",
    "Tratamento:\n",
    "- Remo√ß√£o de stop words\n",
    "- Lematiza√ß√£o b√°sica (sufixos comuns)\n",
    "- Normaliza√ß√£o de termos BI-RADS\n",
    "- Extra√ß√£o de features num√©ricas\n",
    "\n",
    "---\n",
    "## üì• MODELO - `models/download_bertimbau.ipynb`\n",
    "\n",
    "**Kaggle Models:** Add Input ‚Üí Models ‚Üí `bertimbau-ptbr-complete` (fabianofilho)\n",
    "\n",
    "---\n",
    "**CONFIGURA√á√ÉO KAGGLE:**\n",
    "1. Settings ‚Üí Internet ‚Üí **OFF**\n",
    "2. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SETUP E IMPORTS =====\n",
    "print(\"[1/9] Configurando ambiente...\")\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "NUM_CLASSES = 7\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "DATA_DIR = '/kaggle/input/spr-2026-mammography-report-classification'\n",
    "\n",
    "# ==== AUTO-DETECTAR PATH DO MODELO ====\n",
    "def find_model_path():\n",
    "    base = '/kaggle/input'\n",
    "    def has_config(path):\n",
    "        return os.path.isdir(path) and os.path.exists(os.path.join(path, 'config.json'))\n",
    "    def search_dir(directory, depth=0, max_depth=10):\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "        try:\n",
    "            for item in os.listdir(directory):\n",
    "                path = os.path.join(directory, item)\n",
    "                if os.path.isdir(path):\n",
    "                    if has_config(path):\n",
    "                        return path\n",
    "                    result = search_dir(path, depth + 1, max_depth)\n",
    "                    if result:\n",
    "                        return result\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    return search_dir(base)\n",
    "\n",
    "MODEL_PATH = find_model_path()\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"Adicione o modelo BERTimbau ao notebook!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model: {MODEL_PATH}')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PREPROCESSAMENTO DE TEXTO AVAN√áADO ====\n",
    "print(\"[2/9] Definindo preprocessamento avan√ßado...\")\n",
    "\n",
    "# Stop words b√°sicas\n",
    "STOP_WORDS = {\n",
    "    'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'para', 'com', 'n√£o',\n",
    "    'na', 'no', 'os', 'as', 'que', 'se', 'por', 'mais', 'como', 'mas', 'foi',\n",
    "    'ao', 'ele', 'ela', 'entre', 'sem', 'mesmo', 'ter', 'seus', 'sua', 'seu',\n",
    "    'laudo', 'exame', 'paciente', 'data', 'realizado', 'realizada', 'm√©dico',\n",
    "    'dr', 'dra', 'solicitante', 'hospital', 'cl√≠nica', 'mamografia', 'bilateral'\n",
    "}\n",
    "\n",
    "# Lematiza√ß√£o b√°sica (sufixos comuns em portugu√™s m√©dico)\n",
    "LEMMA_RULES = [\n",
    "    (r'√ß√µes$', '√ß√£o'),      # calcifica√ß√µes -> calcifica√ß√£o\n",
    "    (r'√µes$', '√£o'),        # n√≥dulos... (alguns casos)\n",
    "    (r'ais$', 'al'),        # bilaterais -> bilateral\n",
    "    (r'eis$', 'el'),        # prov√°veis -> prov√°vel\n",
    "    (r'is$', 'il'),         # dif√≠ceis -> dif√≠cil\n",
    "    (r'os$', 'o'),          # n√≥dulos -> n√≥dulo\n",
    "    (r'as$', 'a'),          # massas -> massa\n",
    "    (r'es$', 'e'),          # (alguns casos)\n",
    "    (r'mente$', ''),        # provavelmente -> provavel\n",
    "    (r'ado$', 'ar'),        # localizado -> localizar\n",
    "    (r'ido$', 'ir'),        # definido -> definir\n",
    "    (r'√¢ncia$', 'ante'),    # import√¢ncia -> importante\n",
    "    (r'√™ncia$', 'ente'),    # evid√™ncia -> evidente\n",
    "]\n",
    "\n",
    "# Normaliza√ß√£o de termos BI-RADS\n",
    "BIRADS_NORMALIZE = {\n",
    "    r'bi[\\-\\s]?rads?\\s*(categoria\\s*)?(\\d)': r'birads\\2',  # BI-RADS categoria 3 -> birads3\n",
    "    r'categoria\\s*(\\d)': r'birads\\1',                       # categoria 3 -> birads3\n",
    "    r'cat\\.?\\s*(\\d)': r'birads\\1',                          # cat. 3 -> birads3\n",
    "}\n",
    "\n",
    "# Termos m√©dicos importantes (n√£o modificar)\n",
    "PROTECTED_TERMS = {\n",
    "    'birads', 'birads0', 'birads1', 'birads2', 'birads3', 'birads4', 'birads5', 'birads6',\n",
    "    'calcifica√ß√£o', 'n√≥dulo', 'massa', 'assimetria', 'benigno', 'maligno',\n",
    "    'suspeito', 'at√≠pico', 'microcalcifica√ß√£o', 'densidade', 'espiculado',\n",
    "    'irregular', 'circunscrito', 'linear', 'segmentar', 'regional', 'difuso',\n",
    "    'heterog√™neo', 'homog√™neo', 'negativo', 'positivo', 'sugestivo'\n",
    "}\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    \"\"\"Lematiza√ß√£o simples baseada em regras.\"\"\"\n",
    "    if word in PROTECTED_TERMS:\n",
    "        return word\n",
    "    for pattern, replacement in LEMMA_RULES:\n",
    "        if re.search(pattern, word):\n",
    "            lemma = re.sub(pattern, replacement, word)\n",
    "            if len(lemma) >= 3:  # Evitar palavras muito curtas\n",
    "                return lemma\n",
    "    return word\n",
    "\n",
    "def preprocess_text_v2(text):\n",
    "    \"\"\"Preprocessamento avan√ßado: stop words + lematiza√ß√£o + normaliza√ß√£o BI-RADS.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalizar BI-RADS\n",
    "    for pattern, replacement in BIRADS_NORMALIZE.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remover caracteres especiais\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Normalizar n√∫meros (medidas)\n",
    "    text = re.sub(r'\\d+\\s*(mm|cm|ml)', ' MEDIDA ', text)\n",
    "    text = re.sub(r'\\d+', ' NUM ', text)\n",
    "    \n",
    "    # Tokenizar\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filtrar stop words e lematizar\n",
    "    filtered = []\n",
    "    for word in words:\n",
    "        if word in STOP_WORDS or len(word) <= 1:\n",
    "            continue\n",
    "        lemma = lemmatize_word(word)\n",
    "        filtered.append(lemma)\n",
    "    \n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Teste\n",
    "sample = \"A paciente realizou exame de mamografia bilateral. BI-RADS categoria 2, achados benignos. Calcifica√ß√µes bilaterais.\"\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Tratado v2: {preprocess_text_v2(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FOCAL LOSS ====\n",
    "print(\"[3/9] Definindo Focal Loss...\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "print(f'Focal Loss: gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CARREGAR E PREPROCESSAR DADOS ====\n",
    "print(\"[4/9] Carregando e preprocessando dados...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# Aplicar preprocessamento v2\n",
    "train_df['report_treated'] = train_df['report'].apply(preprocess_text_v2)\n",
    "test_df['report_treated'] = test_df['report'].apply(preprocess_text_v2)\n",
    "\n",
    "print(f'Train: {train_df.shape}, Test: {test_df.shape}')\n",
    "print(f'\\nExemplo original:\\n{train_df[\"report\"].iloc[0][:200]}...')\n",
    "print(f'\\nExemplo tratado v2:\\n{train_df[\"report_treated\"].iloc[0][:200]}...')\n",
    "\n",
    "# Estat√≠sticas\n",
    "orig_len = train_df['report'].str.len().mean()\n",
    "treated_len = train_df['report_treated'].str.len().mean()\n",
    "print(f'\\nComprimento m√©dio original: {orig_len:.0f} chars')\n",
    "print(f'Comprimento m√©dio tratado: {treated_len:.0f} chars ({100*treated_len/orig_len:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DATASET CLASS ====\n",
    "print(\"[5/9] Preparando dataset...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "print('Tokenizer carregado!')\n",
    "\n",
    "# Split - usando texto TRATADO v2\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['report_treated'].values, train_df['target'].values,\n",
    "    test_size=0.1, random_state=SEED, stratify=train_df['target']\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_ds = TextDataset(test_df['report_treated'].values, None, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c396923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== MODELO ====\n",
    "print(\"[6/9] Carregando modelo...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    local_files_only=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'Modelo carregado! Parametros: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TREINAMENTO ====\n",
    "print(\"[7/9] Treinando modelo...\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
    "            if 'labels' in batch:\n",
    "                labels.extend(batch['labels'].numpy())\n",
    "    if labels:\n",
    "        return f1_score(labels, preds, average='macro')\n",
    "    return preds\n",
    "\n",
    "best_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    val_f1 = evaluate(model, val_loader)\n",
    "    print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Val F1={val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_model.pt')\n",
    "        print(f'  -> Melhor modelo salvo! F1={best_f1:.4f}')\n",
    "\n",
    "print(f'\\nMelhor F1 valida√ß√£o: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c00685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PREDI√á√ÉO ====\n",
    "print(\"[8/9] Gerando predi√ß√µes...\")\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_model.pt'))\n",
    "predictions = evaluate(model, test_loader)\n",
    "print(f'Predi√ß√µes geradas: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SUBMISS√ÉO ====\n",
    "print(\"[9/9] Criando submiss√£o...\")\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print('‚úÖ Submiss√£o salva!')\n",
    "print(f'\\nDistribui√ß√£o das predi√ß√µes:')\n",
    "print(submission['target'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
