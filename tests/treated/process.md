# Processamento de Dados - Modelos Tratados

## Objetivo
Aplicar t√©cnicas de pr√©-processamento de texto para melhorar a performance dos melhores modelos baseline identificados.

---

## Pipeline de Processamento

### 1. An√°lise Explorat√≥ria (EDA)
- [ ] Distribui√ß√£o das classes (target)
- [ ] Comprimento dos textos (min, max, m√©dia)
- [ ] Frequ√™ncia de palavras
- [ ] Identificar padr√µes nos laudos
- [ ] Verificar textos duplicados
- [ ] Analisar vocabul√°rio espec√≠fico de mamografia

### 2. Limpeza de Texto
- [ ] Remover caracteres especiais
- [ ] Normalizar acentua√ß√£o
- [ ] Converter para min√∫sculas
- [ ] Remover n√∫meros (ou normalizar: `12mm` ‚Üí `NUM`)
- [ ] Remover espa√ßos extras
- [ ] Corrigir encoding (UTF-8)

### 3. Stop Words
- [ ] Remover stop words em portugu√™s (NLTK/spaCy)
- [ ] **N√ÉO remover** termos m√©dicos importantes
- [ ] Criar lista customizada de stop words
- [ ] Testar com/sem remo√ß√£o (comparar performance)

### 4. Normaliza√ß√£o de Vocabul√°rio
- [ ] **Lematiza√ß√£o** (spaCy pt_core_news_lg)
  - Ex: `calcifica√ß√µes` ‚Üí `calcifica√ß√£o`
  - Ex: `assim√©tricos` ‚Üí `assim√©trico`
- [ ] **Stemming** (alternativa: RSLP Stemmer)
  - Mais agressivo, pode perder significado
- [ ] Expandir abrevia√ß√µes m√©dicas
  - Ex: `BI-RADS` (manter)
  - Ex: `cm` ‚Üí `cent√≠metros` (ou manter)

### 5. Filtros e Regras
- [ ] Extrair termos BI-RADS expl√≠citos (regex)
- [ ] Identificar palavras-chave de cada categoria
- [ ] Remover se√ß√µes irrelevantes (cabe√ßalho, assinatura)
- [ ] Extrair features estruturadas:
  - Men√ß√£o de "massa"
  - Men√ß√£o de "calcifica√ß√£o"
  - Men√ß√£o de "assimetria"
  - Men√ß√£o de "achado benigno"

### 6. Feature Engineering
- [ ] N-grams (bigramas, trigramas)
- [ ] TF-IDF com vocabul√°rio filtrado
- [ ] Embeddings com texto limpo
- [ ] Features num√©ricas extra√≠das do texto

---

## Experimentos Planejados

| Experimento | Processamento | Modelo Base | Status |
|-------------|---------------|-------------|--------|
| treated_v1 | Stop words + lowercase | LinearSVC | ‚è≥ |
| treated_v2 | Stop words + lematiza√ß√£o | LinearSVC | ‚è≥ |
| treated_v3 | Lematiza√ß√£o + filtros BI-RADS | LinearSVC | ‚è≥ |
| treated_v4 | Completo (todos os passos) | Top 3 modelos | ‚è≥ |

---

## Depend√™ncias Offline

Para funcionar no Kaggle (Internet OFF), adicionar datasets:

| Biblioteca | Dataset Kaggle |
|------------|----------------|
| spaCy pt_core_news_lg | `rtatman/spacy-pretrained-models` |
| NLTK stopwords | `nltkdata/stopwords` |
| NLTK punkt | `nltkdata/punkt` |

---

## C√≥digo Base

```python
import re
import spacy

# Carregar modelo spaCy (offline)
nlp = spacy.load('/kaggle/input/spacy-models/pt_core_news_lg')

# Stop words customizadas (manter termos m√©dicos)
STOP_WORDS_REMOVE = {'o', 'a', 'os', 'as', 'um', 'uma', 'de', 'da', 'do', 'em', 'para', 'com'}
STOP_WORDS_KEEP = {'n√£o', 'sem', 'normal', 'benigno', 'maligno', 'suspeito'}

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    
    # Remover caracteres especiais (manter h√≠fen para BI-RADS)
    text = re.sub(r'[^\w\s\-]', ' ', text)
    
    # Normalizar espa√ßos
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Lematiza√ß√£o
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc 
              if token.text not in STOP_WORDS_REMOVE or token.text in STOP_WORDS_KEEP]
    
    return ' '.join(tokens)

# Extrair men√ß√µes BI-RADS
def extract_birads(text):
    pattern = r'bi-?rads?\s*:?\s*(\d)'
    match = re.search(pattern, text.lower())
    return int(match.group(1)) if match else None
```

---

## Notas

- ‚ö†Ô∏è Sempre comparar com baseline sem tratamento
- ‚ö†Ô∏è Cuidado para n√£o remover informa√ß√£o importante
- üí° Textos m√©dicos t√™m vocabul√°rio espec√≠fico
- üí° BI-RADS pode estar expl√≠cito ou impl√≠cito no texto
- üí° Fazer valida√ß√£o cruzada para evitar overfitting
